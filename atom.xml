<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>态度决定高度,细节决定成败</title>
  
  <subtitle>疯狂的码农</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.fengqinglei.top/"/>
  <updated>2019-12-22T16:21:52.105Z</updated>
  <id>https://www.fengqinglei.top/</id>
  
  <author>
    <name>FQL</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Meet Solr Lucene Codec</title>
    <link href="https://www.fengqinglei.top/2019/12/14/meet-solr-lucene-codec/"/>
    <id>https://www.fengqinglei.top/2019/12/14/meet-solr-lucene-codec/</id>
    <published>2019-12-14T13:05:13.000Z</published>
    <updated>2019-12-22T16:21:52.105Z</updated>
    
    <content type="html"><![CDATA[<h1 id="小试-Solr-Lucene-Codec"><a href="#小试-Solr-Lucene-Codec" class="headerlink" title="小试 Solr/Lucene Codec"></a><font color="#0077bb">小试 Solr/Lucene Codec</font></h1><h2 id="Codec-是什么"><a href="#Codec-是什么" class="headerlink" title="Codec 是什么"></a><font color="#2C3E50">Codec 是什么</font></h2><p>Lucene 从4.0版本开始提供codec api，简单的来说Lucene通过codec机制来读写索引文件，说白了就是一层数据访问层的api，正常来说我们在使用Lucene/Solr的时候是不用关心这一层细节的，因为默认的codec实现已经经过了大量的细节优化，但是如果你需要修改索引的存储方式，那么codec就是你的入手之处。<br><strong>因为这里涉及到很多细节，希望大家先不用关心代码细节，而是关注具体的流程和原理，最后对照代码来理解这篇博客,当然如果你还有不明白的地方，欢迎邮件我fengqingleiyue@163.com</strong></p><h2 id="自定义Codec"><a href="#自定义Codec" class="headerlink" title="自定义Codec"></a><font color="#2C3E50">自定义Codec</font></h2><p>既然我们知道了codec是什么，那么作为程序员的我们，就会想到-&gt; 我们怎么自定义一个codec呢?<br><a id="more"></a><br>这里为了和笔者所遇到的实际场景相结合，笔者使用Solr(7.7.2)作为例子进行讲解。但在我们开始讲解如何实现自定义codec之前，我们可以看下solr官方文档中的<strong>SimpleTextCodecFactory</strong>,这个codec是Lucene自带的，主要作用其实是通过可读的文本格式用来描述Lucene是怎么存储索引文件的，当然启用的方式也比较简单,修改solrconfig中的SchemaCodecFactory为:<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">codecFactory</span> <span class="attr">class</span>=<span class="string">"solr.SimpleTextCodecFactory"</span>/&gt;</span></span><br></pre></td></tr></table></figure></p><p>接下来直接往solr中添加文档就行(这里我们不需要修改任何一行索引代码)，但是这里要注意，因为<strong>SimpleTextCodecFactory</strong>的主要目的是用于演示Lucene是如何存储索引文件的，所以千万不要索引太多的文档(毕竟人家的目的是demo),当索引完成,我们可以查看index目录下的<strong>_XXX_XXX.fld</strong>文件的内容(这里笔者只索引了两个文档，这里看的是.fld文件，当然也可以看其他文件)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">doc 0</span><br><span class="line">  field 0</span><br><span class="line">    name id</span><br><span class="line">    type string</span><br><span class="line">    value 1</span><br><span class="line">  field 1</span><br><span class="line">    name text</span><br><span class="line">    type string</span><br><span class="line">    value 如果想指定结果到特定内存，我们可以使用前面介绍的索引来进行替换操作</span><br><span class="line">  field 2</span><br><span class="line">    name textCNNew</span><br><span class="line">    type string</span><br><span class="line">    value 我们需要先调用attach_grad函数来申请存储梯度所需要的内存</span><br><span class="line">END</span><br><span class="line">checksum 00000000001449379451</span><br></pre></td></tr></table></figure></p><p>与使用<strong>SchemaCodecFactory</strong>不同的是，这次我们看到了我们输入的原始内容，但是在<strong>SchemaCodecFactory</strong>的情况下，我们是无法看到这些信息的，因为<strong>SchemaCodecFactory</strong>在存储的时候都是以<strong>二进制形式</strong>，而<strong>SimpleTextCodecFactory</strong>使用的是<strong>文本格式</strong>。到这里我们演示了Lucene自带的<strong>SimpleTextCodec</strong>,相信大家应该对codec有一个比较形象的理解了。那么接下来我们开始实现自己的codec。</p><h3 id="自定义DocValuesFormat"><a href="#自定义DocValuesFormat" class="headerlink" title="自定义DocValuesFormat"></a><font color="#2C3E50">自定义DocValuesFormat</font></h3><p>之前我们说过，codec是Lucene读写索引文件的一种机制，而细心的人一定发现Lucene的索引是有很多文件的，即使我们优化了，也绝对不可能只有一个文件，关于Lucene各种文件的描述，大家可以参考<a href="https://lucene.apache.org/core/7_7_2/core/index.html" target="_blank" rel="noopener">Summary of File Extensions</a>,其实codec是一套api，一套对Lucene各种类型文件进行读写的api，而每种不同类型的文件也是由codec中定义的各种Format来实现的，例如,以solr7.7.2中默认的<strong>Lucene70Codec</strong>为例:</p><div class="table-container"><table><thead><tr><th>索引文件类型</th><th>主要用途</th><th>涉及到的文件类型</th></tr></thead><tbody><tr><td>FieldInfosFormat</td><td>用来Encode/Decode org.apache.lucene.index.FieldInfos,主要是关于文档中文件的信息，如是否索引，是否有payload等信息</td><td>.fnm文件</td></tr><tr><td>DocValuesFormat</td><td>用来Encodes/decodes per-document values，主要是对每个文档的信息进行编码和解码（Docvalues 其实是一种正排索引）</td><td>.dvd &amp; .dvm</td></tr><tr><td>PostingsFormat</td><td>用来Encodes/decodes 倒排表，词频，位置等信息</td><td>.pos .tip .tim .pay .doc </td></tr><tr><td>…</td><td>….</td><td>…</td></tr></tbody></table></div><p>而Solr中开放了两种最常用的Format,<strong>DocValuesFormat</strong>和<strong>PostingsFormat</strong>供我们进行自定义(其实就是倒排索引和正排索引),这里我们以自定义<strong>DocValuesFormat</strong>来演示如何在Solr中使用我们自定义的DocValuesFormat，废话不多说，我们开始:</p><ul><li>第一步: 移花接木<br>一般来说Lucene/Solr提供的默认的DocValuesFormat已经经过了大量的代码优化和实战验证，如果不是因为特定的需求，我们一般不会也不需要进行100%的自定义的DocValuesFormat，所以这么为了演示，我们使用”移花接木”的方式来自定义DocValuesFormat-&gt; copy官方的代码然后改掉类名-&gt;然后就是我们自己的了。例如笔者这里自定义了<a href="https://raw.githubusercontent.com/fengqingleiyue/apache-solr-plugins/master/customized-codec/src/main/java/org/fql/codec/docvalues/FqlDocValuesFormat.java" target="_blank" rel="noopener">FqlDocValuesFormat</a><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.fql.codec.docvalues;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.lucene.codecs.DocValuesConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.lucene.codecs.DocValuesFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.lucene.codecs.DocValuesProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.lucene.index.SegmentReadState;</span><br><span class="line"><span class="keyword">import</span> org.apache.lucene.index.SegmentWriteState;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by fengqinglei on 2019/11/27.</span></span><br><span class="line"><span class="comment"> * see &#123;<span class="doctag">@link</span> org.apache.lucene.codecs.lucene70.Lucene70DocValuesFormat&#125;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">FqlDocValuesFormat</span> <span class="keyword">extends</span> <span class="title">DocValuesFormat</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FqlDocValuesFormat</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(<span class="string">"Fql"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> DocValuesConsumer <span class="title">fieldsConsumer</span><span class="params">(SegmentWriteState state)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> FqlDocValuesConsumer(state, DATA_CODEC, DATA_EXTENSION, META_CODEC, META_EXTENSION);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> DocValuesProducer <span class="title">fieldsProducer</span><span class="params">(SegmentReadState state)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> FqlDocValuesProducer(state, DATA_CODEC, DATA_EXTENSION, META_CODEC, META_EXTENSION);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> String DATA_CODEC = <span class="string">"FqlDocValuesData"</span>;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> String DATA_EXTENSION = <span class="string">"dvd"</span>;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> String META_CODEC = <span class="string">"FqlDocValuesMetadata"</span>;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> String META_EXTENSION = <span class="string">"dvm"</span>;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> VERSION_START = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> VERSION_CURRENT = VERSION_START;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// indicates docvalues type</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">byte</span> NUMERIC = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">byte</span> BINARY = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">byte</span> SORTED = <span class="number">2</span>;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">byte</span> SORTED_SET = <span class="number">3</span>;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">byte</span> SORTED_NUMERIC = <span class="number">4</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> DIRECT_MONOTONIC_BLOCK_SHIFT = <span class="number">16</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> NUMERIC_BLOCK_SHIFT = <span class="number">14</span>;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> NUMERIC_BLOCK_SIZE = <span class="number">1</span> &lt;&lt; NUMERIC_BLOCK_SHIFT;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> TERMS_DICT_BLOCK_SHIFT = <span class="number">4</span>;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> TERMS_DICT_BLOCK_SIZE = <span class="number">1</span> &lt;&lt; TERMS_DICT_BLOCK_SHIFT;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> TERMS_DICT_BLOCK_MASK = TERMS_DICT_BLOCK_SIZE - <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> TERMS_DICT_REVERSE_INDEX_SHIFT = <span class="number">10</span>;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> TERMS_DICT_REVERSE_INDEX_SIZE = <span class="number">1</span> &lt;&lt; TERMS_DICT_REVERSE_INDEX_SHIFT;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> TERMS_DICT_REVERSE_INDEX_MASK = TERMS_DICT_REVERSE_INDEX_SIZE - <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><p>大家可以和源码对比下，<strong>真的就是copy出来改了下类名</strong>。当然由于Lucene部分代码没有开放public权限，所以我们必须自定义<a href="https://github.com/fengqingleiyue/apache-solr-plugins/blob/master/customized-codec/src/main/java/org/fql/codec/docvalues/FqlDocValuesConsumer.java" target="_blank" rel="noopener">FqlDocValuesConsumer</a>和<a href="https://github.com/fengqingleiyue/apache-solr-plugins/blob/master/customized-codec/src/main/java/org/fql/codec/docvalues/FqlDocValuesProducer.java" target="_blank" rel="noopener">FqlDocValuesProducer</a>,完整的代码可以参考<a href="https://github.com/fengqingleiyue/apache-solr-plugins/tree/master/customized-codec/src/main/java/org/fql/codec/docvalues" target="_blank" rel="noopener">apache-solr-plugins/customized-codec/src/main/java/org/fql/codec/docvalues</a>。</p><ul><li>第二步: 开启docValues<br>当我们完成了自定义的<strong>FqlDocValuesFormat</strong>之后，我们就可以在solr中启用了，启用的方式也比较简单，可以在solr的官方文档中直接检索<strong>DocValuesFormat</strong>关键字就可以找到相关的用法，这里直接给出改动的点:</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">field</span> <span class="attr">name</span>=<span class="string">"type"</span> <span class="attr">type</span>=<span class="string">"FQL"</span> <span class="attr">indexed</span>=<span class="string">"false"</span> <span class="attr">stored</span>=<span class="string">"false"</span> <span class="attr">required</span>=<span class="string">"true"</span> <span class="attr">multiValued</span>=<span class="string">"false"</span> <span class="attr">docValues</span>=<span class="string">"true"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">fieldType</span> <span class="attr">name</span>=<span class="string">"FQL"</span> <span class="attr">class</span>=<span class="string">"solr.StrField"</span> <span class="attr">sortMissingLast</span>=<span class="string">"true"</span> <span class="attr">docValuesFormat</span>=<span class="string">"Fql"</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 这里的Fql就是FqlDocValuesFormat#super方法中定义的名称--&gt;</span></span><br></pre></td></tr></table></figure><p>很遗憾，当我们打完包放到solr_home的对应的core的lib目录下之后重启Solr会发现在日志中出现以下的错误:<br>(solr_home/CODEC/lib,这里的CODEC就是core的名称,并且这里的jar不需要包含任何依赖)<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.lang.IllegalArgumentException: An SPI class of type org.apache.lucene.codecs.PostingsFormat with name 'Fql' does not exist.  You need to add the corresponding JAR file supporting this SPI to your classpath.  The current classpath supports the following names: [MockRandom, RAMOnly, LuceneFixedGap, LuceneVarGapFixedInterval, LuceneVarGapDocFreqInterval, TestBloomFilteredLucenePostings, Asserting, BlockTreeOrds, BloomFilter, Direct, FSTOrd50, FST50, Memory, Lucene50, IDVersion, completion]</span><br><span class="line">at org.apache.lucene.util.NamedSPILoader.lookup(NamedSPILoader.java:<span class="number">116</span>) ~[java/:?]</span><br><span class="line">at org.apache.lucene.codecs.PostingsFormat.forName(PostingsFormat.java:<span class="number">112</span>) ~[java/:?]</span><br><span class="line">at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat$FieldsReader.&lt;init&gt;(PerFieldPostingsFormat.java:<span class="number">280</span>) ~[java/:?]</span><br><span class="line">at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat.fieldsProducer(PerFieldPostingsFormat.java:<span class="number">363</span>) ~[java/:?]</span><br><span class="line">at org.apache.lucene.index.SegmentCoreReaders.&lt;init&gt;(SegmentCoreReaders.java:<span class="number">113</span>) ~[java/:?]</span><br><span class="line">at org.apache.lucene.index.SegmentReader.&lt;init&gt;(SegmentReader.java:<span class="number">83</span>) ~[java/:?]</span><br><span class="line">at org.apache.lucene.index.ReadersAndUpdates.getReader(ReadersAndUpdates.java:<span class="number">172</span>) ~[java/:?]</span><br><span class="line">at org.apache.lucene.index.ReadersAndUpdates.getReadOnlyClone(ReadersAndUpdates.java:<span class="number">214</span>) ~[java/:?]</span><br><span class="line">at org.apache.lucene.index.StandardDirectoryReader.open(StandardDirectoryReader.java:<span class="number">106</span>) ~[java/:?]</span><br><span class="line">at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:<span class="number">525</span>) ~[java/:?]</span><br><span class="line">at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:<span class="number">103</span>) ~[java/:?]</span><br><span class="line">at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:<span class="number">79</span>) ~[java/:?]</span><br><span class="line">at org.apache.solr.core.StandardIndexReaderFactory.newReader(StandardIndexReaderFactory.java:<span class="number">39</span>) ~[java/:?]</span><br><span class="line">at org.apache.solr.core.SolrCore.openNewSearcher(SolrCore.java:<span class="number">2101</span>) ~[java/:?]</span><br><span class="line">at org.apache.solr.core.SolrCore.getSearcher(SolrCore.java:<span class="number">2257</span>) ~[java/:?]</span><br><span class="line">at org.apache.solr.core.SolrCore.initSearcher(SolrCore.java:<span class="number">1106</span>) ~[java/:?]</span><br><span class="line">at org.apache.solr.core.SolrCore.&lt;init&gt;(SolrCore.java:<span class="number">993</span>) ~[java/:?]</span><br><span class="line">at org.apache.solr.core.SolrCore.&lt;init&gt;(SolrCore.java:<span class="number">874</span>) ~[java/:?]</span><br><span class="line">at org.apache.solr.core.CoreContainer.createFromDescriptor(CoreContainer.java:<span class="number">1187</span>) ~[java/:?]</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p><p>这是因为codec采用了java的<a href="https://en.wikipedia.org/wiki/Service_provider_interface" target="_blank" rel="noopener">Service Provider Interface (SPI)</a>来加载，这是为了保证codec可以做成可插拔的模式。当然既然知道为什么出错，解决方案自然也就比较简单了，直接在项目的<strong>resources</strong>目录下建立以下的结构和文件:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./META-INF</span><br><span class="line">./META-INF/services</span><br><span class="line">./META-INF/services/org.apache.lucene.codecs.DocValuesFormat</span><br></pre></td></tr></table></figure></p><p>这里需要注意的是文件名<strong>org.apache.lucene.codecs.DocValuesFormat</strong>是不可以随便修改的，另外文件<strong>org.apache.lucene.codecs.DocValuesFormat</strong>的内容为<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ cat org.fql.codec.docvalues.FqlDocValuesFormat</span><br><span class="line">org.fql.codec.docvalues.FqlDocValuesFormat</span><br><span class="line"><span class="comment"># 文件的内容就是我们自定义的DocValuesFormat的类完整类名(包含包路径)</span></span><br></pre></td></tr></table></figure></p><p>重新打包更新jar包并且重启solr之后会发现之前的错误已经消失了，并且可以通过索引代码更新数据。细心的读者可能会问，貌似我的客户端(index程序)一行代码也没有改，确实，客户端的代码是不需要任何改动的，我去，貌似搞了这么长的时间啥动静也没有，改了这么多有啥用？貌似并不能改变世界…. 是的，但是当你打开索引目录你会发现除了常见的<br><strong>_xx_Lucene70_0.dvd</strong> 和 <strong>_xx_Lucene70_0.dvm</strong>文件，多了 <strong>_xx_Fql_0.dvd</strong> 和 <strong>_xx_Fql_0.dvm</strong>文件，额。。。 貌似还是没啥鸟用。。。  不要着急，下一章就是见证奇迹的时刻!!!</p><h2 id="让solr支持按字段更新"><a href="#让solr支持按字段更新" class="headerlink" title="让solr支持按字段更新"></a><font color="#2C3E50">让solr支持按字段更新</font></h2><p>假设你维护的solr有以下几个特点:</p><blockquote><ul><li>对实时没有太高的要求，数据最迟可以按天更新</li><li>索引容量庞大(过亿的文档总数，过TB的索引大小，并且每条数据都可能被用户检索到，都是热数据)，为了节约成本，你每次都会将索引优化之后上线</li><li>用户的某种设置可以造成万甚至百万级的数据更新，例如用户可以根据已有文档的某个属性某一些值(这里的一些可能是几个也可能是几百个到千个)给已有的几万或者几百万的文档打上标签(其实就是就是增加一个属性)，要求可以检索到这个属性，也可以对这个属性做一维或者二维的分析(solr的facet的功能)，并且这个属性的值是用户手动输入的。</li></ul></blockquote><p>这个需求还是比较变态的，如果我们很有钱，可以怼上几万台机器，管你几百万的更新量，反正给加机器，加到秒回就行了。（现实是我们只有买个位数台服务器的钱，但是还是得满足变态客户的需求)。<br>好吧，既然我们无法做到实时更新，我们只能做近实时或者按照天更新了，那么如果是按照天更新我们该怎么做？大概的解决想法大家肯定能想到:</p><ul><li>既然有用户的输入的数据，那么我们至少需要一张表来存储用户输入和已有文档属性的映射关系，例如: 假设上面提到某个文档的属性我们成为Field_A，该字段可能取值为a<sub>1</sub>,a<sub>2</sub>,a<sub>3</sub>…a<sub>n</sub>,用户打上的标签的字段我们称作为CUSTOM_FIELD,那么可能用户可能的操作为，将Field_A值为a<sub>1</sub>,a<sub>100</sub>,a<sub>10000</sub>….a<sub>k</sub>的文档的CUSTOM_FIELD的值定义为:”自定义标签T”,那么一种可能的schema设计为(这里以dynamodb为例子,笔者所在的公司使用的aws的dynamodb服务)</li></ul><div class="table-container"><table><thead><tr><th>Field_A_VALUE(HashKey)</th><th>UserID(RangeKey)</th><th>CUSTOM_FIELD_VALUE</th></tr></thead><tbody><tr><td>a<sub>1</sub></td><td>123456</td><td>自定义标签T</td></tr><tr><td>a<sub>100</sub></td><td>123456</td><td>自定义标签T</td></tr><tr><td>…</td><td>123456</td><td>自定义标签T</td></tr><tr><td>a<sub>K</sub></td><td>123456</td><td>自定义标签T</td></tr></tbody></table></div><ul><li>在每天的数据更新的时候我们通过文档Field_A的值(一般来说这个不会改变,因为是文档的固有属性)，查询上述的表结构,然后将<strong>UserID</strong>和<strong>CUSTOM_FIELD_VALUE</strong>的值拼接起来作为字段CUSTOM_FIELD字段的值，例如可能的<strong>SolrInputDocument</strong>为:</li></ul><div class="table-container"><table><thead><tr><th>DocUnique(文档唯一健，与业务强相关)</th><th>Field_A</th><th>CUSTOM_FIELD</th></tr></thead><tbody><tr><td>1</td><td>a<sub>1</sub></td><td>123456_自定义标签T,34553_自定义标签R…</td></tr><tr><td>1</td><td>a<sub>k</sub>,a<sub>100</sub></td><td>123456_自定义标签T,89882_自定义标签M…</td></tr></tbody></table></div><p>那么问题来了，为了尽可能的缩小数据更新的范围，我可定得知道哪一些文档会受到用户打上的标签的影响，这时候可能有人会说，这个简单，用户打标签的时候，通过query: Field_A:(a<sub>1</sub> OR a<sub>100</sub> OR ..) 查询后导出不就行了嘛，理论上确实可行，<strong>但是我们在之前的描述中说过，用户打标签这个动作可能会影响几万或者几百万的文档，几万的文档我们还可能导出，但是几百万的文档，我相信几台服务器肯定挂了。</strong>,那么剩下的解决方案只能是全量<strong>更新索引</strong>，如果你维护的索引比较小只有几个GB的话，这种方法确实没有什么不好的，重新更新索引也就是分分钟的事情，但是如果你维护的索引有几个TB的大小，重新做一遍的话可能需要几天或者几十个小时的时间，就算我们复制一份线上的数据到单独的服务器，然后更新用户的标签的变更找到搜到影响的数据，可能这批数据的总量也将近全量的数据了，那么这种情况我们怎么处理？难道我们真的得每天重新更新一下可能有几个TB的索引？</p><h3 id="通过Codec单独更新CUSTOM-FIELD字段"><a href="#通过Codec单独更新CUSTOM-FIELD字段" class="headerlink" title="通过Codec单独更新CUSTOM_FIELD字段"></a><font color="#2C3E50">通过Codec单独更新CUSTOM_FIELD字段</font></h3><p>在之前的文章中我们介绍了Lucene的codec，那么我们能不能通过codec的方式来解决问题呢？(<strong>这里给出的解决方案是笔者使用的方式，欢迎大家提出更加完美的解决方案</strong>),我们先来分析分析问题:</p><ul><li>其实我们需要更新的只有一个CUSTOM_FIELD的字段</li><li>这个CUSTOM_FIELD的值是根据Field_A字段生成出来的，而Field_A的数据我们可以通过读索引得到</li><li>有没有办法只更新CUSTOM_FIELD字段？</li></ul><p>当然solr是支持只更新某个字段的，但是前提是其他字段都得存储下来(stored=”true”),如果我们的索引容量比较小的话还是可以这么玩的，但是如果我们维护的是TB级别的索引，所有字段设置stored=”true”估计容量就得翻倍了，本来就没钱买服务器，现在更加。。。。,那么我们有没有办法既能不用存储其他字段也能做到只更新某个字段呢？答案肯定是有的。我们下面来分析下:</p><ul><li>Solr/Lucene的索引本质实际上就是docid(注意这里的docid是Lucene的唯一健,和Solr的uniquekey注意区分)和term的映射关系，要们倒过来叫倒排索引，要么正过来叫正排索引(docvalues)</li><li>所谓的更新数据要们就是删除/修改原来的docid和term的映射关系，也就是说理论上来说如果我们知道这些数据在索引文件中是怎么存储的，我们应该可以重建这样的映射关系然后在覆盖原来的数据<h3 id="爬坑指南"><a href="#爬坑指南" class="headerlink" title="爬坑指南"></a><font color="#2C3E50">爬坑指南</font></h3>之前我们说过，如果我们能知道docid和term的关系是如何在索引中保存的，那么我们就能通过代码按照格式修改/删除docid和term的映射关系，就相当于我们可以只更新某一个字段了,为了能做到只更新某一个字段，我们第一步我们需要将这个字段和其他字段的索引隔离开来</li><li>隔离需要单独更新的字段的索引数据<br>这一步比较简单，参考上面的例子，我们可以通过codec来隔离例如:(这里的type字段就是上面我们举的例子的CUSTOM_FIELD字段)<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">field</span> <span class="attr">name</span>=<span class="string">"type"</span> <span class="attr">type</span>=<span class="string">"FQL"</span> <span class="attr">indexed</span>=<span class="string">"false"</span> <span class="attr">stored</span>=<span class="string">"false"</span> <span class="attr">required</span>=<span class="string">"true"</span> <span class="attr">multiValued</span>=<span class="string">"false"</span> <span class="attr">docValues</span>=<span class="string">"true"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">fieldType</span> <span class="attr">name</span>=<span class="string">"FQL"</span> <span class="attr">class</span>=<span class="string">"solr.StrField"</span> <span class="attr">sortMissingLast</span>=<span class="string">"true"</span> <span class="attr">docValuesFormat</span>=<span class="string">"Fql"</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 这里的Fql就是FqlDocValuesFormat#super方法中定义的名称--&gt;</span></span><br></pre></td></tr></table></figure></li></ul><p>通过这样的修改我们就会在优化之后的索引文件中发现多出了_xx_Fql_0.dvd和_xx_Fql_0.dvm,其中dvm是docvalues的metadata信息，dvd文件就是保存docvalues的data信息。而type字段的docvalues数据其实已经和其他的字段的docvalues信息隔离开来了。</p><ul><li>使用Lucene的代码生成新的索引,这里我们给出伪代码，最终版本的代码比较复杂，我们不再这里亮出<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Directory directory = FSDirectory.open(Paths.get(original_index_location))</span><br><span class="line"><span class="comment">//original_index_location是指我们已经有的索引文件的路径，需要保证已经优化了</span></span><br><span class="line">IndexReader reader = DirectoryReader.open(directory)</span><br><span class="line">Directory newDir = FSDirectory.open(Paths.get(new_index_location))</span><br><span class="line">IndexWriterConfig config = <span class="keyword">new</span> IndexWriterConfig(<span class="keyword">new</span> StandardAnalyzer())</span><br><span class="line">config.setUseCompoundFile(<span class="keyword">false</span>);<span class="comment">//因为我们需要读取原始数据，所以必须关闭compoundfile</span></span><br><span class="line">config.setCodec(customCodec);<span class="comment">//使用我们自定义的codec（包含我们自定义的docValuesFormat）</span></span><br><span class="line">config.setOpenMode(IndexWriterConfig.OpenMode.CREATE);<span class="comment">// 创建新索引</span></span><br><span class="line">IndexWriter writer = <span class="keyword">new</span> IndexWriter(newDir,config)</span><br><span class="line"><span class="comment">//new_index_location是指我们要生成的新的索引文件的路径</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;reader.macDoc();i++)&#123;</span><br><span class="line">    Document document = <span class="keyword">new</span> Document();</span><br><span class="line">    <span class="comment">// 这里添加我们的业务代码，主要是给type字段进行赋值</span></span><br><span class="line">    writer.addDocument(document);</span><br><span class="line">&#125;</span><br><span class="line">writer.commit()</span><br><span class="line">writer.forceMerge(<span class="number">1</span>)</span><br><span class="line">....</span><br></pre></td></tr></table></figure></li></ul><p><a href="https://github.com/fengqingleiyue/apache-solr-plugins/blob/master/customized-codec/src/main/java/org/fql/codec/custom/CustomCodec.java" target="_blank" rel="noopener">customCodec</a></p><ul><li>使用新的生成的数据覆盖原始索引中对应的文件<br>笔者最开始的时候就直接cat new_index.dvd &gt;old_index.dvd &amp;&amp; new_index.dvm &gt; old_index.dvm,结果发现Solr都无法启动，当然原因也比较简单，是因为Lucene会为生成的索引文件加上一些指纹信息，如果segment的信息和对应的索引文件中的指纹信息不一致，那么就会认为索引文件已经破损。其实就是我们之前说的，我们得知道索引文件是格式是啥，这个我们可以通过读源代码来获取，例如dvd文件的格式我们可以通过查看org.apache.lucene.codecs.lucene70.Lucene70DocValuesProducer的源码了解数据是如何读取的(生成那就是倒过来的步骤)<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);</span><br><span class="line"><span class="keyword">this</span>.data = state.directory.openInput(dataName, state.context);</span><br><span class="line"><span class="keyword">boolean</span> success = <span class="keyword">false</span>;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> version2 = CodecUtil.checkIndexHeader(data, dataCodec,</span><br><span class="line">                                                Lucene70DocValuesFormat.VERSION_START,</span><br><span class="line">                                                Lucene70DocValuesFormat.VERSION_CURRENT,</span><br><span class="line">                                                state.segmentInfo.getId(),</span><br><span class="line">                                                state.segmentSuffix);</span><br><span class="line">    <span class="keyword">if</span> (version != version2) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> CorruptIndexException(<span class="string">"Format versions mismatch: meta="</span> + version + <span class="string">", data="</span> + version2, data);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// <span class="doctag">NOTE:</span> data file is too costly to verify checksum against all the bytes on open,</span></span><br><span class="line">    <span class="comment">// but for now we at least verify proper structure of the checksum footer: which looks</span></span><br><span class="line">    <span class="comment">// for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption</span></span><br><span class="line">    <span class="comment">// such as file truncation.</span></span><br><span class="line">    CodecUtil.retrieveChecksum(data);</span><br><span class="line"></span><br><span class="line">    success = <span class="keyword">true</span>;</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (!success) &#123;</span><br><span class="line">    IOUtils.closeWhileHandlingException(<span class="keyword">this</span>.data);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">  .... CodecUtil.checkIndexHeader 为 ....</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">checkIndexHeader</span><span class="params">(DataInput in, String codec, <span class="keyword">int</span> minVersion, <span class="keyword">int</span> maxVersion, <span class="keyword">byte</span>[] expectedID, String expectedSuffix)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">int</span> version = checkHeader(in, codec, minVersion, maxVersion);</span><br><span class="line">checkIndexHeaderID(in, expectedID);</span><br><span class="line">checkIndexHeaderSuffix(in, expectedSuffix);</span><br><span class="line"><span class="keyword">return</span> version;</span><br></pre></td></tr></table></figure></li></ul><p>通过源代码我们可以了解到dvd的文件格式为:<br><img src="https://s2.ax1x.com/2019/12/17/QT8658.png" alt="index_file_format.png"><br>这样的话，<strong>我们就可以使用老的索引文件的指纹信息，和新生成的索引文件的数据信息合成一份一新的索引,然后使用新生成的索引文件覆盖老的索引文件</strong>，应该就可以了。那废话不多说，这里放出读取原始(老)索引的指纹信息的代码<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">processCodecCheckList</span><span class="params">(IndexInput sourceInput,IndexInput desInput,IndexOutput output)</span> <span class="keyword">throws</span> IOException</span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="comment">// 这里的sourceInput 就是指原始(老)的索引文件</span></span><br><span class="line">        <span class="comment">// 这里的desInput 就是指 新的索引文件 </span></span><br><span class="line">        <span class="comment">// 这里的output就是指最终生成的索引文件</span></span><br><span class="line">        <span class="comment">/* process CODEC_MAGIC*/</span></span><br><span class="line">        output.writeInt(sourceInput.readInt());</span><br><span class="line">        System.out.println(<span class="string">"Skip codec magic "</span>+(desInput.readInt()== CodecUtil.CODEC_MAGIC));</span><br><span class="line">        <span class="comment">/* process codec name*/</span></span><br><span class="line">        output.writeString(sourceInput.readString());</span><br><span class="line">        System.out.println(<span class="string">"Skip codec name "</span> +desInput.readString());</span><br><span class="line">        <span class="comment">/* process codec index version*/</span></span><br><span class="line">        output.writeInt(sourceInput.readInt());</span><br><span class="line">        System.out.println(<span class="string">"Skip codec index version"</span> +desInput.readInt());</span><br><span class="line"></span><br><span class="line">        <span class="comment">/** process index header ID (byte array with length 16)</span></span><br><span class="line"><span class="comment">         *  here we need the source index header ID</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">byte</span> id[] = <span class="keyword">new</span> <span class="keyword">byte</span>[StringHelper.ID_LENGTH];</span><br><span class="line">        sourceInput.readBytes(id,<span class="number">0</span>,id.length);</span><br><span class="line">        output.writeBytes(id,id.length);</span><br><span class="line">        desInput.readBytes(id,<span class="number">0</span>,id.length);</span><br><span class="line">        System.out.println(<span class="string">"Skip codec index header id "</span>+StringHelper.idToString(id));</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* process the suffix*/</span></span><br><span class="line">        <span class="keyword">byte</span> suffixLength = sourceInput.readByte();</span><br><span class="line">        <span class="keyword">byte</span> suffixBytes[] = <span class="keyword">new</span> <span class="keyword">byte</span> [suffixLength];</span><br><span class="line">        sourceInput.readBytes(suffixBytes,<span class="number">0</span>,suffixBytes.length);</span><br><span class="line">        output.writeByte(suffixLength);</span><br><span class="line">        output.writeBytes(suffixBytes,suffixBytes.length);</span><br><span class="line"></span><br><span class="line">        suffixLength = desInput.readByte();</span><br><span class="line">        suffixBytes = <span class="keyword">new</span> <span class="keyword">byte</span>[suffixLength];</span><br><span class="line">        desInput.readBytes(suffixBytes,<span class="number">0</span>,suffixBytes.length);</span><br><span class="line">        System.out.println(<span class="string">"Skip codec index suffix "</span>+<span class="keyword">new</span> String(suffixBytes,<span class="number">0</span>,suffixLength, StandardCharsets.UTF_8));</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p><p>当然，我们这里只是使用了索引文件的指纹信息，<strong>具体的索引数据还是需要借用新生成的索引文件</strong>，这里给出部分代码<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*copy the index data */</span></span><br><span class="line"><span class="keyword">long</span> currentFilePointer = desInput.getFilePointer();</span><br><span class="line"><span class="keyword">while</span>(currentFilePointer!=(desInput.length()-CodecUtil.footerLength()))&#123;</span><br><span class="line">    output.writeByte(desInput.readByte());</span><br><span class="line">    currentFilePointer = desInput.getFilePointer();</span><br><span class="line">&#125;</span><br><span class="line">System.out.println(<span class="string">"Skip original indexing data"</span>);</span><br><span class="line">currentFilePointer = sourceInput.getFilePointer();</span><br><span class="line"><span class="keyword">while</span>(currentFilePointer!=(sourceInput.length()-CodecUtil.footerLength()))&#123;</span><br><span class="line">    sourceInput.readByte();</span><br><span class="line">    currentFilePointer = sourceInput.getFilePointer();</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 根据上面的索引文件格式分析，我们需要覆盖FOOTER_MAGIC,algorithmID和 checksum</span></span><br><span class="line"><span class="comment">/* process FOOTER_MAGIC*/</span></span><br><span class="line">output.writeInt(sourceInput.readInt());</span><br><span class="line"><span class="comment">/* process algorithmID*/</span></span><br><span class="line">output.writeInt(sourceInput.readInt());</span><br><span class="line">output.writeLong(output.getChecksum());</span><br><span class="line">output.close();</span><br><span class="line">sourceInput.close();</span><br><span class="line">desInput.close();</span><br></pre></td></tr></table></figure></p><p>很遗憾当我们使用上述的方式通过原始索引文件指纹信息骗过了Solr/Lucene启动时候的文件检查，成功的可以让Solr启动成功，但是当我们运行对<strong>type</strong>字段的查询或者统计的时候(如请求为<a href="http://127.0.0.1:8983/solr/CODEC/select?q=xx&amp;facet=true&amp;facet.field=type)会遇到" target="_blank" rel="noopener">http://127.0.0.1:8983/solr/CODEC/select?q=xx&amp;facet=true&amp;facet.field=type)会遇到</a>: <strong>java.lang.NullPointerException当然如果你非常非常幸运，你也不会遇到</strong>,这里我们先放出错误的信息，然后再解释如果解决这类错误，然后再解释为啥有的时候你不会遇到</p><ul><li><p>错误信息</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">10</span>:<span class="number">58</span>:<span class="number">52.418</span> [qtp752316209-<span class="number">18</span>] ERROR org.apache.solr.handler.RequestHandlerBase - org.apache.solr.common.SolrException: Exception during facet.field: type</span><br><span class="line">at org.apache.solr.request.SimpleFacets.lambda$getFacetFieldCounts$<span class="number">0</span>(SimpleFacets.java:<span class="number">832</span>)</span><br><span class="line">at java.util.concurrent.FutureTask.run(FutureTask.java:<span class="number">266</span>)</span><br><span class="line">at org.apache.solr.request.SimpleFacets$<span class="number">3</span>.execute(SimpleFacets.java:<span class="number">771</span>)</span><br><span class="line">at org.apache.solr.request.SimpleFacets.getFacetFieldCounts(SimpleFacets.java:<span class="number">841</span>)</span><br><span class="line">at org.apache.solr.handler.component.FacetComponent.getFacetCounts(FacetComponent.java:<span class="number">329</span>)</span><br><span class="line">at org.apache.solr.handler.component.FacetComponent.process(FacetComponent.java:<span class="number">273</span>)</span><br><span class="line">at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:<span class="number">298</span>)</span><br><span class="line">at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:<span class="number">199</span>)</span><br><span class="line">at org.apache.solr.core.SolrCore.execute(SolrCore.java:<span class="number">2551</span>)</span><br><span class="line">at org.apache.solr.servlet.HttpSolrCall.execute(HttpSolrCall.java:<span class="number">711</span>)</span><br><span class="line">at org.apache.solr.servlet.HttpSolrCall.call(HttpSolrCall.java:<span class="number">516</span>)</span><br><span class="line">at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:<span class="number">395</span>)</span><br><span class="line">at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:<span class="number">341</span>)</span><br><span class="line">at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:<span class="number">1602</span>)</span><br><span class="line">at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:<span class="number">540</span>)</span><br><span class="line">at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:<span class="number">146</span>)</span><br><span class="line">at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:<span class="number">548</span>)</span><br><span class="line">at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:<span class="number">132</span>)</span><br><span class="line">at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:<span class="number">257</span>)</span><br><span class="line">at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:<span class="number">1588</span>)</span><br><span class="line">at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:<span class="number">255</span>)</span><br><span class="line">at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:<span class="number">1345</span>)</span><br><span class="line">at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:<span class="number">203</span>)</span><br><span class="line">at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:<span class="number">480</span>)</span><br><span class="line">at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:<span class="number">1557</span>)</span><br><span class="line">at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:<span class="number">201</span>)</span><br><span class="line">at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:<span class="number">1247</span>)</span><br><span class="line">at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:<span class="number">144</span>)</span><br><span class="line">at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:<span class="number">132</span>)</span><br><span class="line">at org.eclipse.jetty.server.Server.handle(Server.java:<span class="number">502</span>)</span><br><span class="line">at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:<span class="number">364</span>)</span><br><span class="line">at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:<span class="number">260</span>)</span><br><span class="line">at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:<span class="number">305</span>)</span><br><span class="line">at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:<span class="number">103</span>)</span><br><span class="line">at org.eclipse.jetty.io.ChannelEndPoint$<span class="number">2</span>.run(ChannelEndPoint.java:<span class="number">118</span>)</span><br><span class="line">at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:<span class="number">765</span>)</span><br><span class="line">at org.eclipse.jetty.util.thread.QueuedThreadPool$<span class="number">2</span>.run(QueuedThreadPool.java:<span class="number">683</span>)</span><br><span class="line">at java.lang.Thread.run(Thread.java:<span class="number">748</span>)</span><br><span class="line">Caused by: java.lang.NullPointerException</span><br><span class="line">at org.fql.codec.docvalues.FqlDocValuesProducer.getSorted(FqlDocValuesProducer.java:<span class="number">785</span>)</span><br><span class="line">at org.fql.codec.docvalues.FqlDocValuesProducer.getSorted(FqlDocValuesProducer.java:<span class="number">781</span>)</span><br><span class="line">at org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat$FieldsReader.getSorted(PerFieldDocValuesFormat.java:<span class="number">329</span>)</span><br><span class="line">at org.apache.lucene.index.CodecReader.getSortedDocValues(CodecReader.java:<span class="number">157</span>)</span><br><span class="line">at org.apache.solr.uninverting.UninvertingReader.getSortedDocValues(UninvertingReader.java:<span class="number">360</span>)</span><br><span class="line">at org.apache.lucene.index.FilterLeafReader.getSortedDocValues(FilterLeafReader.java:<span class="number">378</span>)</span><br><span class="line">at org.apache.lucene.index.MultiDocValues.getSortedValues(MultiDocValues.java:<span class="number">566</span>)</span><br><span class="line">at org.apache.solr.index.SlowCompositeReaderWrapper.getSortedDocValues(SlowCompositeReaderWrapper.java:<span class="number">141</span>)</span><br><span class="line">at org.apache.solr.request.DocValuesFacets.getCounts(DocValuesFacets.java:<span class="number">83</span>)</span><br><span class="line">at org.apache.solr.request.SimpleFacets.getTermCounts(SimpleFacets.java:<span class="number">586</span>)</span><br><span class="line">at org.apache.solr.request.SimpleFacets.getTermCounts(SimpleFacets.java:<span class="number">426</span>)</span><br><span class="line">at org.apache.solr.request.SimpleFacets.lambda$getFacetFieldCounts$<span class="number">0</span>(SimpleFacets.java:<span class="number">826</span>)</span><br><span class="line">... <span class="number">37</span> more</span><br></pre></td></tr></table></figure></li><li><p>为什么会出现这中错误，以及如何解决<br>如果你认真阅读<strong>org.apache.lucene.codecs.lucene70.Lucene70DocValuesConsumer</strong>这个类的源代码，你会发现在每一个<strong>addXXXField</strong>方法(例如<strong>addBinaryField</strong>)方法都会有一个<strong>meta.writeInt(field.number);</strong>的调用，如:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addBinaryField</span><span class="params">(FieldInfo field, DocValuesProducer valuesProducer)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    meta.writeInt(field.number);</span><br><span class="line">    meta.writeByte(Lucene70DocValuesFormat.BINARY);</span><br><span class="line"></span><br><span class="line">    BinaryDocValues values = valuesProducer.getBinary(field);</span><br><span class="line">    <span class="keyword">long</span> start = data.getFilePointer();</span><br><span class="line">    meta.writeLong(start);</span><br><span class="line">...</span><br></pre></td></tr></table></figure></li></ul><p>其实这非常好解释，Lucene是<strong>面向列存储的</strong>，而每种类型的索引又都存储在一个文件中（优化索引后）,所以各个域需要有唯一标识符来表示，在Lucene中这个唯一标示符号就是<strong>field.number</strong>,所以在我们新生成的索引中，<strong>字段的field.number必须要和原始索引中的字段的field.number保持一致才行</strong>。</p><ul><li>为什么如果我们运气好的话，这个问题就不会出现?<br>上面讲到了，如果新生成的字段的field.number与原始索引中字段的field.number不一致的话，在试图读取数据的时候就会报<strong>java.lang.NullPointerException</strong>,那么如果字段一致是不是就不会报了呢？ 答案是:确实如此，例如如果我们的索引代码中solrinputDocument中添加字段的顺序为:<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">SolrInputDocument document = <span class="keyword">new</span> SolrInputDocument();</span><br><span class="line">document.setField(<span class="string">"id"</span>,dataArray[<span class="number">0</span>]); <span class="comment">//字段id 的field.number 为0 </span></span><br><span class="line">document.setField(<span class="string">"type"</span>,dataArray[<span class="number">1</span>]); <span class="comment">// 此时字段type的field.number 为1</span></span><br><span class="line">document.setField(<span class="string">"country"</span>,dataArray[<span class="number">3</span>]);</span><br><span class="line">document.setField(<span class="string">"date"</span>,Integer.valueOf(dataArray[<span class="number">4</span>].replaceAll(<span class="string">"-"</span>,<span class="string">""</span>)));</span><br><span class="line">document.setField(<span class="string">"abstract"</span>,dataArray[<span class="number">5</span>]);</span><br><span class="line">document.setField(<span class="string">"title"</span>,dataArray[<span class="number">6</span>]);</span><br><span class="line">document.setField(<span class="string">"kind"</span>,dataArray[<span class="number">7</span>]);</span><br></pre></td></tr></table></figure></li></ul><p>并且在生成新索引的时候Document中添加字段的顺序为:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Document doc = <span class="keyword">new</span> Document();</span><br><span class="line">doc.add(<span class="keyword">new</span> StringField(<span class="string">"id"</span>,String.valueOf(i), Field.Store.YES));</span><br><span class="line">doc.add(<span class="keyword">new</span> StringField(<span class="string">"type"</span>,......)) <span class="comment">// 字段type的field.number 为1</span></span><br></pre></td></tr></table></figure></p><p>那么这中场景就是我们说的lucky场景,原始索引和新生成的索引的field.number是一致的，程序不会有任何问题，但是如果我们修改了type字段在代码中出现的顺序那么程序就可能出错了，<strong>实际上Lucene的field.number的生成机制为: 按照字段出现的顺序为每个字段编号，从0到n，在多线程场景中，以优先处理的文档的字段顺序为标准</strong>（这也是为什么我们说如果我们非常非常lucky的话上面的错误我们就不会遇到了）,既然我们知道了原因那么解决方案也比较简单了</p><ul><li>如果解决此类错误<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// step 1, 获得原始索引中的某个字段的field.number</span></span><br><span class="line"><span class="keyword">int</span> number = sourceReader.leaves().get(<span class="number">0</span>).reader().getFieldInfos().fieldInfo(fieldName).number;</span><br><span class="line"><span class="comment">// step 2, 随机添加fake字段使得目标字段的field.number与原始字段一致</span></span><br><span class="line"><span class="keyword">if</span>(isFirstTime)&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> fieldNumber=<span class="number">1</span>;fieldNumber&lt;number;fieldNumber++)&#123;</span><br><span class="line">        doc.add( <span class="keyword">new</span> StringField(<span class="string">"xxx_"</span>+fieldNumber,<span class="string">"xxx_"</span>+fieldNumber , Field.Store.NO));</span><br><span class="line">    &#125;</span><br><span class="line">    isFirstTime =<span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//这里解释下为什么fieldNumber要从1开始，其实很简单，因为0被我们代码中的doc.add(new StringField("id",String.valueOf(i), Field.Store.YES)); id 字段使用了，这里的id和uniquekey没有任何关系，如果你把id字段写在添加fake字段的代码后面，那么fieldNumber从0开始也是对的</span></span><br></pre></td></tr></table></figure></li></ul><p>当我们补上这些代码之后我们重新生成并覆盖数据之后发现我们可以正常的查询或者统计目标字段了(这里就是type字段)。貌似我们成功了，哈哈，当然不是，如果你只是跟着这篇博客处理个几十条数据，我保证你一辈子都不会遇到下面的问题，但是如果你的索引很大，新索引的生成时间不是1s而是几分钟生成的时候，你就会发现这个坑爹的问题。</p><ul><li><p>最蛋疼的问题-&gt;数据错位<br>之前我们说过，我们之所以这么做的原因是因为Lucene的本质其实就是docid和term的关系，我们也是通过修改docid和term的关系是现实这种非官方的<strong>按字段更新</strong>,目前为止我们也通过代码生成了新的索引而且通过覆盖原始索引的方式实现的按照字段的更新，那么这里的数据错位打底是指为什么？我们先来回顾下docid是怎么生成的，翻看Lucene的源码你会发现，<strong>Lucene会为每一个接受到的文档赋予一个integer的id，而这个id就是docid</strong>,这里再强调以下，Lucene中并没有类似solr的UniqueKey这种东西，docid才是Lucene的唯一键，docid标识了每一个document(即使document的内容没有任何差别),而在实际应用中，我们几乎不会将docid放到业务中使用和存储—&gt; 因为docid会发生变化—&gt; ???? 说好的唯一键呢，唯一键都变了这是要闹哪样，尼玛坑爹中的战斗机啊。。。。。</p><ul><li>为什么唯一键会变?<br>了解Lucene的原理的人都知道，Lucene在写入数据的时候是写入内存的，当内存满了或者其他条件达到了，Lucene会将内存中的文件刷到磁盘上并称为一个segment，那么如果文档非常多的话，就会有非常多的段，所以Lucene又有一个merge的操作，类似于后台线程，不断将新生成的segment合并成一个新的大的段来保持段的数量在合理的范围，因为段太多会导致查询速度不断的降低。而docid的改变就发生在segment的合并中（具体个变化和merge policy有关，我会在另外一篇文章中详细介绍）</li><li>唯一键变了会有什么影响<br>注意到新生成的索引文件是按照docid的顺序读取的，而且是单线程的，也就是说我们通过这种方式来使得新加入的数据的docid和原始的docid保持一致，而达到数据修改和替换的目的，但是由于Lucene并没有在api成面给我们设置或修改docid，所以我们只能在添加文档的顺序上来模拟内部的docid的顺序从而保证我们的机制能够运行，但是我们忘记了一个重要的索引生成过程—&gt; segment的合并过程，Lucene 7默认的合并策略是<strong>TieredMergePolicy</strong>,而这个合并策略并不能保证<strong>相邻的段会合并</strong> (如果不是相邻的段合并，那么docid可能就被在合并的过程中被改变),所以唯一键的改变会导致成<strong>数据错位</strong> </li><li><p>如何解决数据错位的问题<br>既然我们知道了原因，那么解决方案就比较简单，Lucene中的<strong>LogByteSizeMergePolicy</strong>可以保证永远都是合并相邻的segments，具体为什么是LogByteSizeMergePolicy,我会在后续的文章中给出解释，目前大家可以记住LogByteSizeMergePolicy可以保证段的合并是相邻的进行合并,代码改动点则为:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">IndexWriterConfig writerConfig = <span class="keyword">new</span> IndexWriterConfig();</span><br><span class="line">writerConfig.setUseCompoundFile(<span class="keyword">false</span>);</span><br><span class="line">writerConfig.setCodec(customCodec);</span><br><span class="line">writerConfig.setOpenMode(IndexWriterConfig.OpenMode.CREATE);</span><br><span class="line"><span class="comment">/*we need use LogMergePolicy since we need keep the doc Order */</span></span><br><span class="line">writerConfig.setMergePolicy(<span class="keyword">new</span> LogByteSizeMergePolicy());</span><br></pre></td></tr></table></figure><p>到这里我们才完成了所有的改动点，也使用了这种黑科技的方法做到了solr按照字段更新。</p></li></ul></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><font color="#2C3E50">总结</font></h2><ul><li>codec是Lucene用开控制索引读写的一层api，如果我们要自定义索引的格式，我们可以通过自定义的codec来实现</li><li>这里介绍的<strong>Solr按字段更新数据</strong>并不是solr官方的方式，这其实是一种黑科技，我们先在来梳理下这个黑科技是如何想到的，并且如何一步一步的解决其中的坑的<ul><li>我们想出这种类似<strong>移花接木</strong>的方案的根本来源是因为Lucene的索引文件的本质就是存储了docid和term的映射关系</li><li>刚开始的时候我们什么指纹信息都没有改动，直接使用新索引覆盖原始索引，这才发现了原来lucene写入了这些指纹信息来方式索引文件发生损坏，其实这里少了一步，最早的时候笔者是使用<strong>SimpleTextCodecFactory</strong>来测试的，而这个codec不会写入任何指纹信息，所以刚开始直接覆盖没有任何问题，直接成功了，这也是为什么后来再改动codec之后上来就直接使用文件覆盖的方式来验证想法的原因，虽然这是最笨的方式，确实能够暴露问题</li><li>field.number的信息也不算隐蔽，但是当我们hack了指纹信息以为万事大吉的时候，最终的查询还是告诉我们，还有信息没有注意到。</li><li>Merge的问题确实是最难发现的，如果数据容量不到一定规模，根本不会发现问题，因为测试的时候笔者使用的数量很少，这也是当时困扰笔者最长的时间的一个细节点，很隐蔽，但是经过大量的源码debug还是发现了问题所在并且成功了解决了现实业务中的问题。</li></ul></li><li>由于篇幅关系，笔者没有直接复制粘贴所有的代码，而是希望通过各种关节点的描述来分享给问题是什么，笔者是如何想到解决方案的，在实施解决方案中的关键问题是什么，笔者相信，知道为什么比知道怎么做更加重要。</li><li>所有的代码可以在<a href="https://github.com/fengqingleiyue/apache-solr-plugins/tree/master/customized-codec" target="_blank" rel="noopener">customized-codec</a>找到，可以使用<a href="https://github.com/fengqingleiyue/apache-solr-plugins/blob/master/customized-codec/src/main/resources/demo/demo.sh" target="_blank" rel="noopener">demo.sh</a>来进行本地环境的复现。这里给一些简单的说明<ul><li>测试的数据是<a href="http://s3.amazonaws.com/data.patentsview.org/20191008/download/patent.tsv.zip" target="_blank" rel="noopener">patent.tsv.zip</a>总条数大概是7百万条</li><li>测试在centos环境中验证通过，其他环境欢迎大家尝试</li><li>为了代码简单，我这里的修改只是将原始索引的某个字段(type字段)的值修改成了另外一个值(将utility改成了fql_demo)</li><li>本篇博客中只描述了DocValuesFormat的hack方式，其实solr中的PostingsFormat也可以按照相同的方式进行hack，笔者给的例子就是同时自定了DocValuesFormat和PostingsFormat。基本上这两个Format解决了查询和分析的需求。<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a><font color="#2C3E50">参考文章</font></h2></li></ul></li><li><a href="https://www.elastic.co/blog/what-is-an-apache-lucene-codec" target="_blank" rel="noopener">https://www.elastic.co/blog/what-is-an-apache-lucene-codec</a></li><li><a href="https://en.wikipedia.org/wiki/Service_provider_interface" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Service_provider_interface</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;小试-Solr-Lucene-Codec&quot;&gt;&lt;a href=&quot;#小试-Solr-Lucene-Codec&quot; class=&quot;headerlink&quot; title=&quot;小试 Solr/Lucene Codec&quot;&gt;&lt;/a&gt;&lt;font color=&quot;#0077bb&quot;&gt;小试 Solr/Lucene Codec&lt;/font&gt;&lt;/h1&gt;&lt;h2 id=&quot;Codec-是什么&quot;&gt;&lt;a href=&quot;#Codec-是什么&quot; class=&quot;headerlink&quot; title=&quot;Codec 是什么&quot;&gt;&lt;/a&gt;&lt;font color=&quot;#2C3E50&quot;&gt;Codec 是什么&lt;/font&gt;&lt;/h2&gt;&lt;p&gt;Lucene 从4.0版本开始提供codec api，简单的来说Lucene通过codec机制来读写索引文件，说白了就是一层数据访问层的api，正常来说我们在使用Lucene/Solr的时候是不用关心这一层细节的，因为默认的codec实现已经经过了大量的细节优化，但是如果你需要修改索引的存储方式，那么codec就是你的入手之处。&lt;br&gt;&lt;strong&gt;因为这里涉及到很多细节，希望大家先不用关心代码细节，而是关注具体的流程和原理，最后对照代码来理解这篇博客,当然如果你还有不明白的地方，欢迎邮件我fengqingleiyue@163.com&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;自定义Codec&quot;&gt;&lt;a href=&quot;#自定义Codec&quot; class=&quot;headerlink&quot; title=&quot;自定义Codec&quot;&gt;&lt;/a&gt;&lt;font color=&quot;#2C3E50&quot;&gt;自定义Codec&lt;/font&gt;&lt;/h2&gt;&lt;p&gt;既然我们知道了codec是什么，那么作为程序员的我们，就会想到-&amp;gt; 我们怎么自定义一个codec呢?&lt;br&gt;
    
    </summary>
    
    
      <category term="Solr" scheme="https://www.fengqinglei.top/tags/Solr/"/>
    
      <category term="Lucene" scheme="https://www.fengqinglei.top/tags/Lucene/"/>
    
      <category term="codec" scheme="https://www.fengqinglei.top/tags/codec/"/>
    
      <category term="hacking" scheme="https://www.fengqinglei.top/tags/hacking/"/>
    
  </entry>
  
  <entry>
    <title>Text Retrieval and Search Engines 4</title>
    <link href="https://www.fengqinglei.top/2019/10/28/text-retrieval-and-search-engines-4/"/>
    <id>https://www.fengqinglei.top/2019/10/28/text-retrieval-and-search-engines-4/</id>
    <published>2019-10-28T13:46:03.000Z</published>
    <updated>2019-11-03T14:23:41.831Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Text-Retrieval-and-Search-Engines学习笔记-四"><a href="#Text-Retrieval-and-Search-Engines学习笔记-四" class="headerlink" title="Text Retrieval and Search Engines学习笔记(四)"></a><font color="#0077bb">Text Retrieval and Search Engines学习笔记(四)</font></h1><p>这篇文章是接着上一篇<a href="https://www.fengqinglei.top/2019/10/17/text-retrieval-and-search-engines-3/">Text Retrieval and Search Engines学习笔记(三)</a> ，这一篇我们介如何绍下实现<strong>Vector Space Model</strong>所需要的一些步骤和流程。</p><h2 id="System-Implementation"><a href="#System-Implementation" class="headerlink" title="System Implementation"></a><font color="#0077bb">System Implementation</font></h2><p>前面我们介绍了<strong>Vector Space Model</strong>的一种实现-&gt; <strong>BM25</strong>,虽然我们从数学上已经定义好了rank的函数，但是我们还得想办法进行高效的计算rank函数，因为有的时候我们会在我们的搜索引擎中存放千万甚至几十亿的数据,为了让用户在几百毫秒的时间内完成一次检索，我们必须非常高效的计算rank函数。<br><a id="more"></a></p><h3 id="一个典型的信息检索系统的架构"><a href="#一个典型的信息检索系统的架构" class="headerlink" title="一个典型的信息检索系统的架构"></a><font color="#0077bb">一个典型的信息检索系统的架构</font></h3><p><a href="https://imgchr.com/i/KgB2z6" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/10/28/KgB2z6.md.png" alt="KgB2z6.md.png"></a><br>当然刚开始的信息检索系统并不是这个样子，很久以前，信息检索系统并没有这么复杂，因为网上的数据还没有今天这么庞大，在那个软盘只有几十kb的年代，所谓的搜索引擎真的就是使用用户的关键字一篇一篇的找，可以是正则表达式也可以是普通的字符串查找，但是随着网页上的数据越来越多，这种一篇一篇查找的方式已经不能满足用户对检索的实时响应性，所以google的创始人搞出了<strong>Invert Index</strong>,为啥叫<strong>Invert Index</strong>,因为以前是以文档为主键，遍历文档进行关键字查找，而<strong>Invert Index</strong>是以关键词为主键，通过关键词找到对应的文档，其实这也是正常的用户场景，我们在google检索的时候，大部分情况下输入都是很短的。既然是对关键词，所以才会有<strong>Tokenization</strong>的概念.</p><h4 id="分词"><a href="#分词" class="headerlink" title="分词"></a><font color="#0077bb">分词</font></h4><ul><li>同义词处理，相同意思的词应该映射到同一个词上</li><li>词干提取(stemming)和词形还原(lemmatization) </li><li>关键字提取，英文可能比较简单，直接按照空格切分就行了，但是类似于中午我们需要对关键词进行一次切分</li><li>…</li></ul><h4 id="索引"><a href="#索引" class="headerlink" title="索引"></a><font color="#0077bb">索引</font></h4><ul><li>索引就是将原始的文档转换成可以支持快速检索的数据结构，说白了就是预计算</li><li>倒排索引就是为了支持计算算法所设计的一种索引方式</li><li>。。。</li></ul><h4 id="倒排索引"><a href="#倒排索引" class="headerlink" title="倒排索引"></a><font color="#0077bb">倒排索引</font></h4><p>下面我们重点介绍下倒排索引，倒排索引是一种数据结构，通过这种数据结构我们可以快速进行排序算法的计算,下面是倒排索引的一个例子:<br><a href="https://imgchr.com/i/KgRQsI" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/10/28/KgRQsI.md.png" alt="KgRQsI.md.png"></a><br>这里我们可以通过这个例子清楚的理解<strong>正排索引</strong>和<strong>倒排索引</strong>的区别</p><div class="table-container"><table><thead><tr><th>-/-</th><th>倒排索引</th><th>正排索引 </th></tr></thead><tbody><tr><td>特点</td><td>记录某个关键字在哪些文档中出现和出现的次数</td><td>记录某个文档中出现哪些关键词</td></tr><tr><td>使用方式</td><td>构建一次，多次使用</td><td>不需要构建，每次scan 就行了 </td></tr></tbody></table></div><p>并且倒排索引在搜索引擎中常见的term检索，bool检索的效率都远远高于正排索引，这是因为倒排索引采用了预计算的方式，对每个关键词出现的文档编号都做了记录，用的时候取出来就行了，即使对于稍微复杂的bool检索，也只需要对文档集合做对应的交、并、补等集合操作即可。那么倒排索引中到底有哪一些数据结构来保证我们能够快速获得计算rank函数的参数呢？</p><ul><li>字典:正常来说字典的大小不会特别大，虽然中国汉字有非常多，常用的也就3500个，也就是说字典的大小由于人类使用自然语言的方式会是一可控的大小<ul><li>我们需要对字典进行随机读取</li><li>一般来说为了高效率，字典我们都选择放在内存中</li><li>可以使用hash表 ，B-树,前缀树等方式实现</li></ul></li><li>Postings(倒排表): 一般来说这个很大，因为文章的关键词数量比较多，由于有位置信息，所以即使相同的关键词由于出现的位置不一样，在存储上这些空间是没法像字典一样节约下来的。<ul><li>期望是通过顺序读写（太大了不能放到内存中）</li><li>可以放在磁盘上</li><li>一般会包含文档编号，词频，词的位置等信息</li><li>由于会很大所以需要进行数据的压缩<br>那么到底倒排索引是怎么生成的呢?</li></ul></li></ul><h4 id="如何生成倒排索引"><a href="#如何生成倒排索引" class="headerlink" title="如何生成倒排索引"></a><font color="#0077bb">如何生成倒排索引</font></h4><ul><li>第一步: 针对每个文档生成三元组 (termID,docID,freq)</li><li>第二步: 按照termID 对三元组进行排序 </li><li>第三步: 根据termID 进行docID,freq 合并</li><li>第四步: 输出倒排文件<br>如下图所示:<br><a href="https://imgchr.com/i/Kj18MQ" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/11/03/Kj18MQ.md.png" alt="Kj18MQ.md.png"></a><br>实际上生成倒排文件并不是那么困难，最难的是如何使用有限的内存生成倒排文件，因为往往我们真实场景中文档可能很多很大，就算是TB内存级别的机器可能也无法在内存中一次性生成倒排文件，但是我们可以生成很多倒排文件，然后对这些文件在内存中合并(因为文件的内容都是排好顺序的，所以我们不必将倒排文件全部读入到内存中进行合并))，这也是lucene生成索引的步骤，在lucene中可以生成很多很多的段(segment), lucene在后台使用单独的线程对这些段进行合并来避免段太多导致的查询性能低下的问题。</li></ul><h4 id="索引压缩"><a href="#索引压缩" class="headerlink" title="索引压缩"></a><font color="#0077bb">索引压缩</font></h4><p>为了节约硬件成本,我们需要对索引进行压缩，这里举个例子来说明为什么我们需要对索引进行压缩，假设我们有1亿条文档需要进行压缩，这里文档编号我们使用int类型存储(占用4个字节),那么为了存储这一亿个文档编号我们需要100000000<em>4/1024/1024=381.4MB的内存，但是如果我对这1亿个文档编号使用bitset来存储的话大约需要100000000</em>4/32/1024/1024=11.9MB就够了，可以对文档编号压缩可以大大降低存储所消耗的资源。这里我们介绍几种常见的整型压缩算法。</p><ul><li>biset: bitset可以简单的理解为使用<strong>位信息</strong>来存储integer，例如正常情况下一个整型就占用了4个字节(32位),如果我们只是为了表示一个整型的集合(不管顺序),那么其实我们可以使用位信息来存储，每个位表示一个数，32位表示0～31这32个数字。同理表示0～63需要2个整型(整型数组)就行了</li><li>差值压缩: 想象下我们需要存储每个term的位置信息(起始和结束的位置)，那么当文档很长的时候表示位置的数字可能比较大，例如这篇文档有1万个字符，那么位置信息就可能是9000,但是因为位置信息都是连续的，所以前后位置信息的差值会很小，以中文为例子，前一个汉字和后一个汉字的差距永远都是1(即使文档的长度很长很长)，存储9000至少需要10个bit,但是存储1只需要1个bit就行了，当文档很长而且文档非常多的时候这样的压缩算法将会节约大量的存储空间。</li></ul><h4 id="倒排索引如何加速打分公式"><a href="#倒排索引如何加速打分公式" class="headerlink" title="倒排索引如何加速打分公式"></a><font color="#0077bb">倒排索引如何加速打分公式</font></h4><p>我们在之前的文章中描述过BM25的打分公式:<br>f(q,d) = $\sum_{w \in q \cap d}$ [C(w,q) $\frac{(k+1)C(w,d)}{C(w,d)+k(1-b+b \frac {d}{avdl})}$log$\frac{M+1}{df(w)}$)]</p><ul><li>C(w,d) 表示w在文档d中出现的次数，从上面的倒排索引生成的过程中，我们知道词频信息已经在生成索引的时候被我们写入到倒排表中</li><li>avdl 平均文档长度，我们在生成倒排表的时候会遍历所有的文档，我们可以存储每个文档的长度，在进行打分的时候通过每个文档的长度计算出平均文档长度</li><li>df(w) 文档频率,表示有多少文档出现w，虽然我们但在生成倒排表的时候没有直接将这个数字写入到倒排文件中，但是我们记录了每个w出现的文档编号，统计文档编号的个数，我们可以直接得到df(w)<br>到这里也就说: 基本上BM25打分公式中用于计算最终文档的分数的算子，我们基本上已经在倒排索引中进行的直接或者间接的存储，那么当我们查询的结果有几十万或者几百万的文档时，我们依然可以在几百或者几十毫秒对命中到的文档进行排序输出。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Text-Retrieval-and-Search-Engines学习笔记-四&quot;&gt;&lt;a href=&quot;#Text-Retrieval-and-Search-Engines学习笔记-四&quot; class=&quot;headerlink&quot; title=&quot;Text Retrieval and Search Engines学习笔记(四)&quot;&gt;&lt;/a&gt;&lt;font color=&quot;#0077bb&quot;&gt;Text Retrieval and Search Engines学习笔记(四)&lt;/font&gt;&lt;/h1&gt;&lt;p&gt;这篇文章是接着上一篇&lt;a href=&quot;https://www.fengqinglei.top/2019/10/17/text-retrieval-and-search-engines-3/&quot;&gt;Text Retrieval and Search Engines学习笔记(三)&lt;/a&gt; ，这一篇我们介如何绍下实现&lt;strong&gt;Vector Space Model&lt;/strong&gt;所需要的一些步骤和流程。&lt;/p&gt;
&lt;h2 id=&quot;System-Implementation&quot;&gt;&lt;a href=&quot;#System-Implementation&quot; class=&quot;headerlink&quot; title=&quot;System Implementation&quot;&gt;&lt;/a&gt;&lt;font color=&quot;#0077bb&quot;&gt;System Implementation&lt;/font&gt;&lt;/h2&gt;&lt;p&gt;前面我们介绍了&lt;strong&gt;Vector Space Model&lt;/strong&gt;的一种实现-&amp;gt; &lt;strong&gt;BM25&lt;/strong&gt;,虽然我们从数学上已经定义好了rank的函数，但是我们还得想办法进行高效的计算rank函数，因为有的时候我们会在我们的搜索引擎中存放千万甚至几十亿的数据,为了让用户在几百毫秒的时间内完成一次检索，我们必须非常高效的计算rank函数。&lt;br&gt;
    
    </summary>
    
    
      <category term="NLP" scheme="https://www.fengqinglei.top/tags/NLP/"/>
    
      <category term="Text Retrieval" scheme="https://www.fengqinglei.top/tags/Text-Retrieval/"/>
    
      <category term="Vector Space Model" scheme="https://www.fengqinglei.top/tags/Vector-Space-Model/"/>
    
      <category term="Search Engine" scheme="https://www.fengqinglei.top/tags/Search-Engine/"/>
    
      <category term="Invert Index" scheme="https://www.fengqinglei.top/tags/Invert-Index/"/>
    
      <category term="Compression" scheme="https://www.fengqinglei.top/tags/Compression/"/>
    
      <category term="postings list" scheme="https://www.fengqinglei.top/tags/postings-list/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch docker volume permission issue</title>
    <link href="https://www.fengqinglei.top/2019/10/27/elasticsearch-docker-install/"/>
    <id>https://www.fengqinglei.top/2019/10/27/elasticsearch-docker-install/</id>
    <published>2019-10-26T16:23:37.000Z</published>
    <updated>2019-10-27T13:38:55.450Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Elasticsearch之docker安装"><a href="#Elasticsearch之docker安装" class="headerlink" title="Elasticsearch之docker安装"></a><font color="#0077bb">Elasticsearch之docker安装</font></h1><p>最近由于工作原因，需要倒腾下<strong>Elasticsearch</strong> ,虽然本人的工作中，大部分情况下使用的都是<strong>Solr</strong>,但是这并不能挡住我学习Elasticsearch的激情，这里我并不做这两个工具的好坏，只要用的顺手用的舒服就是好工具。这里我以docker安装为例子。</p><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a><font color="#0077bb">环境准备</font></h2><p>笔者使用的环境为<strong>华为云</strong>上的一台虚拟机,这里简单的描述下<strong>docker</strong>和<strong>docker-compose</strong>的安装<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum -y install docker docker-compose </span><br><span class="line"><span class="comment">#具体的版本为: docker-compose-1.18.0-4.el7.noarch , docker-1.13.1-103.git7f2769b.el7.centos.x86_64</span></span><br></pre></td></tr></table></figure></p><p>docker 安装elasticsearch 还是比较简单的,我们直接参考<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html就行了" target="_blank" rel="noopener">官方文档</a>，这里我们使用的版本号码是<strong>7.4.1</strong>。大部分情况下，如果你是为了体验elasticsearch，或者搭建一个简单的测试环境，那么elasticsearch的官方文档应该就能解决这个问题,但是由于笔者需要存储大概几十万的数据进去，而且由于使用的是云上的环境，一般来说系统盘的小大都很小（常见的值为8GB），所以必须将elasticsearch的data目录挂载到数据盘上。而问题就出现在docker的目录映射上。<br><a id="more"></a></p><h2 id="AccessDeniedException"><a href="#AccessDeniedException" class="headerlink" title="AccessDeniedException"></a><font color="#0077bb">AccessDeniedException</font></h2><p>上面提过，笔者大部分情况下使用的是Solr,也自定了Solr的docker，在Solr中(目前使用的版本是6.5.1),如果需要将索引数据挂载出来，只要在docker-compose文件中直接做volume映射就行了，例如<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">   volumes:</span><br><span class="line">    - system_data_path:inside_docker_solr_index_data_path</span><br><span class="line">..</span><br></pre></td></tr></table></figure></p><p>同样的道理，我直接修改了docker-compose文件，将instance上的另外一块磁盘上的某个目录映射到docker中的/usr/share/elasticsearch/data/,这里直接上笔者的docker-compose文件<br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## docker-compose.yml</span></span><br><span class="line"><span class="attr">version:</span> <span class="string">'3'</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line"><span class="attr"> elasticsearch:</span></span><br><span class="line"><span class="attr">   image:</span> <span class="string">'docker.elastic.co/elasticsearch/elasticsearch:7.4.1'</span></span><br><span class="line"><span class="attr">   ports:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="number">6200</span><span class="string">:9200</span></span><br><span class="line"><span class="bullet">    -</span> <span class="number">6300</span><span class="string">:9300</span></span><br><span class="line"><span class="attr">   env_file:</span> <span class="string">elasticsearch_env</span></span><br><span class="line"><span class="attr">   ulimits:</span></span><br><span class="line"><span class="attr">     memlock:</span></span><br><span class="line"><span class="attr">       soft:</span> <span class="bullet">-1</span></span><br><span class="line"><span class="attr">       hard:</span> <span class="bullet">-1</span></span><br><span class="line"><span class="attr">     nofile:</span></span><br><span class="line"><span class="attr">       soft:</span> <span class="number">65535</span></span><br><span class="line"><span class="attr">       hard:</span> <span class="number">65535</span></span><br><span class="line"><span class="attr">   volumes:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">./tmp:/usr/share/elasticsearch/data/</span></span><br><span class="line"><span class="attr">   privileged:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment">## elasticsearch_env </span></span><br><span class="line"><span class="string">discovery.type=single-node</span></span><br><span class="line"><span class="string">cluster.name=test</span></span><br><span class="line"><span class="string">node.name=node-01</span></span><br><span class="line"><span class="string">bootstrap.memory_lock=true</span></span><br></pre></td></tr></table></figure></p><p>但是当我直接<strong>docker-compose up -d</strong>之后，缺发现container无法启动，查下了日志具体的错误为<strong>org.elasticsearch.bootstrap.StartupException: ElasticsearchException[failed to bind service]; nested: AccessDeniedException</strong>,貌似是没有权限???<br>完整的错误为:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">OpenJDK <span class="number">64</span>-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version <span class="number">9.0</span> and will likely be removed in a future release.</span><br><span class="line">&#123;<span class="string">"type"</span>: <span class="string">"server"</span>, <span class="string">"timestamp"</span>: <span class="string">"2019-10-27T03:27:37,816Z"</span>, <span class="string">"level"</span>: <span class="string">"WARN"</span>, <span class="string">"component"</span>: <span class="string">"o.e.b.ElasticsearchUncaughtExceptionHandler"</span>, <span class="string">"cluster.name"</span>: <span class="string">"test"</span>, <span class="string">"node.name"</span>: <span class="string">"node-01"</span>, <span class="string">"message"</span>: <span class="string">"uncaught exception in thread [main]"</span>,</span><br><span class="line"><span class="string">"stacktrace"</span>: [<span class="string">"org.elasticsearch.bootstrap.StartupException: ElasticsearchException[failed to bind service]; nested: AccessDeniedException[/usr/share/elasticsearch/data/nodes];"</span>,</span><br><span class="line"><span class="string">"at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:163) ~[elasticsearch-7.4.1.jar:7.4.1]"</span>,</span><br><span class="line"><span class="string">"at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:150) ~[elasticsearch-7.4.1.jar:7.4.1]"</span>,</span><br><span class="line"><span class="string">"at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) ~[elasticsearch-7.4.1.jar:7.4.1]"</span>,</span><br><span class="line"><span class="string">"at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:125) ~[elasticsearch-cli-7.4.1.jar:7.4.1]"</span>,</span><br><span class="line"><span class="string">"at org.elasticsearch.cli.Command.main(Command.java:90) ~[elasticsearch-cli-7.4.1.jar:7.4.1]"</span>,</span><br><span class="line"><span class="string">"at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:115) ~[elasticsearch-7.4.1.jar:7.4.1]"</span>,</span><br><span class="line"><span class="string">"at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:92) ~[elasticsearch-7.4.1.jar:7.4.1]"</span>,</span><br><span class="line"><span class="string">"Caused by: org.elasticsearch.ElasticsearchException: failed to bind service"</span>,</span><br><span class="line"><span class="string">"at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:614) ~[elasticsearch-7.4.1.jar:7.4.1]"</span>,</span><br><span class="line"><span class="string">"at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:255) ~[elasticsearch-7.4.1.jar:7.4.1]"</span>,</span><br><span class="line"><span class="string">"at org.elasticsearch.bootstrap.Bootstrap$5.&lt;init&gt;(Bootstrap.java:221) ~[elasticsearch-7.4.1.jar:7.4.1]"</span>,</span><br><span class="line"><span class="string">"at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:221) ~[elasticsearch-7.4.1.jar:7.4.1]"</span>,</span><br><span class="line"><span class="string">"at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:349) ~[elasticsearch-7.4.1.jar:7.4.1]"</span>,</span><br><span class="line"><span class="string">"at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:159) ~[elasticsearch-7.4.1.jar:7.4.1]"</span>,</span><br><span class="line"><span class="string">"... 6 more"</span>,</span><br><span class="line"><span class="string">"Caused by: java.nio.file.AccessDeniedException: /usr/share/elasticsearch/data/nodes"</span>,</span><br><span class="line"><span class="string">"at sun.nio.fs.UnixException.translateToIOException(UnixException.java:90) ~[?:?]"</span>,</span><br><span class="line"><span class="string">"at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111) ~[?:?]"</span>,</span><br><span class="line"><span class="string">"at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116) ~[?:?]"</span>,</span><br><span class="line"><span class="string">"at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:389) ~[?:?]"</span>,</span><br><span class="line"><span class="string">"at java.nio.file.Files.createDirectory(Files.java:693) ~[?:?]"</span>,</span><br><span class="line"><span class="string">"at java.nio.file.Files.createAndCheckIsDirectory(Files.java:800) ~[?:?]"</span>,</span><br><span class="line"><span class="string">"at java.nio.file.Files.createDirectories(Files.java:786) ~[?:?]"</span>,</span><br><span class="line"><span class="string">"at org.elasticsearch.env.NodeEnvironment.lambda$new$0(NodeEnvironment.java:272) ~[elasticsearch-7.4.1.jar:7.4.1]"</span>,</span><br><span class="line"><span class="string">"at org.elasticsearch.env.NodeEnvironment$NodeLock.&lt;init&gt;(NodeEnvironment.java:209) ~[elasticsearch-7.4.1.jar:7.4.1]"</span>,</span><br><span class="line"><span class="string">"at org.elasticsearch.env.NodeEnvironment.&lt;init&gt;(NodeEnvironment.java:269) ~[elasticsearch-7.4.1.jar:7.4.1]"</span>,</span><br><span class="line"><span class="string">"at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:275) ~[elasticsearch-7.4.1.jar:7.4.1]"</span>,</span><br><span class="line"><span class="string">"at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:255) ~[elasticsearch-7.4.1.jar:7.4.1]"</span>,</span><br><span class="line"><span class="string">"at org.elasticsearch.bootstrap.Bootstrap$5.&lt;init&gt;(Bootstrap.java:221) ~[elasticsearch-7.4.1.jar:7.4.1]"</span>,</span><br><span class="line"><span class="string">"at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:221) ~[elasticsearch-7.4.1.jar:7.4.1]"</span>,</span><br><span class="line"><span class="string">"at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:349) ~[elasticsearch-7.4.1.jar:7.4.1]"</span>,</span><br><span class="line"><span class="string">"at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:159) ~[elasticsearch-7.4.1.jar:7.4.1]"</span>,</span><br><span class="line"><span class="string">"... 6 more"</span>] &#125;</span><br></pre></td></tr></table></figure></p><p>这里想到之前在没有使用docker安装elasticsearch的时候elasticsearch默认是不使用root用户进行启动的，这应该就是root cause,一顿google之后发现解决方案大致有两种</p><blockquote><ul><li>将elasticsearch用户加入到root组<br>因为需要重新打docker，比较蛋疼，这里直接放上解决方案的链接<a href="https://github.com/elastic/elasticsearch-docker/issues/49" target="_blank" rel="noopener">https://github.com/elastic/elasticsearch-docker/issues/49</a></li><li>使用docker volume来解决权限的问题<br>在官方的给出的docker-compose中有个 <strong>driver: local</strong>,使用这个确实elasticsearch就可以启动了，是应为这里的<strong>local</strong>是指docker安装的磁盘，如果docker安装所有的磁盘空间足够大，那么确实没啥问题，但是一般来说各种云的虚拟主机的系统盘默认也就8GB,确实不太够用。这里也是经过一番google之后直接找到了解决方案(直接上docker-compose的内容)<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">'3'</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line"><span class="attr"> elasticsearch:</span></span><br><span class="line"><span class="attr">   image:</span> <span class="string">'docker.elastic.co/elasticsearch/elasticsearch:7.4.1'</span></span><br><span class="line"><span class="attr">   ports:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="number">9200</span><span class="string">:9200</span></span><br><span class="line"><span class="bullet">    -</span> <span class="number">9300</span><span class="string">:9300</span></span><br><span class="line"><span class="attr">   env_file:</span> <span class="string">elasticsearch_env</span></span><br><span class="line"><span class="attr">   ulimits:</span></span><br><span class="line"><span class="attr">     memlock:</span></span><br><span class="line"><span class="attr">       soft:</span> <span class="bullet">-1</span></span><br><span class="line"><span class="attr">       hard:</span> <span class="bullet">-1</span></span><br><span class="line"><span class="attr">     nofile:</span></span><br><span class="line"><span class="attr">       soft:</span> <span class="number">65535</span></span><br><span class="line"><span class="attr">       hard:</span> <span class="number">65535</span></span><br><span class="line"><span class="attr">   volumes:</span></span><br><span class="line"><span class="attr">    - esdata:</span><span class="string">/usr/share/elasticsearch/data/</span></span><br><span class="line"><span class="attr">    - eslog:</span><span class="string">/usr/share/elasticsearch/logs/</span></span><br><span class="line"><span class="attr">   privileged:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="attr">volumes:</span></span><br><span class="line"><span class="attr">  esdata:</span></span><br><span class="line"><span class="attr">    driver:</span> <span class="string">local</span></span><br><span class="line"><span class="attr">    driver_opts:</span></span><br><span class="line"><span class="attr">      device:</span> <span class="string">/data/elastic_search/index_data/</span> <span class="comment"># 挂载日志目录到/data下(/data是数据盘)</span></span><br><span class="line"><span class="attr">      o:</span> <span class="string">bind</span></span><br><span class="line"><span class="attr">  eslog:</span></span><br><span class="line"><span class="attr">    driver:</span> <span class="string">local</span></span><br><span class="line"><span class="attr">    driver_opts:</span></span><br><span class="line"><span class="attr">      device:</span> <span class="string">/data/elastic_search/logs/</span>  <span class="comment"># 挂载日志目录到/data下(/data是数据盘)</span></span><br><span class="line"><span class="attr">      o:</span> <span class="string">bind</span></span><br></pre></td></tr></table></figure></li></ul></blockquote><p>修改完之后就启动成功了。如果想看docker中有哪些volume可以通过<strong>docker volume list</strong>来查看<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># docker volume list</span></span><br><span class="line">DRIVER              VOLUME NAME</span><br><span class="line"><span class="built_in">local</span>               elasticsearch_esdata</span><br><span class="line"><span class="built_in">local</span>               elasticsearch_eslog</span><br></pre></td></tr></table></figure></p><h1 id="关于docker的小技巧"><a href="#关于docker的小技巧" class="headerlink" title="关于docker的小技巧"></a><font color="#0077bb">关于docker的小技巧</font></h1><ul><li>如何加快国内docker pull的速度</li></ul><p>由于<strong>docker hub</strong>的服务器在海外，所以直接从docker hub上拉取一些docker大家会发现巨慢无比，往往拉取一些镜像需要等好久，这里给大家推荐<a href="https://www.daocloud.io/mirror" target="_blank" rel="noopener">Daocloud</a>,配置起来也非常简单:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://f1361db2.m.daocloud.io</span><br></pre></td></tr></table></figure></p><p>如果执行失败的话直接修改/etc/docker/daemon.json，加入<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"registry-mirrors"</span>: [<span class="string">"http://f1361db2.m.daocloud.io"</span>]&#125;</span><br></pre></td></tr></table></figure></p><p>重启docker服务之后再拉取即可，笔者尝试了之后，发现速度确实比之前快很多</p><ul><li>如何使用非root用户玩转docker</li></ul><p>很多时候我们在服务器或者个人的开发电脑上，都不是root用户（个人的测试机器除外），而且有的时候公司的运维由于安全问题也不愿意给开发root权限，那么是不是玩转docker一定要root权限呢？答案是不用,具体的原理也很简单，就是把某个非root用户加入到docker对应的用户组中，这里以centos 7 为例子:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 例如当前的xxx user用户没有指定docker命令的权限</span></span><br><span class="line">[xxx user]$ docker ps -a</span><br><span class="line">Got permission denied <span class="keyword">while</span> trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.26/containers/json?all=1: dial unix /var/run/docker.sock: connect: permission denied</span><br></pre></td></tr></table></figure></p><p>1)在root在执行下列命令，将当前用户加入到docker对应的用户组中<br>(可以通过cat /etc/group | grep docker命令查看具体的docker对应的用户组)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">usermod -aG dockerroot &lt;user_name&gt;</span><br></pre></td></tr></table></figure></p><p>2)添加dockerroot用户组信息到/etc/docker/daemon.json中（root在执行）<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">   <span class="attr">"group"</span>: <span class="string">"dockerroot"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>3)重启docker服务(在root下执行)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root<span class="comment">#service docker restart </span></span><br><span class="line">Redirecting to /bin/systemctl restart docker.service</span><br></pre></td></tr></table></figure></p><p>4) 在<user_ame>下重新执行docker命令后，就会发现非root用户也可以执行docker了<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;user_name&gt;$ docker ps -a</span><br><span class="line">CONTAINER ID        IMAGE                                                                                      COMMAND                  CREATED             STATUS                        PORTS                     NAMES</span><br><span class="line">595d4fc7057b        local-dtr.zhihuiya.com/base/ant-maven:latest                                               &quot;bash&quot;                   4 weeks ago         Exited (0)</span><br></pre></td></tr></table></figure></user_ame></p><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a><font color="#0077bb">参考文章</font></h2><p><a href="http://www.softpanorama.org/VM/Docker/Installation/rhel7_docker_package_dockerroot_problem.shtml" target="_blank" rel="noopener">http://www.softpanorama.org/VM/Docker/Installation/rhel7_docker_package_dockerroot_problem.shtml</a><br><a href="https://ywnz.com/linuxjc/4487.html" target="_blank" rel="noopener">https://ywnz.com/linuxjc/4487.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Elasticsearch之docker安装&quot;&gt;&lt;a href=&quot;#Elasticsearch之docker安装&quot; class=&quot;headerlink&quot; title=&quot;Elasticsearch之docker安装&quot;&gt;&lt;/a&gt;&lt;font color=&quot;#0077bb&quot;&gt;Elasticsearch之docker安装&lt;/font&gt;&lt;/h1&gt;&lt;p&gt;最近由于工作原因，需要倒腾下&lt;strong&gt;Elasticsearch&lt;/strong&gt; ,虽然本人的工作中，大部分情况下使用的都是&lt;strong&gt;Solr&lt;/strong&gt;,但是这并不能挡住我学习Elasticsearch的激情，这里我并不做这两个工具的好坏，只要用的顺手用的舒服就是好工具。这里我以docker安装为例子。&lt;/p&gt;
&lt;h2 id=&quot;环境准备&quot;&gt;&lt;a href=&quot;#环境准备&quot; class=&quot;headerlink&quot; title=&quot;环境准备&quot;&gt;&lt;/a&gt;&lt;font color=&quot;#0077bb&quot;&gt;环境准备&lt;/font&gt;&lt;/h2&gt;&lt;p&gt;笔者使用的环境为&lt;strong&gt;华为云&lt;/strong&gt;上的一台虚拟机,这里简单的描述下&lt;strong&gt;docker&lt;/strong&gt;和&lt;strong&gt;docker-compose&lt;/strong&gt;的安装&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;yum -y install docker docker-compose &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;#具体的版本为: docker-compose-1.18.0-4.el7.noarch , docker-1.13.1-103.git7f2769b.el7.centos.x86_64&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;docker 安装elasticsearch 还是比较简单的,我们直接参考&lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html就行了&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官方文档&lt;/a&gt;，这里我们使用的版本号码是&lt;strong&gt;7.4.1&lt;/strong&gt;。大部分情况下，如果你是为了体验elasticsearch，或者搭建一个简单的测试环境，那么elasticsearch的官方文档应该就能解决这个问题,但是由于笔者需要存储大概几十万的数据进去，而且由于使用的是云上的环境，一般来说系统盘的小大都很小（常见的值为8GB），所以必须将elasticsearch的data目录挂载到数据盘上。而问题就出现在docker的目录映射上。&lt;br&gt;
    
    </summary>
    
    
      <category term="elasticearch" scheme="https://www.fengqinglei.top/tags/elasticearch/"/>
    
      <category term="docker" scheme="https://www.fengqinglei.top/tags/docker/"/>
    
      <category term="volume permission" scheme="https://www.fengqinglei.top/tags/volume-permission/"/>
    
      <category term="docker volume 权限" scheme="https://www.fengqinglei.top/tags/docker-volume-%E6%9D%83%E9%99%90/"/>
    
  </entry>
  
  <entry>
    <title>Text Retrieval and Search Engines 3</title>
    <link href="https://www.fengqinglei.top/2019/10/17/text-retrieval-and-search-engines-3/"/>
    <id>https://www.fengqinglei.top/2019/10/17/text-retrieval-and-search-engines-3/</id>
    <published>2019-10-16T16:20:21.000Z</published>
    <updated>2019-11-03T14:04:35.891Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Text-Retrieval-and-Search-Engines学习笔记-三"><a href="#Text-Retrieval-and-Search-Engines学习笔记-三" class="headerlink" title="Text Retrieval and Search Engines学习笔记(三)"></a><font color="#0077bb">Text Retrieval and Search Engines学习笔记(三)</font></h1><p>这篇文章是继上一篇<a href="https://www.fengqinglei.top/2019/10/09/text-retrieval-and-search-engines-2/">Text Retrieval and Search Engines学习笔记(二)</a> 的后续部分，这一篇我们重点介如何绍改进<strong>Vector Space Model</strong></p><h2 id="Vector-Space-Model"><a href="#Vector-Space-Model" class="headerlink" title="Vector Space Model"></a><font color="#0077bb">Vector Space Model</font></h2><p>在上一篇我们介绍了由<strong>BOW</strong>模型和<strong>Dot Product</strong>相似度计算方法组成的<strong>Simplest Vector Model</strong> ,在某种成都上说，这个方法确实解决了一些我们对文档排序的一些需求，但是从上面的例子中我们也可以简单的发现两个问题:<br><a id="more"></a><br><img src="https://s2.ax1x.com/2019/10/19/Kn4ADs.png" alt="Kn4ADs.png"></p><h3 id="文档中出现关键字presidential越多，分数应该越高"><a href="#文档中出现关键字presidential越多，分数应该越高" class="headerlink" title="文档中出现关键字presidential越多，分数应该越高"></a><font color="#0077bb">文档中出现关键字<strong>presidential</strong>越多，分数应该越高</font></h3><p>针对这个问题，我们可以使用关键字出现的次数替换<strong>BOW</strong>模型中的{0,1}表示方法，即如果文章中出现了关键词，那么将原来的1替换成在该文章中出现某关键词的次数，而如果没有出现，那么还是0保持不变,也就是说我们的similarity计算公式变为: Sim(q,d)=x’<sub>1</sub>y’<sub>1</sub>+…+x’<sub>i</sub>y’<sub>i</sub>=$\sum_{i=1}^N$x’<sub>i</sub>y’<sub>i</sub>(这里的x’<sub>i</sub>=Count(W<sub>i</sub>,q)表示关键词W<sub>i</sub>在query中出现的次数,y’<sub>i</sub>=Count(W<sub>i</sub>,d)表示关键词W<sub>i</sub>在文档出现的次数，一般来说x<sub>i</sub>的值都是1)<br><img src="https://s2.ax1x.com/2019/10/19/Kn42Pf.png" alt="Kn42Pf.png"><br>我们将改进过的similarity计算方式重新计算<strong>Simplest Vector Space Model</strong>中的例子:<br><img src="https://s2.ax1x.com/2019/10/19/Kn5Kot.png" alt="Kn5Kot.png"><br>我们可以看到Sim(q,d<sub>4</sub>的分数从原来的<strong>3</strong>变成了<strong>4</strong>)，确实这样的改动能够达到我们的诉求。</p><h3 id="关键字presidential要比关键词about的权重更高"><a href="#关键字presidential要比关键词about的权重更高" class="headerlink" title="关键字presidential要比关键词about的权重更高"></a><font color="#0077bb">关键字<strong>presidential</strong>要比关键词<strong>about</strong>的权重更高</font></h3><p>当我们仔细的去分析/理解query<strong>news about presidential campaign</strong>,的时候我们会自然而然的认为关键词<strong>presidential</strong>应该比<strong>about</strong>总要-&gt; 为什么？，我们为什么会得出这样的结论，这几乎是所有正常人都会得出的结论—&gt; 因为我们看到的太多了，这就好比我们每天都看到/遇到各种个样的人,这时候你在路上遇到个人你会觉得这是一件非常正常的事情,但是如果哪天你在路上看见一头大象你一定非常的在意这头大象，因为它太少见了.同样的道理，因为关键词<strong>about</strong>几乎出现在所有的文章中，那么关键字<strong>about</strong>也就没有关键字<strong>presidential</strong>那么”值钱”了，为了解决这种常见词的问题，我们这里可以使用IDF(逆文档频率来解决这个问题),<strong>IDF(W)=log[(M+1)/k]</strong>,k表示有多少文档中出现了关键词<strong>W</strong> (和出现次数没有关系)<br><img src="https://s2.ax1x.com/2019/10/19/Kn7F7d.png" alt="Kn7F7d.png"><br>那么改进之后的Sim计算方法为:Sim(q,d)=x’<sub>1</sub>y’’<sub>1</sub>+…+x’<sub>i</sub>y’’<sub>i</sub>=$\sum_{i=1}^N$x’<sub>i</sub>y’’<sub>i</sub>(这里的x’<sub>i</sub>=Count(W<sub>i</sub>,q)表示关键词W<sub>i</sub>在query中出现的次数,y’’<sub>i</sub>=Count(W<sub>i</sub>,d)*IDF(W<sub>i</sub>)表示关键词W<sub>i</sub>在文档出现的次数乘以关键词W<sub>i</sub>的逆文档频率，我们没有将IDF应用到query中的关键词的权重计算中，个人理解是因为用户的query中的关键字一定是重要的词，不然为啥要当成检索式的组成要素，但是文本中的关键词因为表达的意义比较多会有主次之分),我们将改进之后的公式应用到同样的例子中(我们现对比下d2,d3,因为d2,d3的得分是一样的))<br><img src="https://s2.ax1x.com/2019/10/19/KnbaTJ.png" alt="KnbaTJ.png"><br>我们可以看到在引入idf之后d3文档的得分按照我们的想法得到的更多的提升，从而使用f(q,d2)=5.6 &lt; f(q,d3)=7.1 ,这样两个文档之间因为得分不一样从而产生了区分度，我们也可以将文档d3排在文档d2的前面。</p><p>至此，我们通过使用<strong>TF</strong>, <strong>IDF</strong>分别解决了上述两个问题，我们来计算下所有的文档和query的相关度,看看这种计算方法是不是在所有情况下都能反应文档和query的真实相关度，<br><img src="https://s2.ax1x.com/2019/10/19/Knq7CR.png" alt="Knq7CR.png"><br>我们在上一篇文章中说过，一种idea的排序应该是(d4+, d3+ ,d1- ,d2- d5- ),貌似d4,d3,d1,d3的顺序和分数已经达到了我们想要的效果，但是d5的得分却是13.9，这和我们的期望相差的有点大。。。。</p><h3 id="TF-Transformation-解决词频轰炸"><a href="#TF-Transformation-解决词频轰炸" class="headerlink" title="TF Transformation,解决词频轰炸"></a><font color="#0077bb">TF Transformation,解决词频轰炸</font></h3><p>首先我们得明白为什么d5的相似度为13.9,这里我们可以分析下我们的rank方法f(q,d)=$\sum_{i=1}^N$x<sub>i</sub>y<sub>i</sub>=$\sum_{w \in q \cap w}$c(w,q)c(w,d)log$\frac{M+1}{df(w)}$, 不难发现f(q,d5)=13.9的原始是c(campaign,d5)=4, 这里idf(campaign)对于所有包含关键词campaign的文档来说都是一样的，为了解决这种高tf所带来的问题，我们需要对t这一指标进行改写，这里我们介绍<strong>BM25</strong>模型对tf的改写规则: TF(w,d) = $\frac{(k+1)x}{x+k}$,不难发现该函数有个特性就是上界为<strong>k+1</strong>,这里我们解释下，为什么不使用log函数对tf进行平滑？我们可以现看下这几个函数的走势图:<br><img src="https://s2.ax1x.com/2019/10/20/KnxNgs.png" alt="KnxNgs.png"><br>从图中我们可以看出，log函数是没有上界的，当k非常大的时候经过log函数的变化，结果依然会很大，假设我在网上发了一篇关于某主题的文章，文章的内容就是重复某个关键字几万甚至几十万次，那么如果我检索改关键词，那么这篇文章按照我们目前定义的函数来计算相似度的话，一定是排第一的，但是显然这又是非常不合理的(文章没有实质性的内容),所以为了不出现上述这种词频轰炸的问题，我们必须设置一个词频对分数影响的上届，当词频超过一定数量之后，它的影响不会随着tf的增加而发生缓慢的变化，并且这种变化有一个理论上的尽头。</p><p>经过上述对两个问题的分析和提出的解决方案，我们可以得到:<br>f(q,d) = $\sum_{i=1}^{N}$x<sub>i</sub>y<sub>i</sub>=$\sum_{w \in q \cap w}C(w,q)\frac{(k+1)C(w,d)}{C(w,d)+k}log\frac{M+1}{df}(w)$,其中</p><div class="table-container"><table><thead><tr><th>符号名称</th><th>符号含义</th></tr></thead><tbody><tr><td>C(w,q)</td><td>表示关键词w<sub>i</sub>在query中出现的次数，一般都是1词</td></tr><tr><td>M</td><td>表示文档的总数</td></tr><tr><td>df(W)</td><td>表示关键词w<sub>i</sub>在文档d中出现的次数</td></tr></tbody></table></div><h3 id="文档长度也需要考虑在其中"><a href="#文档长度也需要考虑在其中" class="headerlink" title="文档长度也需要考虑在其中"></a><font color="#0077bb">文档长度也需要考虑在其中</font></h3><p>除了词频和低频词的问题，正常来说大家都会发现文档有长有短，一般来说都是文章越长，命中某个query关键字的概率也就越大(不绝对，但是有一定的可信度),所以类似tf的处理规则一样，我们得对文档长度进行处理。我们使用<strong>Pivoted Length Normalization</strong>规则对文档的长度进行处理具体的公式为</p><p>normalizer = 1 -b + b$\frac{|d|}{avdl}$ (其中avdl是所有文档的平均长度,并且b$ \in [0,1]$)<br>如果b=0,那么normalizer=1 ,也就是说这个时候文档长度不参与最终的打分，当b&gt;0时，如果某个文档的长度超过平均文档长度，那么随着b增加，normalizer的取值也就越大(这里注意最终的分数是越小的，因为我们要对这种长文档进行降分处理),当文档长度小于平均长度的时候，normalizer的取值也就越小(但是一定是大于0的),最终的分数应该是越大的，因为我们要对短文档的分数进行一定的补偿，也就是说b的值越大，对文档长度的打分的降低或者补偿的力度越大。从下面的图中，大家可以有个直观的理解。<br><img src="https://s2.ax1x.com/2019/10/21/KQNz1x.png" alt="KQNz1x.png"></p><h2 id="最终的打分公式"><a href="#最终的打分公式" class="headerlink" title="最终的打分公式"></a><font color="#0077bb">最终的打分公式</font></h2><p>到这里，加上我们对文档长度的处理，我们可以得到最终的BM25打分公式为:<br>f(q,d) = $\sum_{w \in q \cap d}$ [C(w,q) $\frac{(k+1)C(w,d)}{C(w,d)+k(1-b+b \frac {d}{avdl})}$log$\frac{M+1}{df(w)}$)]<br>其中:<br>C(w,q)= 关键词w在query中出现的次数，一般为1<br>C(w,q)= 关键词w在文档中出现的次数<br>k为正实数 k&gt;0<br>b的取值范围为 b $\in [0,1] $<br>M 表示文档总数<br>df(w) 所有关键词w出现的文档的总数。<br>当然BM25只是vector space model的一种实现，而实验证明还有另外一种实现也和BM25一样高效, 即 <strong>Pivoted Length Normalization VSM</strong>,这里我们直接给出它的打分公式:<br><img src="https://s2.ax1x.com/2019/10/21/KQ09Gd.png" alt="KQ09Gd.png"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><font color="#0077bb">总结</font></h2><p>到这里我们简单介绍了<strong>Simplest Vector Space Model</strong>所面临的问题，以及我们相应的改进方案，从0～1的介绍了BM25算法公式的由来和实现，在一篇中我们将介绍如何高效的计算BM25打分公式，毕竟真实场景中我们遇到的文档数量可能是千万甚至几十亿的级别，我们要以高效的计算方式实现BM25,否则一个检索式输入到系统中，我们到等几分钟或者几个小时才能得到答案，这肯定是不现实的。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Text-Retrieval-and-Search-Engines学习笔记-三&quot;&gt;&lt;a href=&quot;#Text-Retrieval-and-Search-Engines学习笔记-三&quot; class=&quot;headerlink&quot; title=&quot;Text Retrieval and Search Engines学习笔记(三)&quot;&gt;&lt;/a&gt;&lt;font color=&quot;#0077bb&quot;&gt;Text Retrieval and Search Engines学习笔记(三)&lt;/font&gt;&lt;/h1&gt;&lt;p&gt;这篇文章是继上一篇&lt;a href=&quot;https://www.fengqinglei.top/2019/10/09/text-retrieval-and-search-engines-2/&quot;&gt;Text Retrieval and Search Engines学习笔记(二)&lt;/a&gt; 的后续部分，这一篇我们重点介如何绍改进&lt;strong&gt;Vector Space Model&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;Vector-Space-Model&quot;&gt;&lt;a href=&quot;#Vector-Space-Model&quot; class=&quot;headerlink&quot; title=&quot;Vector Space Model&quot;&gt;&lt;/a&gt;&lt;font color=&quot;#0077bb&quot;&gt;Vector Space Model&lt;/font&gt;&lt;/h2&gt;&lt;p&gt;在上一篇我们介绍了由&lt;strong&gt;BOW&lt;/strong&gt;模型和&lt;strong&gt;Dot Product&lt;/strong&gt;相似度计算方法组成的&lt;strong&gt;Simplest Vector Model&lt;/strong&gt; ,在某种成都上说，这个方法确实解决了一些我们对文档排序的一些需求，但是从上面的例子中我们也可以简单的发现两个问题:&lt;br&gt;
    
    </summary>
    
    
      <category term="NLP" scheme="https://www.fengqinglei.top/tags/NLP/"/>
    
      <category term="Text Retrieval" scheme="https://www.fengqinglei.top/tags/Text-Retrieval/"/>
    
      <category term="Vector Space Model" scheme="https://www.fengqinglei.top/tags/Vector-Space-Model/"/>
    
      <category term="Search Engine" scheme="https://www.fengqinglei.top/tags/Search-Engine/"/>
    
      <category term="Probabilistic Model" scheme="https://www.fengqinglei.top/tags/Probabilistic-Model/"/>
    
  </entry>
  
  <entry>
    <title>Text Retrieval and Search Engines 2</title>
    <link href="https://www.fengqinglei.top/2019/10/09/text-retrieval-and-search-engines-2/"/>
    <id>https://www.fengqinglei.top/2019/10/09/text-retrieval-and-search-engines-2/</id>
    <published>2019-10-09T12:23:02.000Z</published>
    <updated>2019-10-16T16:22:07.003Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Text-Retrieval-and-Search-Engines学习笔记-二"><a href="#Text-Retrieval-and-Search-Engines学习笔记-二" class="headerlink" title="Text Retrieval and Search Engines学习笔记(二)"></a><font color="#0077bb">Text Retrieval and Search Engines学习笔记(二)</font></h1><p>这篇文章是继上一篇<a href="https://www.fengqinglei.top/2019/10/09/text-retrieval-and-search-engines-1/">Text Retrieval and Search Engines学习笔记(一)</a> 的后续部分，这一篇我们重点介绍如何定义和计算<strong>f(q,d)</strong></p><h2 id="Text-Retrieval-Methods"><a href="#Text-Retrieval-Methods" class="headerlink" title="Text Retrieval Methods"></a><font color="#0077bb">Text Retrieval Methods</font></h2><p>首先，我们得知道如何设计一个排序方法，这里我们可以列举一个好的排序方法都有那些特征或者属性</p><blockquote><ul><li>关于Query :q=q<sub>1</sub>,…,q<sub>m</sub>, q<sub>i</sub> $\in$ V</li><li>关于Document: d<sub>i</sub>=d<sub>i1</sub>,…,d<sub>ij</sub>, d<sub>ij</sub> $\in$ V</li><li>排序方法： f(q,d)$\in$R (这里表示排序方法的值在实数域，说人话就是可以取任意实数)</li><li>一个好的排序方法必须将相关的文档排在不相关的文档之前,那么问题来了，我们如何衡量query和document的相关的似然度呢？ </li><li>我们必须给排序模型/方法下一个明确的定义，并且这个定义还是可以明确计算的(说白了就是能够以数学公式的方式定义出来，我们可以通过数学公式计算出相关度)<a id="more"></a>好消息是，Text Retrieval 这个问题很久之前就已经出现了，前辈们已经给了我们可行的方法,下面我们列举一些召回模型。</li><li>Similarity-based models (基于相似度的模型)): f(q,d)=similarity(q,d) ,最常见的就是我们经常听说的<strong>Vector Space Model</strong> (空间向量模型)</li><li>Probabilistic models(概率模型): f(d,q)=p(R=1|d,q), 这里的R={0,1} （0表示不相关，1表示相关）,这里我们假设查询和文档都是来自随机变量的观察结果，常见的有<br>1.经典的概率模型(Classic probabilistic model)<br>2.<strong>语言模型(Language model)</strong><br>3.随机性偏差模型(Divergence-from-randomness model)</li><li>Probabilistic interence model (概率推理模型): f(q,d)=p(d-&gt;q) ,这类模型的想法是将不确定性与推理规则关联起来，然后我们可以量化某个query我们应该显示的文档的概率。</li><li>Axiomatic model(基于规则的)))): f(q,d) 必须匹配一些列的条件</li></ul></blockquote><p>虽然这些模型的出发点不尽相同，但是这些模型排序方法都类似，并且排序方法中都使用了类似的参数（例如：tf，|d|,df）等<br><img src="https://s2.ax1x.com/2019/10/09/uIbWNR.png" alt="commom ideas.png"><br>那么问题来了，我们有这么多模型可以选择，到底那个/些模型的效果是最好的？<br>实际上，当经过一些优化，下面的4中模型的效果是等价的</p><ul><li>Pivoted length normalization </li><li><strong>BM25</strong>(Solr的从5.x版本开始将BM25作为默认的排序模型)</li><li>Query likelihood</li><li>PL2<br><strong>BM25</strong>是空间向量模型的一种实现，下面我们就开始介绍<strong>Vector Space Model</strong></li></ul><h2 id="Vector-Space-Model"><a href="#Vector-Space-Model" class="headerlink" title="Vector Space Model"></a><font color="#0077bb">Vector Space Model</font></h2><p>空间向量模型的概念还是非常简单的: 把对文本内容的处理简化为向量空间中的向量运算，并且它以空间上的相似度表达语义的相似度，直观易懂。当文档被表示为文档空间的向量，就可以通过计算向量之间的相似性来度量文档间的相似性。文本处理中最常用的相似性度量方式是余弦距离。<br><img src="https://s2.ax1x.com/2019/10/09/uIv19s.png" alt="vector_space_model.png">,简单来说Vector Space Model是一个框架，每个term(最基本的概念，例如可以是一个单词或者词组)是一个纬度，query和文档都是有term组成，所以query和文档都可以映射成n维的向量，f(q,d)就是计算query和文档相似度的方法。虽然说这个一个从理论上可行的方法，但是vector Space Model在很多细节上并没有定义清楚，例如</p><blockquote><ul><li>如何定义/选择基础概念，因为基础向量是需要正交的</li><li>如何将文档和query映射到空间中(其实就是如何定义term的权重),因为term在query中的权重表示在这个term在这个query中的重要性，term在文档中的权重刻画了文档的特征</li><li>如何定义相似度的计算方式</li></ul></blockquote><p><img src="https://s2.ax1x.com/2019/10/11/uHTHUJ.png" alt="what_vsm_not_say.png"><br>也就是说Vector Space Model这个框架更多是在理论上解决了如果计算文档和query相似度(R<sup>,</sup>(q,d)),我们需要给出一种实现才能够真正解决我们的实际问题。</p><h3 id="Simplest-Vector-Space-Model"><a href="#Simplest-Vector-Space-Model" class="headerlink" title="Simplest Vector Space Model"></a><font color="#0077bb">Simplest Vector Space Model</font></h3><blockquote><ul><li>用<strong>Bag of words</strong>的方式来放置query和文档的向量<br>我们可以用{0,1}来构造query/文档向量，简单说就是如果某个term出现在文档中，那么我们用1标识，如果没有出现过，我们就使用0标识。(可以想象，query中向量中会有非常多的0，因为一般而言query比较短，但是文档比较长),这里我们可以令q=(x<sub>1</sub>,x<sub>2</sub>,…,x<sub>N</sub>) ,d=(y<sub>1</sub>,y<sub>2</sub>,…,y<sub>N</sub>),这里的x<sub>i</sub>,y<sub>i</sub> $\in$ {0,1}</li><li>使用<strong>Dot Product</strong> 来计算query和文档的相似度<br>Sim(q,d)=q*d =x<sub>i</sub>y<sub>1</sub>+…+x<sub>N</sub>y<sub>N</sub>=$\sum_{i=1}^N$x<sub>i</sub>y<sub>i</sub></li></ul></blockquote><p>举个例子:<br>Query =  “<font color="#CC0000">news about presidential campaign</font>“ </p><div class="table-container"><table><thead><tr><th>doc</th><th>content </th></tr></thead><tbody><tr><td>d1</td><td>… <font color="#CC0000">news about</font>…</td></tr><tr><td>d2</td><td>… <font color="#CC0000">news about</font> organic food <font color="#CC0000">campaign</font>…</td></tr><tr><td>d3</td><td>… <font color="#CC0000">news</font> of <font color="#CC0000">presidential campaign</font>…</td></tr><tr><td>d4</td><td><font color="#CC0000">news</font> of <font color="#CC0000">presidential campaign</font>… <font color="#CC0000">presidential</font> candidate …</td></tr><tr><td>d5</td><td><font color="#CC0000">news</font> of organic food <font color="#CC0000">campaign</font> … <font color="#CC0000">campaign</font> … <font color="#CC0000">campaign</font> … <font color="#CC0000">campaign</font> …</td></tr></tbody></table></div><p>如果让我们来根据这个query对这5个文档排序的话，一种可能的结果是<br>d4+, d3+ ,d1- ,d2- d5- (这里的顺序就是表示文档的相似度的排序，+表示正相关，-表示负相关)。<br>那如果我们使用<strong>Simplest Vecotr Model</strong>来对文档来打分，结果会怎么样？<br>Query =  “<font color="#CC0000">news about presidential campaign</font>“</p><div class="table-container"><table><thead><tr><th>doc</th><th>content </th></tr></thead><tbody><tr><td>d1</td><td>… <font color="#CC0000">news about</font>…</td></tr><tr><td>d3</td><td>… <font color="#CC0000">news</font> of <font color="#CC0000">presidential campaign </font></td></tr></tbody></table></div><p>这里V={news, about, presidential, campaign,food …},那么我们可以通过<strong>Bag of words</strong> 和<strong>Bit Vector</strong> 来表示query和文档d1,d3,并且计算对应的similarity (通过下面的例子我们可以看到词袋模型没有按照文章中单词的顺序进行排序，而是根据V中所有的词的顺序来进行赋值的，有就是1，没有就是0))<br>q=(     1,   1,     1,      1,      0, …)<br>d1=(    1,   1,     0,      0,      0, …) , <font color="#CC0000">f(q,d1)= 1x1+1x1+1x0+1x0+0x0+ .. =2 </font><br>d3=(    1,   0,     1,      1,      0, …),  <font color="#CC0000">f(q,d1)=1x1+1x0+1x1+1x1+0x0+ .. =3 </font><br>按照同样的方法，我们计算出所有文档的分数</p><div class="table-container"><table><thead><tr><th>doc</th><th>content</th><th>score</th></tr></thead><tbody><tr><td>d1</td><td>… <font color="#CC0000">news about</font>…</td><td>f(q,d1)=2</td></tr><tr><td>d2</td><td>… <font color="#CC0000">news about</font> organic food <font color="#CC0000">campaign</font>…</td><td>f(q,d2)=3</td></tr><tr><td>d3</td><td>… <font color="#CC0000">news</font> of <font color="#CC0000">presidential campaign</font>…</td><td>f(q,d3)=3</td></tr><tr><td>d4</td><td><font color="#CC0000">news</font> of <font color="#CC0000">presidential campaign</font>… <font color="#CC0000">presidential</font> candidate …</td><td>f(q,d4)=3</td></tr><tr><td>d5</td><td><font color="#CC0000">news</font> of organic food <font color="#CC0000">campaign</font> … <font color="#CC0000">campaign</font> … <font color="#CC0000">campaign</font> … <font color="#CC0000">campaign</font> …</td><td>f(q,d5)=2</td></tr></tbody></table></div><p>最终的按照score排序的结果为 (d4,d3,d2) (d1,d5) [因为d4,d3,d2 分数一样，所以部分前后， d1,d5同样如此]],貌似<strong>Simplest Vector Model</strong> 反映了一些文档和query的相关程度，但是效果没有那么的达到预期。<br>如果我们仔细会想<strong>Bit Vector</strong> 就会发现，很多信息在生成query/文档向量的时候已经被丢掉了(例如词频)，那么如果我们加上这些信息<strong>Simplest Vector Model</strong> 是不是会好很多呢? 我们将下一篇详细介绍<strong>Vector Space Model</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Text-Retrieval-and-Search-Engines学习笔记-二&quot;&gt;&lt;a href=&quot;#Text-Retrieval-and-Search-Engines学习笔记-二&quot; class=&quot;headerlink&quot; title=&quot;Text Retrieval and Search Engines学习笔记(二)&quot;&gt;&lt;/a&gt;&lt;font color=&quot;#0077bb&quot;&gt;Text Retrieval and Search Engines学习笔记(二)&lt;/font&gt;&lt;/h1&gt;&lt;p&gt;这篇文章是继上一篇&lt;a href=&quot;https://www.fengqinglei.top/2019/10/09/text-retrieval-and-search-engines-1/&quot;&gt;Text Retrieval and Search Engines学习笔记(一)&lt;/a&gt; 的后续部分，这一篇我们重点介绍如何定义和计算&lt;strong&gt;f(q,d)&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;Text-Retrieval-Methods&quot;&gt;&lt;a href=&quot;#Text-Retrieval-Methods&quot; class=&quot;headerlink&quot; title=&quot;Text Retrieval Methods&quot;&gt;&lt;/a&gt;&lt;font color=&quot;#0077bb&quot;&gt;Text Retrieval Methods&lt;/font&gt;&lt;/h2&gt;&lt;p&gt;首先，我们得知道如何设计一个排序方法，这里我们可以列举一个好的排序方法都有那些特征或者属性&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;关于Query :q=q&lt;sub&gt;1&lt;/sub&gt;,…,q&lt;sub&gt;m&lt;/sub&gt;, q&lt;sub&gt;i&lt;/sub&gt; $\in$ V&lt;/li&gt;
&lt;li&gt;关于Document: d&lt;sub&gt;i&lt;/sub&gt;=d&lt;sub&gt;i1&lt;/sub&gt;,…,d&lt;sub&gt;ij&lt;/sub&gt;, d&lt;sub&gt;ij&lt;/sub&gt; $\in$ V&lt;/li&gt;
&lt;li&gt;排序方法： f(q,d)$\in$R (这里表示排序方法的值在实数域，说人话就是可以取任意实数)&lt;/li&gt;
&lt;li&gt;一个好的排序方法必须将相关的文档排在不相关的文档之前,那么问题来了，我们如何衡量query和document的相关的似然度呢？ &lt;/li&gt;
&lt;li&gt;我们必须给排序模型/方法下一个明确的定义，并且这个定义还是可以明确计算的(说白了就是能够以数学公式的方式定义出来，我们可以通过数学公式计算出相关度)
    
    </summary>
    
    
      <category term="NLP" scheme="https://www.fengqinglei.top/tags/NLP/"/>
    
      <category term="Text Retrieval" scheme="https://www.fengqinglei.top/tags/Text-Retrieval/"/>
    
      <category term="Vector Space Model" scheme="https://www.fengqinglei.top/tags/Vector-Space-Model/"/>
    
      <category term="Search Engine" scheme="https://www.fengqinglei.top/tags/Search-Engine/"/>
    
      <category term="Probabilistic Model" scheme="https://www.fengqinglei.top/tags/Probabilistic-Model/"/>
    
  </entry>
  
  <entry>
    <title>Text Retrieval and Search Engines 1</title>
    <link href="https://www.fengqinglei.top/2019/10/09/text-retrieval-and-search-engines-1/"/>
    <id>https://www.fengqinglei.top/2019/10/09/text-retrieval-and-search-engines-1/</id>
    <published>2019-10-09T11:58:38.000Z</published>
    <updated>2019-10-11T13:11:06.663Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Text-Retrieval-and-Search-Engines学习笔记-一"><a href="#Text-Retrieval-and-Search-Engines学习笔记-一" class="headerlink" title="Text Retrieval and Search Engines学习笔记(一)"></a><font color="#0077bb">Text Retrieval and Search Engines学习笔记(一)</font></h1><p>这篇文章主要是记录了学习Coursera上的课程<a href="https://www.coursera.org/learn/text-retrieval/home/welcome" target="_blank" rel="noopener">Text Retrieval and Search Engines</a>的一些笔记和个人的理解,这个课程总体来说比较偏理论，虽然实战非常重要，但是有的时候我们也需要学一些理论知识，这样我们才可以慢慢的<strong>知其然，并且知其所以然</strong>。</p><h2 id="课程大纲"><a href="#课程大纲" class="headerlink" title="课程大纲"></a><font color="#0077bb">课程大纲</font></h2><blockquote><ul><li>Natural Language Content Analysis</li><li>Text Access</li><li>Text Retrieval Problem</li><li>Text Retrieval Methods</li><li>Vector Space Model</li><li>System Implementation</li><li>Evaluation</li><li>Probabilistic Model</li><li>Feedback</li><li>Web Search</li><li>Recommendation<a id="more"></a><img src="https://s2.ax1x.com/2019/10/05/uyjSp9.png" alt="course.png"><h2 id="Natural-Language-Content-Analysis"><a href="#Natural-Language-Content-Analysis" class="headerlink" title="Natural Language Content Analysis"></a><font color="#0077bb">Natural Language Content Analysis</font></h2></li><li>自然语言处理是啥?<br>百度百科对自然语言的解释是:自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、计算机科学、数学于一体的科学。因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分。<br>说人话就是<strong>用机器处理人类的语言</strong>(这里的语言可以是说的，写的等等))</li><li>NLP 的例子<br>常见的nlp问题有<strong>POS</strong>,<strong>Syntatics Analysis</strong>,<strong>Semantic Analysis</strong> 等等。<br><img src="https://s2.ax1x.com/2019/10/05/uyHGtA.png" alt="nlp_example"></li><li>NLP的现状和难题<br>现在的机器学习和深度学习方法或者crf等算法在分词，机器翻译，情感分析，词性标注等问题上都有很好的表现，但是一般都是在特定领域上的表现比较好，并且这还是基于大量的人工标注或者大量的语料库的情况下，虽然最终的准确率或者各种指标都是97%甚至99%，但是自然语言处理是非常难达到100%的完美的，即使在某个特定的领域，因为有的时候甚至人也很难达到100%，比如3个人做词性标注，同一个输入，标出不同的结果非常常见，在消除歧义等任务上这种情况就更加常见了，另外在深度语义理解上NLP现有的技术也很难达到精准，但这些都不影响我们对NLP技术的学习和研究，随着技术的发展，这些问题在今后或者未来的某天一定会得到更好的解决</li><li>自然语言处理和信息检索的结合<br>了解搜索引擎历史的人都知道，很久以前的搜索引擎原理并不是倒排索引，实际上是根据用户输入的关键词到所有的文章中找的，这也是为什么很多在看老的美国大片到某个系统查询的时候输入的内容带大小写的，但是现在大家在用baidu或者google检索的时候很少有人关注某个单词的大小写(机器翻译有的时候会关注)，是因为现在的搜索引擎都是基于倒排索引的原理，英文有与生俱来的分词界线，但是中文没有(当然英文有其他的问题，例如stemming/lemmatization)，这些分词，词性标注等NLP任务都是信息检索中不可或缺的要素。<h2 id="Text-Access"><a href="#Text-Access" class="headerlink" title="Text Access"></a><font color="#0077bb">Text Access</font></h2>这个章节的内容是比较简单的，主要是讲解了文本信息系统是如何帮助用户获得相关的文本信息的,这里讲一下信息获取的两种模式</li><li>pull(拉取)<br>顾名思义，这里用户掌握主动权，可以用户查询或者浏览的方式进行信息的获取</li></ul></blockquote><div class="table-container"><table><thead><tr><th>获取信息的方式</th><th>说明</th></tr></thead><tbody><tr><td>Querying(查询)</td><td>这个是最常见的模式，比如大家到百度或者google上使用关键词查询</td></tr><tr><td>Browsing(浏览)</td><td>这个也是比较常见的，基本上当你不知道买啥的时候去淘宝上瞎逛就是浏览模式</td></tr></tbody></table></div><blockquote><ul><li>push(推送)<br>这种模式下，系统占有主动权，系统可以通过对用户profile或者用户的行为产生一定的“理解”，将用户可能想要的信息推送给用户（类似电商更具购买记录推送类似商品，或者类似用户推荐商品）</li></ul></blockquote><h2 id="Text-Retrieval-Problem"><a href="#Text-Retrieval-Problem" class="headerlink" title="Text Retrieval Problem"></a><font color="#0077bb">Text Retrieval Problem</font></h2><blockquote><ul><li>Text Retrieval 是什么<br>文本检索应该可以说是<strong>Information Retrieval</strong>的一种特殊情况把，说白就是收集现有的文本信息存储到搜索引擎中，根据用户给定的检索语句，搜索引擎返回<strong>相关</strong>的文档给用户。</li><li>Text Retrieval(TR)和数据库检索的有什么区别</li></ul></blockquote><div class="table-container"><table><thead><tr><th>-/-</th><th>文本检索</th><th>数据库检索</th></tr></thead><tbody><tr><td>信息结构</td><td>非结构化，自由结构的文本</td><td>结构化数据</td></tr><tr><td>检索方式</td><td>有歧义的，非完整的</td><td>完整的，带有语义定义的</td></tr><tr><td>返回方式</td><td>有相关性的文档</td><td>完全匹配到的结果</td></tr></tbody></table></div><p>TR(Text Retrieval) 是经验性的问题，我们很难在数学上证明一个方法比另外一个方法好，并且必须依赖经验性的评估。</p><blockquote><ul><li>Text Retrieval 的定义<br>虽然我们没法使用数学公式给出100%的定义，但是我们依然可以在某种成都上给Text Retrieval下定义。</li></ul></blockquote><div class="table-container"><table><thead><tr><th>术语</th><th>定义</th></tr></thead><tbody><tr><td>Vocabulary(词汇):</td><td>V={W<sub>1</sub>,W<sub>2</sub>,W<sub>3</sub>,W<sub>4</sub>….W<sub>N</sub>}, 这里W<sub>i</sub> 表示某个词</td></tr><tr><td>Query(查询条件):</td><td>q=q<sub>1</sub>,…,q<sub>m</sub>, q<sub>i</sub> $\in$ V</td></tr><tr><td>Document(文档):</td><td>d<sub>i</sub>=d<sub>i1</sub>,…,d<sub>ij</sub>, d<sub>ij</sub> $\in$ V</td></tr><tr><td>Collection(集合):</td><td>C={d<sub>i</sub>,…, d<sub>M</sub>}，所有的文档组成了一个集合</td></tr><tr><td>Set of relevant documents(相关文档):</td><td>R(q) $\subseteq$ C，相关文档是所有的文档的一个子集</td></tr><tr><td>Task(任务):</td><td>计算文档相关度R<sup>,</sup>(q),这里的R<sup>,</sup>(q)其实是R(q)的一个近似表示，前面说了我们没法给出100%的数学定义，但是我们可以给出一个近似的定义(可以度量并且有效的定义))</td></tr></tbody></table></div><h2 id="如何计算R-q"><a href="#如何计算R-q" class="headerlink" title="如何计算R,(q)"></a><font color="#0077bb">如何计算R<sup>,</sup>(q)</font></h2><p>由于Text Retrieval的目标是找到相关的文档，那么如何定义相关，我们则需要通过完成R<sup>,</sup>(q)的计算，这里给出两种可行的方法.(方法有很多，不仅仅局限于这两种))</p><blockquote><ul><li>策略(一)：文档选择方式<br>R<sup>,</sup>(q)={d$\in$C|f(d,q)=1},这里的f(d,q)$\in${0,1}是我们定义的一个方法，这个方法取值要么为0要么为1 （二元分类器），说白了就是要么相关(1),要不不相关(0)</li><li>策略(二)：文档排序<br>R<sup>,</sup>(q)={d$\in$C|f(d,q)&gt;$\theta$}, 这里的f(d,q)$\in$R是一个度量的方法，这里的$\theta$}是一取决于用户的选择，此时系统只需要决定是否某个文档比另外的文档<strong>更相关</strong>（相对相关度）<br>当然通过这两种方式计算R<sup>,</sup>(q)都是有效的，但是那种方法更加切合实际？我们可以通过以下几个点进行分析。<br><img src="https://s2.ax1x.com/2019/10/08/ufarWQ.png" alt="Document_selection_vs_ranking.png"></li><li>二元分类其很有可能不太准确，因为很难区分一个文档到底是相关还是不想关，可能我们设定的条件会约束太强，也有可能约束太弱，那么或多或少的导致会将相关的文档会遗漏或者不相关的文档会被返回给最终用户。</li><li>即使我们在分类的时候将所有的结果都正确的放置在正确的分类上，但是实际上所有的文档的相关性不是都是一样的，不同的文档的相关程度是不尽相同的。所以我们需要设置一定的优先级。</li><li>综上两个观点，我们认为ranking是一个更好的解决方案。<h2 id="ranking方式的理论论证"><a href="#ranking方式的理论论证" class="headerlink" title="ranking方式的理论论证"></a><font color="#0077bb">ranking方式的理论论证</font></h2><a href="https://www.emerald.com/insight/content/doi/10.1108/eb026647/full/html" target="_blank" rel="noopener">The probability ranking principle in IR</a> 原文是:<br>Returning a ranked list of documents in descending order of probability that a document is relevant to the query is the optimal strategy under the following two assumptions:<br>– The utility of a document (to a user) is <strong>independent</strong> of the utility of any other document<br>– A user would browse the results <strong>sequentially</strong><br>简单来说就是:文本检索引擎按照文档和query相关的概率降序排列是一个最优策略，当然这个最有策略基于以下两个前提</li><li>某一个文档的作用和其他文档的作用是互不相关的</li><li>用户会按照结果的顺序来浏览结果<br>但是实际上这两个假设并不是100%都是成立的，两个文档可能是相似的，所以肯定不是100%完全独立的，而且用户可能会跳着看一些结果，但是，即使这些假设都不是100%成立的，按照文档和query相关的概率对结果进行排序在大部分的场景下仍然是一种合理的做法。</li></ul></blockquote><p>至此，我们可以得出-&gt; 对文档进行排序相比对文档进行二元分类似乎是一种更加有效的解决文本检索问题的方法，前面我们也提到过，解决文档和query相关的关键在于如何计算R<sup>,</sup>(q),其实就是如何定义一个有效的f(q,d)方法。这个我会在后面的章节继续探讨。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Text-Retrieval-and-Search-Engines学习笔记-一&quot;&gt;&lt;a href=&quot;#Text-Retrieval-and-Search-Engines学习笔记-一&quot; class=&quot;headerlink&quot; title=&quot;Text Retrieval and Search Engines学习笔记(一)&quot;&gt;&lt;/a&gt;&lt;font color=&quot;#0077bb&quot;&gt;Text Retrieval and Search Engines学习笔记(一)&lt;/font&gt;&lt;/h1&gt;&lt;p&gt;这篇文章主要是记录了学习Coursera上的课程&lt;a href=&quot;https://www.coursera.org/learn/text-retrieval/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Text Retrieval and Search Engines&lt;/a&gt;的一些笔记和个人的理解,这个课程总体来说比较偏理论，虽然实战非常重要，但是有的时候我们也需要学一些理论知识，这样我们才可以慢慢的&lt;strong&gt;知其然，并且知其所以然&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&quot;课程大纲&quot;&gt;&lt;a href=&quot;#课程大纲&quot; class=&quot;headerlink&quot; title=&quot;课程大纲&quot;&gt;&lt;/a&gt;&lt;font color=&quot;#0077bb&quot;&gt;课程大纲&lt;/font&gt;&lt;/h2&gt;&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Natural Language Content Analysis&lt;/li&gt;
&lt;li&gt;Text Access&lt;/li&gt;
&lt;li&gt;Text Retrieval Problem&lt;/li&gt;
&lt;li&gt;Text Retrieval Methods&lt;/li&gt;
&lt;li&gt;Vector Space Model&lt;/li&gt;
&lt;li&gt;System Implementation&lt;/li&gt;
&lt;li&gt;Evaluation&lt;/li&gt;
&lt;li&gt;Probabilistic Model&lt;/li&gt;
&lt;li&gt;Feedback&lt;/li&gt;
&lt;li&gt;Web Search&lt;/li&gt;
&lt;li&gt;Recommendation
    
    </summary>
    
    
      <category term="NLP" scheme="https://www.fengqinglei.top/tags/NLP/"/>
    
      <category term="Text Retrieval" scheme="https://www.fengqinglei.top/tags/Text-Retrieval/"/>
    
      <category term="Vector Space Model" scheme="https://www.fengqinglei.top/tags/Vector-Space-Model/"/>
    
      <category term="Search Engine" scheme="https://www.fengqinglei.top/tags/Search-Engine/"/>
    
      <category term="Probabilistic Model" scheme="https://www.fengqinglei.top/tags/Probabilistic-Model/"/>
    
  </entry>
  
  <entry>
    <title>玩转Solr源码之(三)Solr源码Deploy</title>
    <link href="https://www.fengqinglei.top/2019/10/03/solr-source-code-deploy/"/>
    <id>https://www.fengqinglei.top/2019/10/03/solr-source-code-deploy/</id>
    <published>2019-10-02T23:57:04.000Z</published>
    <updated>2019-10-03T08:30:56.061Z</updated>
    
    <content type="html"><![CDATA[<h1 id="玩转Solr源码之—Solr源码Deploy"><a href="#玩转Solr源码之—Solr源码Deploy" class="headerlink" title="玩转Solr源码之—Solr源码Deploy"></a><font color="#0077bb">玩转Solr源码之—Solr源码Deploy</font></h1><p>这篇文章是<strong>玩转Solr源码</strong>系列的第三篇，紧接着上一篇的<a href="https://www.fengqinglei.top/2019/10/01/solr-source-code-debug/">源码Deug</a>。如果你已经修改好了源码，并且调试ok，想要将源码打包后分发给同时或者其他项目引用，那么这边文章一定能够帮助你。<br><a id="more"></a></p><h2 id="不到万不得己，别直接改源代码"><a href="#不到万不得己，别直接改源代码" class="headerlink" title="不到万不得己，别直接改源代码"></a><font color="#0077bb">不到万不得己，别直接改源代码</font></h2><p>一般而言，如果将Solr/Elasticsearch应用于企业级开发，Solr原生的功能或多或少都不能100%的满足企业的业务需求(客户的需求都是比较变态的),这也使得我们有的时候不得不通过修改源代码的方式来达到我们的目的，改源码难，管理源代码更难，毕竟几万甚至几十万的行的Solr源码，你只动了其中一个类中的某一个方法，结果为了管理，你需要专门搞个git仓库，然后还要定义好版本号，放到nexus仓库给其他组员或者其他项目引用，改代码可能花了你一个小时，但是管理这些代码可能需要让你忙活一个上午。所以不到万不得已尽量通过继承/实现等方式来修改源码，这样在版本升级的时候，工作量会小很多。否则的话，干过的人都知道很痛苦，特别是上一个改了源码的人已经离职了。<br><img src="https://s2.ax1x.com/2019/10/03/uwD7dI.jpg" alt="uwD7dI.jpg"></p><h2 id="从源码到Nexus"><a href="#从源码到Nexus" class="headerlink" title="从源码到Nexus"></a><font color="#0077bb">从源码到Nexus</font></h2><p>笔者所在的公司Java开发使用的构建和依赖管理工具是Maven,仓库的话使用的是自建的nexus,我相信这也是大部分公司的模式，当然也有用Gradle，这里不比较两个工具的好坏，能完成开发，高效率并且使用的顺手的都是好工具。笔者这里以maven+ nexus为例。</p><h3 id="安装Nexus"><a href="#安装Nexus" class="headerlink" title="安装Nexus"></a><font color="#0077bb">安装Nexus</font></h3><p>大部分情况下，nexus仓库这种工作都是交给运维去搞的，一般开发只需要知道如何使用就行，如果你对安装nexus没有任何兴趣，那么这部分可以选择跳过。<br>为了方便演示，这里笔者使用了docker进行安装和部署nexus，流程比较简单，几行命令就行了,这里以<a href="https://hub.docker.com/r/sonatype/nexus/" target="_blank" rel="noopener">nexus2</a>为例子.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker pull sonatype/nexus</span><br><span class="line">docker run -d -p 8081:8081 --name nexus-oss sonatype/nexus</span><br></pre></td></tr></table></figure></p><p>打开浏览器输入<strong><a href="http://127.0.0.1:8081/nexus" target="_blank" rel="noopener">http://127.0.0.1:8081/nexus</a></strong>就可以看到nexuys界面啦，默认的登陆用户名和密码为admin,admin123<br><img src="https://s2.ax1x.com/2019/10/03/uwRhqS.png" alt="Nexus2"></p><h3 id="使用ant生成maven-artifacts并上传"><a href="#使用ant生成maven-artifacts并上传" class="headerlink" title="使用ant生成maven artifacts并上传"></a><font color="#0077bb">使用ant生成maven artifacts并上传</font></h3><p>Solr中的源码已经支持生成maven artifacets并且上传到自定义的仓库中，简单的命令为<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ant -Dversion=my-special-version -Dm2.repository.id=my-repo-id \</span><br><span class="line">          -Dm2.repository.url=https://example.org/my/repo \</span><br><span class="line">          generate-maven-artifacts</span><br></pre></td></tr></table></figure></p><p>这里解释下参数的含义</p><div class="table-container"><table><thead><tr><th>变量名</th><th>解释</th></tr></thead><tbody><tr><td>my-special-version</td><td>指定版本号，这个大家根据需要可以自定义，但是还是需要一定的命名规范（下面会讲到）</td></tr><tr><td>my-repo-id</td><td>表示nexus仓库的id，名称一般都是自定义的</td></tr><tr><td>m2.repository.url</td><td>表示nexuys仓库的地址，笔者这里的地址就是<strong><a href="http://127.0.0.1:8081/nexus/content/repositories/releases/" target="_blank" rel="noopener">http://127.0.0.1:8081/nexus/content/repositories/releases/</a></strong></td></tr></tbody></table></div><p>具体的可以参考文档<a href="https://github.com/apache/lucene-solr/blob/master/dev-tools/maven/README.maven" target="_blank" rel="noopener">Lucene/Solr Maven build instructions</a>。<br>到这里大家基本上已经知道如是将修改过后的solr源代码打包并且上传maven了吧，废话不多说，直接上命令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[fql@localhost solr-7.7.2]$ ant -Dversion=TEST -Dm2.repository.id=nexus -Dm2.repository.url=http://127.0.0.1:8081/nexus/content/repositories/releases/ generate-maven-artifacts</span><br><span class="line">Buildfile: /home/fql/IdeaProjects/solr-7.7.2/build.xml</span><br><span class="line"></span><br><span class="line">BUILD FAILED</span><br><span class="line">/home/fql/IdeaProjects/solr-7.7.2/build.xml:21: The following error occurred <span class="keyword">while</span> executing this line:</span><br><span class="line">/home/fql/IdeaProjects/solr-7.7.2/lucene/common-build.xml:63: If you pass -Dversion=... to <span class="built_in">set</span> a release version, it must match <span class="string">"7.7.2"</span>, optionally followed by a suffix (e.g., <span class="string">"-SNAPSHOT"</span>).</span><br><span class="line"></span><br><span class="line">Total time: 0 seconds</span><br></pre></td></tr></table></figure></p><p>擦，&lt;一顿操作猛如虎，定睛一看原地杵&gt;,原来version的名称海必须匹配<strong>7.7.2</strong>,好吧，再来<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[fql@localhost solr-7.7.2]$ ant -Dversion=7.7.2-TEST -Dm2.repository.id=nexus -Dm2.repository.url=http://127.0.0.1:8081/nexus/content/repositories/releases/ generate-maven-artifacts</span><br><span class="line">Buildfile: /home/fql/IdeaProjects/solr-7.7.2/build.xml</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">[artifact:deploy] Error deploying artifact <span class="string">'org.apache.lucene:lucene-solr-grandparent:pom'</span>: Error deploying artifact: Failed to transfer file: http://127.0.0.1:8081/nexus/content/repositories/releases/org/apache/lucene/lucene-solr-grandparent/7.7.2-TEST/lucene-solr-grandparent-7.7.2-TEST.pom. Return code is: 401</span><br><span class="line"></span><br><span class="line">BUILD FAILED</span><br><span class="line">/home/fql/IdeaProjects/solr-7.7.2/build.xml:209: The following error occurred <span class="keyword">while</span> executing this line:</span><br><span class="line">/home/fql/IdeaProjects/solr-7.7.2/lucene/build.xml:404: The following error occurred <span class="keyword">while</span> executing this line:</span><br><span class="line">/home/fql/IdeaProjects/solr-7.7.2/lucene/common-build.xml:656: Error deploying artifact <span class="string">'org.apache.lucene:lucene-solr-grandparent:pom'</span>: Error deploying artifact: Failed to transfer file: http://127.0.0.1:8081/nexus/content/repositories/releases/org/apache/lucene/lucene-solr-grandparent/7.7.2-TEST/lucene-solr-grandparent-7.7.2-TEST.pom. Return code is: 401</span><br><span class="line">Total time: 8 minutes 53 seconds</span><br></pre></td></tr></table></figure></p><p><strong>Return code is: 401</strong>,貌似权限不对，确实命令中没有传任何用户名和密码的信息，这里通过<strong>generate-maven-artifacts</strong>定位到solr-7.7.2/lucene/common-build.xml 文件中的<strong>m2-deploy</strong>(第646行)，并且在其下面的<strong>remoteRepository</strong>添加<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">authentication</span> <span class="attr">username</span>=<span class="string">"admin"</span> <span class="attr">password</span>=<span class="string">"admin123"</span>/&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- admin 和admin123 就是nexus的默认用户名和密码--&gt;</span></span><br></pre></td></tr></table></figure></p><p>重新执行命令我们就可以看到项目已经成功的上传到了本地的nexus(看起来一行代码搞定的问题，确实让你琢磨了一个上午)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[fql@localhost solr-7.7.2]$ ant -Dversion=7.7.2-TEST -Dm2.repository.id=nexus -Dm2.repository.url=http://192.168.34.128:8081/nexus/content/repositories/releases/ generate-maven-artifacts</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">[artifact:deploy] [INFO] Uploading repository metadata <span class="keyword">for</span>: <span class="string">'artifact org.apache.solr:solr-velocity'</span></span><br><span class="line">[artifact:deploy] Uploading: org/apache/solr/solr-velocity/7.7.2-TEST/solr-velocity-7.7.2-TEST-sources.jar to repository nexus at http://192.168.34.128:8081/nexus/content/repositories/releases/</span><br><span class="line">[artifact:deploy] Transferring 31K from nexus</span><br><span class="line">[artifact:deploy] Uploaded 31K</span><br><span class="line">[artifact:deploy] Uploading: org/apache/solr/solr-velocity/7.7.2-TEST/solr-velocity-7.7.2-TEST-javadoc.jar to repository nexus at http://192.168.34.128:8081/nexus/content/repositories/releases/</span><br><span class="line">[artifact:deploy] Transferring 64K from nexus</span><br><span class="line">[artifact:deploy] Uploaded 64K</span><br><span class="line">[artifact:install] [INFO] Installing /home/fql/IdeaProjects/solr-7.7.2/solr/build/solr.tgz.unpacked/solr-7.7.2-TEST/dist/solr-velocity-7.7.2-TEST.jar to /home/fql/.m2/repository/org/apache/solr/solr-velocity/7.7.2-TEST/solr-velocity-7.7.2-TEST.jar</span><br><span class="line">[artifact:install] [INFO] Installing /home/fql/IdeaProjects/solr-7.7.2/solr/build/contrib/solr-velocity/solr-velocity-7.7.2-TEST-src.jar to /home/fql/.m2/repository/org/apache/solr/solr-velocity/7.7.2-TEST/solr-velocity-7.7.2-TEST-sources.jar</span><br><span class="line">[artifact:install] [INFO] Installing /home/fql/IdeaProjects/solr-7.7.2/solr/build/contrib/solr-velocity/solr-velocity-7.7.2-TEST-javadoc.jar to /home/fql/.m2/repository/org/apache/solr/solr-velocity/7.7.2-TEST/solr-velocity-7.7.2-TEST-javadoc.jar</span><br><span class="line">[artifact:install] [INFO] Installing /home/fql/IdeaProjects/solr-7.7.2/solr/build/contrib/solr-velocity/solr-velocity-7.7.2-TEST-src.jar to /home/fql/.m2/repository/org/apache/solr/solr-velocity/7.7.2-TEST/solr-velocity-7.7.2-TEST-sources.jar</span><br><span class="line">[artifact:install] [INFO] Installing /home/fql/IdeaProjects/solr-7.7.2/solr/build/contrib/solr-velocity/solr-velocity-7.7.2-TEST-javadoc.jar to /home/fql/.m2/repository/org/apache/solr/solr-velocity/7.7.2-TEST/solr-velocity-7.7.2-TEST-javadoc.jar</span><br><span class="line">BUILD SUCCESSFUL</span><br><span class="line">Total time: 6 minutes 37 seconds</span><br></pre></td></tr></table></figure></p><p>打开浏览器<strong><a href="http://127.0.0.1:8081/nexus/content/repositories/releases/org/apache/lucene/" target="_blank" rel="noopener">http://127.0.0.1:8081/nexus/content/repositories/releases/org/apache/lucene/</a></strong> 我们就可以看见solr相关的jar包pom等信息已经上传成功<br><img src="https://s2.ax1x.com/2019/10/03/uwLkQg.png" alt="nexus_solr"></p><h3 id="爬坑小技巧"><a href="#爬坑小技巧" class="headerlink" title="爬坑小技巧"></a><font color="#0077bb">爬坑小技巧</font></h3><p>看似简单的工作，实际操作起来整的耗费了很长的时间，关键是关于<strong>authentication</strong>的那部分google了好久发现没一个人提。为了防止大家再次爬坑，这里给大家分享两个知识点/小技巧</p><blockquote><ul><li>解决ant idea 下载比较慢的问题<br>大家在导入Solr源码项目的时候可能已经发现了，(<strong>ant idea</strong>)整个过程非常的慢,笔者更是等了一个下午才构建好，正常的开发中我们不可能等这么久，这里可能有人说使用maven默认的仓库在海外导致的，我们使用aliyun的仓库就可行了(笔者尝试了，不行，不知道为什么阿里云下载的时候没有http状态码，导致ivy认为仓库不可用)，这里笔者使用了腾讯云的仓库(<a href="http://mirrors.cloud.tencent.com/nexus/repository/maven-public/),企鹅貌似比阿里守规范。。,具体的改动点为，修改**solr-7.7.2/lucene/default-nested-ivy-settings.xml**配置文件，添加自定义resolver" target="_blank" rel="noopener">http://mirrors.cloud.tencent.com/nexus/repository/maven-public/),企鹅貌似比阿里守规范。。,具体的改动点为，修改**solr-7.7.2/lucene/default-nested-ivy-settings.xml**配置文件，添加自定义resolver</a> (+表示在源文件中增加一行，-表示删除源文件中的某一行)<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">resolvers</span>&gt;</span></span><br><span class="line"> +   <span class="tag">&lt;<span class="name">ibiblio</span> <span class="attr">name</span>=<span class="string">"tecent"</span> <span class="attr">root</span>=<span class="string">"http://mirrors.cloud.tencent.com/nexus/repository/maven-public/"</span>  <span class="attr">m2compatible</span>=<span class="string">"true"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">ibiblio</span> <span class="attr">name</span>=<span class="string">"sonatype-releases"</span> <span class="attr">root</span>=<span class="string">"https://oss.sonatype.org/content/repositories/releases"</span> <span class="attr">m2compatible</span>=<span class="string">"true"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">ibiblio</span> <span class="attr">name</span>=<span class="string">"maven.restlet.com"</span> <span class="attr">root</span>=<span class="string">"https://maven.restlet.com"</span> <span class="attr">m2compatible</span>=<span class="string">"true"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">ibiblio</span> <span class="attr">name</span>=<span class="string">"releases.cloudera.com"</span> <span class="attr">root</span>=<span class="string">"https://repository.cloudera.com/cloudera/libs-release-local"</span> <span class="attr">m2compatible</span>=<span class="string">"true"</span> /&gt;</span></span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">-    <span class="tag">&lt;<span class="name">resolver</span> <span class="attr">ref</span>=<span class="string">"main"</span>/&gt;</span></span><br><span class="line">+    <span class="tag">&lt;<span class="name">resolver</span> <span class="attr">ref</span>=<span class="string">"tecent"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">resolver</span> <span class="attr">ref</span>=<span class="string">"maven.restlet.com"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">resolver</span> <span class="attr">ref</span>=<span class="string">"sonatype-releases"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">resolver</span> <span class="attr">ref</span>=<span class="string">"releases.cloudera.com"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">chain</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">resolvers</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul></blockquote><p>备份并删除~/.ivy2/cache,否则会使用缓存，重新执行命令(<strong>ant idea</strong>)之后你会发现还是会卡在resolve,但是下载jar包的速度变快了，笔者google了半天发现没有答案，猜想可能是多个resolve 导致的，尝试注释其他的机器resolver,具体的改动为<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">-    <span class="tag">&lt;<span class="name">resolver</span> <span class="attr">ref</span>=<span class="string">"main"</span>/&gt;</span></span><br><span class="line">+    <span class="tag">&lt;<span class="name">resolver</span> <span class="attr">ref</span>=<span class="string">"tecent"</span>/&gt;</span></span><br><span class="line">+   <span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">    &lt;resolver ref="maven.restlet.com" /&gt;</span></span><br><span class="line"><span class="comment">    &lt;resolver ref="sonatype-releases" /&gt;</span></span><br><span class="line"><span class="comment">    &lt;resolver ref="releases.cloudera.com"/&gt;</span></span><br><span class="line"><span class="comment">+   --&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">chain</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">resolvers</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>删除~/.ivy2/cache ,重新执行命令(<strong>ant idea</strong>)之后，果然速度快了很多,至此，慢的问题我们也解决了，简直💯。(使用默认的话执行需要40分钟，改进之后20分钟即可)</p><blockquote><ul><li>如何定位到<strong>authentication</strong><br>虽然解决<strong>401</strong>这个问题只需要一行代码就行了，但是找到这行代码笔者确实花了很长的时间，最终我找到了关于<a href="https://maven.apache.org/ant-tasks/reference.html#remoteRepository" target="_blank" rel="noopener">remoteRepository</a>的定义才找到<strong>authentication</strong>(一看名字就知道这货是用来传用户名和密码的),最终在文档中定位了其配置在(<a href="http://maven.apache.org/ref/3.6.2/maven-settings/settings.html#class_server)，最终经过一番尝试才成功，太坑了。" target="_blank" rel="noopener">http://maven.apache.org/ref/3.6.2/maven-settings/settings.html#class_server)，最终经过一番尝试才成功，太坑了。</a><br>当然还有一种方法，就是下载ant的源码看在源码中这个case 到底是如何执行了<strong>应该</strong>也能找到这个解法</li></ul></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><font color="#0077bb">总结</font></h2><p>到这里玩转Solr源码系列就完全结束了，虽然内容不多，但是全是干货(毕竟是笔者一步一个坑爬过来的)。如果你对该系列有补充或者更好的意见可以联系Email:fengqingleiyue@163.com<br>最后来碗鸡汤:&lt;<strong>你能走多远,取决于你填坑能力有多强</strong>&gt;<br><img src="https://s2.ax1x.com/2019/10/03/uwX5Zj.jpg" alt="uwX5Zj.jpg"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;玩转Solr源码之—Solr源码Deploy&quot;&gt;&lt;a href=&quot;#玩转Solr源码之—Solr源码Deploy&quot; class=&quot;headerlink&quot; title=&quot;玩转Solr源码之—Solr源码Deploy&quot;&gt;&lt;/a&gt;&lt;font color=&quot;#0077bb&quot;&gt;玩转Solr源码之—Solr源码Deploy&lt;/font&gt;&lt;/h1&gt;&lt;p&gt;这篇文章是&lt;strong&gt;玩转Solr源码&lt;/strong&gt;系列的第三篇，紧接着上一篇的&lt;a href=&quot;https://www.fengqinglei.top/2019/10/01/solr-source-code-debug/&quot;&gt;源码Deug&lt;/a&gt;。如果你已经修改好了源码，并且调试ok，想要将源码打包后分发给同时或者其他项目引用，那么这边文章一定能够帮助你。&lt;br&gt;
    
    </summary>
    
    
      <category term="Solr" scheme="https://www.fengqinglei.top/tags/Solr/"/>
    
      <category term="源码" scheme="https://www.fengqinglei.top/tags/%E6%BA%90%E7%A0%81/"/>
    
      <category term="Deploy" scheme="https://www.fengqinglei.top/tags/Deploy/"/>
    
      <category term="部署" scheme="https://www.fengqinglei.top/tags/%E9%83%A8%E7%BD%B2/"/>
    
      <category term="Nexus" scheme="https://www.fengqinglei.top/tags/Nexus/"/>
    
      <category term="Maven" scheme="https://www.fengqinglei.top/tags/Maven/"/>
    
  </entry>
  
  <entry>
    <title>玩转Solr源码之(二)Solr源码Debug</title>
    <link href="https://www.fengqinglei.top/2019/10/01/solr-source-code-debug/"/>
    <id>https://www.fengqinglei.top/2019/10/01/solr-source-code-debug/</id>
    <published>2019-10-01T14:19:40.000Z</published>
    <updated>2019-10-03T02:33:08.974Z</updated>
    
    <content type="html"><![CDATA[<h1 id="玩转Solr源码之—Solr源码Debug"><a href="#玩转Solr源码之—Solr源码Debug" class="headerlink" title="玩转Solr源码之—Solr源码Debug"></a><font color="#0077bb">玩转Solr源码之—Solr源码Debug</font></h1><p>这篇文章是<strong>玩转Solr源码</strong>系列的第二篇，紧接着上一篇的<a href="https://www.fengqinglei.top/2019/10/01/solr-source-code-import/">源码导入</a>，如果你还对如何将Solr源码导入到IDE还不了解的话，建议先看<strong>源码导入</strong>的部分。</p><h2 id="定位源码入口"><a href="#定位源码入口" class="headerlink" title="定位源码入口"></a><font color="#2C3E50">定位源码入口</font></h2><p>Solr 的源码入口为<strong>org.apache.solr.client.solrj.StartSolrJetty</strong>,主方法也比较简短,<br><a id="more"></a><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">( String[] args )</span> </span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    <span class="comment">//System.setProperty("solr.solr.home", "../../../example/solr");</span></span><br><span class="line"></span><br><span class="line">    Server server = <span class="keyword">new</span> Server();</span><br><span class="line">    ServerConnector connector = <span class="keyword">new</span> ServerConnector(server, <span class="keyword">new</span> HttpConnectionFactory());</span><br><span class="line">    <span class="comment">// Set some timeout options to make debugging easier.</span></span><br><span class="line">    connector.setIdleTimeout(<span class="number">1000</span> * <span class="number">60</span> * <span class="number">60</span>);</span><br><span class="line">    connector.setSoLingerTime(-<span class="number">1</span>);</span><br><span class="line">    connector.setPort(<span class="number">8983</span>);</span><br><span class="line">    server.setConnectors(<span class="keyword">new</span> Connector[] &#123; connector &#125;);</span><br><span class="line">    </span><br><span class="line">    WebAppContext bb = <span class="keyword">new</span> WebAppContext();</span><br><span class="line">    bb.setServer(server);</span><br><span class="line">    bb.setContextPath(<span class="string">"/solr"</span>);</span><br><span class="line">    bb.setWar(<span class="string">"webapp/web"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//    // START JMX SERVER</span></span><br><span class="line"><span class="comment">//    if( true ) &#123;</span></span><br><span class="line"><span class="comment">//      MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer();</span></span><br><span class="line"><span class="comment">//      MBeanContainer mBeanContainer = new MBeanContainer(mBeanServer);</span></span><br><span class="line"><span class="comment">//      server.getContainer().addEventListener(mBeanContainer);</span></span><br><span class="line"><span class="comment">//      mBeanContainer.start();</span></span><br><span class="line"><span class="comment">//    &#125;</span></span><br><span class="line">    </span><br><span class="line">    server.setHandler(bb);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      System.out.println(<span class="string">"&gt;&gt;&gt; STARTING EMBEDDED JETTY SERVER, PRESS ANY KEY TO STOP"</span>);</span><br><span class="line">      server.start();</span><br><span class="line">      <span class="keyword">while</span> (System.in.available() == <span class="number">0</span>) &#123;</span><br><span class="line">        Thread.sleep(<span class="number">5000</span>);</span><br><span class="line">      &#125;</span><br><span class="line">      server.stop();</span><br><span class="line">      server.join();</span><br><span class="line">    &#125; </span><br><span class="line">    <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">      System.exit(<span class="number">100</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></p><h2 id="配置Solr-Home"><a href="#配置Solr-Home" class="headerlink" title="配置Solr Home"></a><font color="#2C3E50">配置Solr Home</font></h2><p>为了能够进行源码的bug，我们需要配置好solr_home,这里我们可以直接使用官方的Solr发行包中的example来做演示（真实的开发中，一般会有自己定义好的solr_home），直接下载solr的发行包(这里以solr-7.7.2为例子)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/Documents</span><br><span class="line">curl <span class="string">"http://mirrors.tuna.tsinghua.edu.cn/apache/lucene/solr/7.7.2/solr-7.7.2.tgz"</span> -o solr-7.7.2.tgz &amp;&amp; tar -xvf solr-7.7.2.tgz</span><br><span class="line">mkdir solr_home &amp;&amp; <span class="built_in">cd</span> solr_home</span><br><span class="line">cp ~/Documents/solr-7.7.2/server/solr/solr.xml ./</span><br><span class="line">cp -r ~/Documents/solr-7.7.2/server/solr/configsets/sample_techproducts_configs ./</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"collection=TEST"</span> &gt; sample_techproducts_configs/core.properties</span><br></pre></td></tr></table></figure></p><p>这里简单解释下最后一个命令(echo “collection=TEST …”)的含义,Solr中有collection和core的概念。</p><blockquote><ul><li>Collection<br>Solr中的collection由一个或者多个core组成，一个collection对应一份独立的config，在单节点的模式下，core和collection是等价的,在集群模式下，一个collection会有多个分片(shard)组成，每个分片又可以又多个副本(replica),每个shard对应一个core</li><li>core<br>Solr中的core是一个包含index和config的运行实例<h2 id="配置StartSolrJetty"><a href="#配置StartSolrJetty" class="headerlink" title="配置StartSolrJetty"></a><font color="#2C3E50">配置StartSolrJetty</font></h2>solr中的solr_home 可以通过通过环境变量进行设置，直接上代码<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">System.setProperty(<span class="string">"solr.solr.home"</span>, <span class="string">"/home/fql/Documents/solr_home"</span>);</span><br></pre></td></tr></table></figure></li></ul></blockquote><p>还有一个地方需要改动，否则程序无法正常启动，直接上代码<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bb.setWar(<span class="string">"webapp/web"</span>); ==&gt; bb.setWar(<span class="string">"solr/webapp/web"</span>);</span><br></pre></td></tr></table></figure></p><p>设置完毕之后就可以启动<strong>StartSolrJetty</strong>后直接在浏览器中输入<a href="http://127.0.0.1:8983/solr就可以开始畅游solr的源码啦.![本地访问](https://s2.ax1x.com/2019/10/02/udq8Tx.png" target="_blank" rel="noopener">http://127.0.0.1:8983/solr就可以开始畅游solr的源码啦.![本地访问](https://s2.ax1x.com/2019/10/02/udq8Tx.png</a>)</p><h3 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a><font color="#2C3E50">小技巧</font></h3><p>StartSolrJetty默认的配置是没有太多的日志输出的，但是既然我们都已经开始debug solr的源代码了，还是输出更多的日志帮助我们更好的分析源码吧，同样，开启debug日志的代码也比较简单<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">System.setProperty(<span class="string">"log4j2.debug"</span>,<span class="string">"true"</span>);</span><br></pre></td></tr></table></figure></p><p>这样我们就剋有看到所有的日志啦。下一篇我们将要介绍如果将修改过的solr源码部署到公司或者个人的nexus仓库,也是本系列的最后一篇。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;玩转Solr源码之—Solr源码Debug&quot;&gt;&lt;a href=&quot;#玩转Solr源码之—Solr源码Debug&quot; class=&quot;headerlink&quot; title=&quot;玩转Solr源码之—Solr源码Debug&quot;&gt;&lt;/a&gt;&lt;font color=&quot;#0077bb&quot;&gt;玩转Solr源码之—Solr源码Debug&lt;/font&gt;&lt;/h1&gt;&lt;p&gt;这篇文章是&lt;strong&gt;玩转Solr源码&lt;/strong&gt;系列的第二篇，紧接着上一篇的&lt;a href=&quot;https://www.fengqinglei.top/2019/10/01/solr-source-code-import/&quot;&gt;源码导入&lt;/a&gt;，如果你还对如何将Solr源码导入到IDE还不了解的话，建议先看&lt;strong&gt;源码导入&lt;/strong&gt;的部分。&lt;/p&gt;
&lt;h2 id=&quot;定位源码入口&quot;&gt;&lt;a href=&quot;#定位源码入口&quot; class=&quot;headerlink&quot; title=&quot;定位源码入口&quot;&gt;&lt;/a&gt;&lt;font color=&quot;#2C3E50&quot;&gt;定位源码入口&lt;/font&gt;&lt;/h2&gt;&lt;p&gt;Solr 的源码入口为&lt;strong&gt;org.apache.solr.client.solrj.StartSolrJetty&lt;/strong&gt;,主方法也比较简短,&lt;br&gt;
    
    </summary>
    
    
      <category term="Solr" scheme="https://www.fengqinglei.top/tags/Solr/"/>
    
      <category term="Debug" scheme="https://www.fengqinglei.top/tags/Debug/"/>
    
      <category term="源码" scheme="https://www.fengqinglei.top/tags/%E6%BA%90%E7%A0%81/"/>
    
      <category term="调试" scheme="https://www.fengqinglei.top/tags/%E8%B0%83%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>玩转Solr源码之(一)Solr源码导入IDE</title>
    <link href="https://www.fengqinglei.top/2019/10/01/solr-source-code-import/"/>
    <id>https://www.fengqinglei.top/2019/10/01/solr-source-code-import/</id>
    <published>2019-10-01T07:47:33.000Z</published>
    <updated>2019-10-02T13:30:57.770Z</updated>
    
    <content type="html"><![CDATA[<h1 id="玩转Solr源码之—Solr源码导入IDE"><a href="#玩转Solr源码之—Solr源码导入IDE" class="headerlink" title="玩转Solr源码之—Solr源码导入IDE"></a><font color="#0077bb">玩转Solr源码之—Solr源码导入IDE</font></h1><h2 id="源码下载"><a href="#源码下载" class="headerlink" title="源码下载"></a><font color="#2C3E50">源码下载</font></h2><p>这个比较简单,直接到solr的官网上下载源码即可,这里以<a href="https://www.apache.org/dyn/closer.lua/lucene/solr/7.7.2/solr-7.7.2-src.tgz" target="_blank" rel="noopener">Solr 7.7.2</a>为例子。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl <span class="string">"https://www.apache.org/dyn/closer.lua/lucene/solr/7.7.2/solr-7.7.2-src.tgz"</span> -o solr-7.7.2-src.tgz &amp;&amp; tar -xvf solr-7.7.2-src.tgz</span><br></pre></td></tr></table></figure></p><h2 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a><font color="#2C3E50">安装依赖</font></h2><blockquote><ul><li>jdk 1.8 </li><li>ant</li></ul></blockquote><p>笔者使用的是RedHat 系统，所以jdk的安装直接使用yum就行<br><a id="more"></a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install  java-1.8.0-openjdk-devel.x86_64</span><br></pre></td></tr></table></figure></p><p>ant需要配置一下环境，也不算复杂<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl <span class="string">"http://mirrors.tuna.tsinghua.edu.cn/apache//ant/binaries/apache-ant-1.9.14-bin.tar.gz"</span> -o apache-ant-1.9.14-bin.tar.gz</span><br><span class="line">tar -xvf apache-ant-1.9.14-bin.tar.gz</span><br><span class="line">rm -rf apache-ant-1.9.14-bin.tar.gz</span><br></pre></td></tr></table></figure></p><p>配置环境变量(以RedHat为例),追加以下信息到~/.bash_profile (~/.bash_profile)，这里我把ant安装到了/opt目录下<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> ANT_HOME=/opt/apache-ant-1.9.14</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$ANT_HOME</span>/bin</span><br></pre></td></tr></table></figure></p><p>简单验证下，看到以下结果就算ok了(source ~/.bash_profile 是为了使刚刚的配置生效)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$source</span> ~/.bash_profile</span><br><span class="line"><span class="variable">$ant</span> -version</span><br><span class="line">Apache Ant(TM) version 1.9.14 compiled on March 12 2019</span><br></pre></td></tr></table></figure></p><h2 id="Build源码"><a href="#Build源码" class="headerlink" title="Build源码"></a><font color="#2C3E50">Build源码</font></h2><p>Solr的源码是使用ivy来管理依赖的，所以需要安装ivy的lib包，但是Solr的项目中可以通过<strong>ant ivy-bootstrap</strong>直接下载ivy的jar包,这里看到”BUILD SUCCESSFUL”表示执行成功了。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$cd</span> solr-7.7.2</span><br><span class="line"><span class="variable">$ant</span> ivy-bootstrap</span><br><span class="line">-ivy-bootstrap1:</span><br><span class="line">    [mkdir] Created dir: /home/fql/.ant/lib</span><br><span class="line">     [<span class="built_in">echo</span>] installing ivy 2.4.0 to /home/fql/.ant/lib</span><br><span class="line">      [get] Getting: https://repo1.maven.org/maven2/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar</span><br><span class="line">      [get] To: /home/fql/.ant/lib/ivy-2.4.0.jar</span><br><span class="line">-ivy-bootstrap2:</span><br><span class="line">-ivy-checksum:</span><br><span class="line">-ivy-remove-old-versions:</span><br><span class="line">ivy-bootstrap:</span><br><span class="line">BUILD SUCCESSFUL</span><br></pre></td></tr></table></figure></p><h2 id="生成Intelij-Idea-Eclipse项目"><a href="#生成Intelij-Idea-Eclipse项目" class="headerlink" title="生成Intelij Idea/Eclipse项目"></a><font color="#2C3E50">生成Intelij Idea/Eclipse项目</font></h2><p>Intelij Idea 是笔者从2013年开始用的，之前用的都是Eclipse,这里不比较两个工具的好坏，能完成项目开发就行，这里以导入Intellij Idea 为例子 (如果是eclipse则为<strong>ant eclipse</strong>)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$cd</span> solr-7.7.2</span><br><span class="line"><span class="variable">$ant</span> idea</span><br></pre></td></tr></table></figure></p><p>这个执行的过程比较长，是因为ivy会从maven的官方repo下载依赖,这里可以去喝杯茶等着程序自动执行完成就行.</p><h4 id="如何解决下载不了org-restlet打头的包"><a href="#如何解决下载不了org-restlet打头的包" class="headerlink" title="如何解决下载不了org.restlet打头的包"></a><font color="#2C3E50">如何解决下载不了org.restlet打头的包</font></h4><p>可能你在执行ant test的时候会遇到类似以下的错误<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">resolve:</span><br><span class="line">[ivy:retrieve] </span><br><span class="line">[ivy:retrieve] :: problems summary ::</span><br><span class="line">[ivy:retrieve] :::: WARNINGS</span><br><span class="line">[ivy:retrieve] [FAILED     ] org.restlet.jee#org.restlet;2.3.0!org.restlet.jar:  (0ms)</span><br><span class="line">[ivy:retrieve] ==== shared: tried</span><br><span class="line">[ivy:retrieve]   /home/fql/.ivy2/shared/org.restlet.jee/org.restlet/2.3.0/jars/org.restlet.jar</span><br><span class="line">[ivy:retrieve] ==== public: tried</span><br><span class="line">[ivy:retrieve]   https://repo1.maven.org/maven2/org/restlet/jee/org.restlet/2.3.0/org.restlet-2.3.0.jar</span><br><span class="line">[ivy:retrieve] [FAILED     ] org.restlet.jee#org.restlet.ext.servlet;2.3.0!org.restlet.ext.servlet.jar:  (0ms)</span><br><span class="line">[ivy:retrieve] ==== shared: tried</span><br><span class="line">[ivy:retrieve]   /home/fql/.ivy2/shared/org.restlet.jee/org.restlet.ext.servlet/2.3.0/jars/org.restlet.ext.servlet.jar</span><br><span class="line">[ivy:retrieve] ==== public: tried</span><br><span class="line">[ivy:retrieve]   https://repo1.maven.org/maven2/org/restlet/jee/org.restlet.ext.servlet/2.3.0/org.restlet.ext.servlet-2.3.0.jar</span><br><span class="line">[ivy:retrieve] ::::::::::::::::::::::::::::::::::::::::::::::</span><br><span class="line">[ivy:retrieve] ::              FAILED DOWNLOADS            ::</span><br><span class="line">[ivy:retrieve] :: ^ see resolution messages for details  ^ ::</span><br><span class="line">[ivy:retrieve] ::::::::::::::::::::::::::::::::::::::::::::::</span><br><span class="line">[ivy:retrieve] :: org.restlet.jee#org.restlet;2.3.0!org.restlet.jar</span><br><span class="line">[ivy:retrieve] :: org.restlet.jee#org.restlet.ext.servlet;2.3.0!org.restlet.ext.servlet.jar</span><br><span class="line">[ivy:retrieve] ::::::::::::::::::::::::::::::::::::::::::::::</span><br><span class="line">[ivy:retrieve] </span><br><span class="line">[ivy:retrieve] :: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS</span><br><span class="line"></span><br><span class="line">BUILD FAILED</span><br><span class="line">/home/fql/IdeaProjects/solr-7.7.2/build.xml:140: The following error occurred while executing this line:</span><br><span class="line">/home/fql/IdeaProjects/solr-7.7.2/solr/build.xml:602: The following error occurred while executing this line:</span><br><span class="line">/home/fql/IdeaProjects/solr-7.7.2/solr/core/build.xml:68: impossible to resolve dependencies:</span><br><span class="line">resolve failed - see output for details</span><br></pre></td></tr></table></figure></p><p>这是因为这个依赖的目录已经在~/.ivy2/cache下面的目录生成了，但是jar下载失败导致的，直接删除对应的目录重新执行<strong>ant test</strong> 就行了,以上面的错误为例，具体执行的命令为<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rm -rf ~/.ivy2/cache/org.restlet*</span><br><span class="line">ant <span class="built_in">test</span></span><br></pre></td></tr></table></figure></p><h2 id="导入Idea"><a href="#导入Idea" class="headerlink" title="导入Idea"></a><font color="#2C3E50">导入Idea</font></h2><p>执行完<strong>ant idea</strong>之后，如果看见以下类似输出表示build成功，我们可以开始将项目导入<strong>Intelij Idea</strong>了<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">-post-idea-instructions:</span><br><span class="line">     [echo] </span><br><span class="line">     [echo] To complete IntelliJ IDEA setup, you must manually configure</span><br><span class="line">     [echo] File | Project Structure | Project | Project SDK.</span><br><span class="line">     [echo]       </span><br><span class="line">     [echo] You won&apos;t have to do this in the future if you define property</span><br><span class="line">     [echo] $&#123;idea.jdk&#125;, e.g. in ~/lucene.build.properties, ~/build.properties</span><br><span class="line">     [echo] or lucene/build.properties, with a value consisting of the</span><br><span class="line">     [echo] following two XML attributes/values (adjust values according to</span><br><span class="line">     [echo] JDKs you have defined locally - see </span><br><span class="line">     [echo] File | Project Structure | Platform Settings | SDKs):</span><br><span class="line">     [echo] </span><br><span class="line">     [echo]     idea.jdk = project-jdk-name=&quot;1.8&quot; project-jdk-type=&quot;JavaSDK&quot;</span><br><span class="line">     [echo]     </span><br><span class="line"></span><br><span class="line">BUILD SUCCESSFUL</span><br></pre></td></tr></table></figure></p><p>打开Intelij Idea , 选择文件-&gt;打开-&gt;选择源码的路径打开即可,Eclipse 直接选择从本地文件系统中打开项目即可。<br><img src="https://s2.ax1x.com/2019/10/01/uUfCDg.png" alt="项目导入"><br>至此，我们已经完成将项目导入到Intelij Idea，下面我会分享如何在Intelij Idea中debug Solr 的源码。</p><h2 id="参考的网址"><a href="#参考的网址" class="headerlink" title="参考的网址"></a><font color="#2C3E50">参考的网址</font></h2><blockquote><ul><li><a href="https://stackoverflow.com/questions/30630227/java-ivy-maven-build-dependency-resolution-for-lucidworks-auto-phrase-tokenizer" target="_blank" rel="noopener">https://stackoverflow.com/questions/30630227/java-ivy-maven-build-dependency-resolution-for-lucidworks-auto-phrase-tokenizer</a></li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;玩转Solr源码之—Solr源码导入IDE&quot;&gt;&lt;a href=&quot;#玩转Solr源码之—Solr源码导入IDE&quot; class=&quot;headerlink&quot; title=&quot;玩转Solr源码之—Solr源码导入IDE&quot;&gt;&lt;/a&gt;&lt;font color=&quot;#0077bb&quot;&gt;玩转Solr源码之—Solr源码导入IDE&lt;/font&gt;&lt;/h1&gt;&lt;h2 id=&quot;源码下载&quot;&gt;&lt;a href=&quot;#源码下载&quot; class=&quot;headerlink&quot; title=&quot;源码下载&quot;&gt;&lt;/a&gt;&lt;font color=&quot;#2C3E50&quot;&gt;源码下载&lt;/font&gt;&lt;/h2&gt;&lt;p&gt;这个比较简单,直接到solr的官网上下载源码即可,这里以&lt;a href=&quot;https://www.apache.org/dyn/closer.lua/lucene/solr/7.7.2/solr-7.7.2-src.tgz&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Solr 7.7.2&lt;/a&gt;为例子。&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;curl &lt;span class=&quot;string&quot;&gt;&quot;https://www.apache.org/dyn/closer.lua/lucene/solr/7.7.2/solr-7.7.2-src.tgz&quot;&lt;/span&gt; -o solr-7.7.2-src.tgz &amp;amp;&amp;amp; tar -xvf solr-7.7.2-src.tgz&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;h2 id=&quot;安装依赖&quot;&gt;&lt;a href=&quot;#安装依赖&quot; class=&quot;headerlink&quot; title=&quot;安装依赖&quot;&gt;&lt;/a&gt;&lt;font color=&quot;#2C3E50&quot;&gt;安装依赖&lt;/font&gt;&lt;/h2&gt;&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;jdk 1.8 &lt;/li&gt;
&lt;li&gt;ant&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;笔者使用的是RedHat 系统，所以jdk的安装直接使用yum就行&lt;br&gt;
    
    </summary>
    
    
      <category term="Solr" scheme="https://www.fengqinglei.top/tags/Solr/"/>
    
      <category term="源码" scheme="https://www.fengqinglei.top/tags/%E6%BA%90%E7%A0%81/"/>
    
      <category term="Intelij Idea" scheme="https://www.fengqinglei.top/tags/Intelij-Idea/"/>
    
      <category term="Eclipse" scheme="https://www.fengqinglei.top/tags/Eclipse/"/>
    
      <category term="ant" scheme="https://www.fengqinglei.top/tags/ant/"/>
    
  </entry>
  
  <entry>
    <title>IBM ICU 简繁转换工具的性能坑</title>
    <link href="https://www.fengqinglei.top/2019/07/21/IBMICUSlowIssue/"/>
    <id>https://www.fengqinglei.top/2019/07/21/IBMICUSlowIssue/</id>
    <published>2019-07-21T06:33:05.000Z</published>
    <updated>2019-09-27T00:46:01.319Z</updated>
    
    <content type="html"><![CDATA[<h1 id="IBM-ICU-简繁转换工具的性能坑"><a href="#IBM-ICU-简繁转换工具的性能坑" class="headerlink" title="IBM ICU 简繁转换工具的性能坑"></a><font color="#0077bb">IBM ICU 简繁转换工具的性能坑</font></h1><h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font color="#2C3E50">背景介绍</font></h2><p>由于工作内容的需要，我们处理的文本数据中会有繁体中文，但是我们的产品的使用客户都是习惯使用简体中文（台湾是中国不可割舍的一部分），所以为了方便用户使用简体中文检索到繁体中文，我们需要在建立索引的时候（这里以solr为例子）进行简繁转换。简单的查看了Solr的文档，以及简单的百度或者Google, 我们可以在Solr(7.7.2)中添加以下的字段类型来进行中文分词和简繁转换。<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">fieldType</span> <span class="attr">name</span>=<span class="string">"test"</span>  <span class="attr">class</span>=<span class="string">"solr.TextField"</span> &gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">analyzer</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">charFilter</span> <span class="attr">class</span>=<span class="string">"solr.HTMLStripCharFilterFactory"</span>/&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">charFilter</span> <span class="attr">class</span>=<span class="string">"solr.MappingCharFilterFactory"</span> <span class="attr">mapping</span>=<span class="string">"mapping-ISOLatin1Accent.txt"</span>/&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">tokenizer</span> <span class="attr">class</span>=<span class="string">"solr.StandardTokenizerFactory"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"solr.CJKWidthFilterFactory"</span>/&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"solr.ICUTransformFilterFactory"</span> <span class="attr">id</span>=<span class="string">"Traditional-Simplified"</span>/&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"solr.LowerCaseFilterFactory"</span>/&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"solr.CJKBigramFilterFactory"</span> <span class="attr">han</span>=<span class="string">"true"</span> <span class="attr">outputUnigrams</span>=<span class="string">"true"</span>/&gt;</span></span><br><span class="line">         <span class="tag">&lt;/<span class="name">analyzer</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">fieldType</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>可以看到这是最简单的中文分词器-&gt; 二元分词，并且在二元的基础上输出了一元的结果，这里可以简单看下分词结果<br><img src="https://s2.ax1x.com/2019/07/21/eCmvwt.png" alt="二元+一元分词效果"><br>似乎配置+使用起来也就5s(666),但是实际情况是性能不够，射不高，跑不快。<br><a id="more"></a></p><h2 id="发现问题"><a href="#发现问题" class="headerlink" title="发现问题"></a><font color="#2C3E50">发现问题</font></h2><p>因为是需要上线的服务，所以习惯性的需要在本地进行一些索引和查询的性能测试，为了测试索引的性能，我准备了2700万的文档数据（包含简体中文和繁体中文），进行压缩之后这些文档大小在135GB左右，基本上可以做一个长时间的性能摸底测试了。<br>我们简单的看下用户测试的机器配置</p><div class="table-container"><table><thead><tr><th style="text-align:center">机器类型</th><th style="text-align:center">CPU 配置</th><th style="text-align:center">内存配置</th><th style="text-align:center">磁盘配置</th></tr></thead><tbody><tr><td style="text-align:center">Solr服务机器</td><td style="text-align:center">12 Core Intel(R) Xeon(R) CPU E5-2420 v2 @ 2.20GHz</td><td style="text-align:center">30GB</td><td style="text-align:center">7TB LVM</td></tr><tr><td style="text-align:center">索引机器</td><td style="text-align:center">4 Core Intel(R) Core(TM) i5-4590 CPU @ 3.30GHz</td><td style="text-align:center">16GB</td><td style="text-align:center">1TB HDD</td></tr></tbody></table></div><p>并且<strong>索引机器</strong> 和 <strong>Solr 服务机器</strong> 之前的网络为千兆带宽，所以网络上不会成为瓶颈。<br>并且Solr 的一些重要配置为</p><blockquote><ul><li>内存配置为SOLR_JAVA_MEM=”-Xms6144m -Xmx6144m”</li><li><p>ramBufferSizeMB配置为</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">ramBufferSizeMB</span>&gt;</span>100<span class="tag">&lt;/<span class="name">ramBufferSizeMB</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>mergePolicyFactory 为</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mergePolicyFactory</span> <span class="attr">class</span>=<span class="string">"org.apache.solr.index.TieredMergePolicyFactory"</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">int</span> <span class="attr">name</span>=<span class="string">"maxMergeAtOnce"</span>&gt;</span>30<span class="tag">&lt;/<span class="name">int</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">int</span> <span class="attr">name</span>=<span class="string">"segmentsPerTier"</span>&gt;</span>40<span class="tag">&lt;/<span class="name">int</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">double</span> <span class="attr">name</span>=<span class="string">"noCFSRatio"</span>&gt;</span>0<span class="tag">&lt;/<span class="name">double</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">int</span> <span class="attr">name</span>=<span class="string">"maxMergedSegmentMB"</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">int</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mergePolicyFactory</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>mergeScheduler 为</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mergeScheduler</span> <span class="attr">class</span>=<span class="string">"org.apache.lucene.index.ConcurrentMergeScheduler"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">int</span> <span class="attr">name</span>=<span class="string">"maxMergeCount"</span>&gt;</span>12<span class="tag">&lt;/<span class="name">int</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">int</span> <span class="attr">name</span>=<span class="string">"maxThreadCount"</span>&gt;</span>6<span class="tag">&lt;/<span class="name">int</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mergeScheduler</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>autoCommit 的配置为 (防止频繁的autocommit生成太多的小段导致Solr一直在合并段)</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">autoCommit</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">maxTime</span>&gt;</span>$&#123;solr.autoCommit.maxTime:180000&#125;<span class="tag">&lt;/<span class="name">maxTime</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">openSearcher</span>&gt;</span>false<span class="tag">&lt;/<span class="name">openSearcher</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">autoCommit</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul></blockquote><p>为了仅可能的用尽服务端的CPU，我在客户端开了20个线程往服务端丢数据，理论上应该可以将服务端的cpu跑满。但是实际的服务端cpu使用情况为<img src="https://s2.ax1x.com/2019/07/21/eCCX59.png" alt="Solr Server CPU usage"><br>并且客户端的索引速度只有<strong>404 doc/s</strong> (因为和具体的文档的大小和内容有关系，速度的值并没有太多的参考意义，我们需要观察的更多的速度的变化率)<br>显然这个结果（cpu使用率）和预期的相差非常大？为啥还有那么多的cpu资源在idl ？？</p><h2 id="定位问题"><a href="#定位问题" class="headerlink" title="定位问题"></a><font color="#2C3E50">定位问题</font></h2><p>为了搞清楚为啥会有这么多cpu在idl，最简单的办法就是使用一些jvm的profile工具，可以看到cpu把大量的资源消耗在什么地方，从而我们可以定位到发生问题的代码的类甚至某一行，这里我们使用的工具是<a href="https://visualvm.github.io/" target="_blank" rel="noopener">VisualVM</a>,当然为了能够让VisualVM能够访问服务端的jvm进程，我们需要对Solr进行一些配置，这个比较简单，主要是修改<strong>solr.in.sh</strong>文件的内容，直接上具体的配置点<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SOLR_OPTS=<span class="string">"<span class="variable">$SOLR_OPTS</span> -Dcom.sun.management.jmxremote"</span></span><br><span class="line">SOLR_OPTS=<span class="string">"<span class="variable">$SOLR_OPTS</span> -Dcom.sun.management.jmxremote.port=28983"</span></span><br><span class="line">SOLR_OPTS=<span class="string">"<span class="variable">$SOLR_OPTS</span> -Dcom.sun.management.jmxremote.ssl=false"</span></span><br><span class="line">SOLR_OPTS=<span class="string">"<span class="variable">$SOLR_OPTS</span> -Dcom.sun.management.jmxremote.authenticate=false"</span></span><br><span class="line">SOLR_OPTS=<span class="string">"<span class="variable">$SOLR_OPTS</span> -Djava.rmi.server.hostname=192.168.18.2"</span></span><br></pre></td></tr></table></figure></p><p>重新开启服务端和客户端进行压力测试（之前的数据已经清理掉了），在程序运行了几分钟之后我们可以通过VisualVM的CPU profile功能发现，大量的cpu时间被消耗在了<strong>com.ibm.icu.text.RuleBasedTransliterator.handlerTransliterate()</strong> 方法上了,<br><img src="https://s2.ax1x.com/2019/07/21/eCeRVP.png" alt="VisualVM CPU Profile"><br>回想了下Solr的中的字段的定了中有个<strong>solr.ICUTransformFilterFactory</strong>filter，显然就是这个filter耗费了大量的cpu时间（看包名和类型就能联想到）,为了简单的验证是不是这个类导致的问题，我简单的做了下对比实验，我可以把这个filter注释掉看下服务端的cpu使用率和客户端的索引速度。直接修改schema中的field type定义<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">fieldType</span> <span class="attr">name</span>=<span class="string">"test"</span>  <span class="attr">class</span>=<span class="string">"solr.TextField"</span> &gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">analyzer</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">charFilter</span> <span class="attr">class</span>=<span class="string">"solr.HTMLStripCharFilterFactory"</span>/&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">charFilter</span> <span class="attr">class</span>=<span class="string">"solr.MappingCharFilterFactory"</span> <span class="attr">mapping</span>=<span class="string">"mapping-ISOLatin1Accent.txt"</span>/&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">tokenizer</span> <span class="attr">class</span>=<span class="string">"solr.StandardTokenizerFactory"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"solr.CJKWidthFilterFactory"</span>/&gt;</span></span><br><span class="line">                <span class="comment">&lt;!-- &lt;filter class="solr.ICUTransformFilterFactory" id="Traditional-Simplified"/&gt; --&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"solr.LowerCaseFilterFactory"</span>/&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"solr.CJKBigramFilterFactory"</span> <span class="attr">han</span>=<span class="string">"true"</span> <span class="attr">outputUnigrams</span>=<span class="string">"true"</span>/&gt;</span></span><br><span class="line">         <span class="tag">&lt;/<span class="name">analyzer</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">fieldType</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>然后重新开启服务端和客户端进行压力测试（之前的数据已经清理掉了），直接上cpu使用率和索引速度信息<br><img src="https://s2.ax1x.com/2019/07/21/eCAuWj.png" alt="Solr Server CPU usage"><br>并且客户端的索引速度也飙到了<strong>900 doc/s</strong>，尼玛坑爹的<strong>ICUTransformFilterFactory</strong>,搞个简繁转换还能搞出个幺蛾子，再看下VisualVM  的cpu profile结果<br><img src="https://s2.ax1x.com/2019/07/21/eCenH0.png" alt="VisualVM CPU Profile"><br>症状100%消失,更加肯定就是<strong>ICUTransformFilterFactory</strong>搞出来的幺蛾子，怎么弄个简繁转换还能出这种问题，弱爆了。。。</p><h2 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a><font color="#2C3E50">源码分析</font></h2><p>为了100%确认真的是<strong>ICUTransformFilterFactory</strong>导致的问题，我下载了Solr-7.7.2的源代码，并且进行了源码的debug，<br>最终在<strong>com.ibm.icu.text.RuleBasedTransliterator.handlerTransliterate()</strong>中找到的真正的问题所在，废话不说上代码<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Deprecated</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">handleTransliterate</span><span class="params">(Replaceable text, Position index, <span class="keyword">boolean</span> incremental)</span> </span>&#123;</span><br><span class="line">        RuleBasedTransliterator.Data var4 = <span class="keyword">this</span>.data;</span><br><span class="line">        <span class="keyword">synchronized</span>(<span class="keyword">this</span>.data) &#123;</span><br><span class="line">            <span class="keyword">int</span> loopCount = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">int</span> loopLimit = index.limit - index.start &lt;&lt; <span class="number">4</span>;</span><br><span class="line">            <span class="keyword">if</span>(loopLimit &lt; <span class="number">0</span>) &#123;</span><br><span class="line">                loopLimit = <span class="number">2147483647</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span>(index.start &lt; index.limit &amp;&amp; loopCount &lt;= loopLimit &amp;&amp; <span class="keyword">this</span>.data.ruleSet.transliterate(text, index, incremental)) &#123;</span><br><span class="line">                ++loopCount;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p><p>可见看见大大的<strong>synchronized</strong>关键字，难怪嘛，Solr的索引是多线程的，并且在看了ICU的一些初始化过程之后，在简繁转换这个case上的RuleBasedTransliterator.Data只用初始化一次就好，（具体的过程比较复杂，简单的来说这中简繁转换的rule来自于配置文件，显然配置文件不可能加载多次），所以一旦上了多线程就各种卡，cpu全消耗在内斗上了，哎。。。。，不过好消息是这个<strong>RuleBasedTransliterator</strong>类已经被打上了<strong>Deprecated</strong>标签了，所以估计在高版本的icu api中这个问题会不复存在。</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a><font color="#2C3E50">解决方案</font></h2><p>最简单的解决方案就是等—&gt; 等官方干掉这个类，这样期待在未来的版本中icu不再使用这个坑爹的<strong>synchronized</strong>关键字，当然这显然不是最好的版本，当然还有一种相对简单的方案就是直接干掉<strong>synchronized</strong>关键字，然后重新编译打包一下就好，如果只是为了写这篇博客确实可以这么干，因为后期的维护成本为0，但是在真是的企业开发中还是不太可取的，如果solr今后版本升级，我们也要下载对应的icu包然后去掉<strong>synchronized</strong>关键字，然后在重新编译打包等操作，虽然说是一次性工作，但是相当于我们间接的在维护icu的源码了。<br>还有一种最暴力的做法，但是却是我比较推荐的做法，我们可以自己造一个简繁转换的solrplugin，确实，只是简单的简体中文到繁体中文的转换，我们只需要找到简繁的对照表就可以做了。说干就干，我直接提取了icu包中的简体中文和繁体中文的对照表（当然获取途径有多种，甚至wiki上都有），这里是我提取的<a href="https://github.com/fengqingleiyue/apache-solr-plugins/blob/master/zh-converter/src/main/resources/hant_hans.properties" target="_blank" rel="noopener">简体繁体对照表</a>,有了简繁转换表，那么solrplugin也就相对简单了，不多说，直接上<a href="https://github.com/fengqingleiyue/apache-solr-plugins/tree/master/zh-converter" target="_blank" rel="noopener">代码</a>,为了验证问题是否解决，我简单做了下对比测试，索引速度直接从原来的<strong>401/s</strong>上升到了<strong>936/s</strong> ,服务端的cpu使用率也能够飙升至<strong>90%</strong>左右，直接上图<img src="https://s2.ax1x.com/2019/08/18/mlPsMT.png" alt="Solr Server CPU Usage">至此icu导致的性能问题，已经得到解决。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;IBM-ICU-简繁转换工具的性能坑&quot;&gt;&lt;a href=&quot;#IBM-ICU-简繁转换工具的性能坑&quot; class=&quot;headerlink&quot; title=&quot;IBM ICU 简繁转换工具的性能坑&quot;&gt;&lt;/a&gt;&lt;font color=&quot;#0077bb&quot;&gt;IBM ICU 简繁转换工具的性能坑&lt;/font&gt;&lt;/h1&gt;&lt;h2 id=&quot;背景介绍&quot;&gt;&lt;a href=&quot;#背景介绍&quot; class=&quot;headerlink&quot; title=&quot;背景介绍&quot;&gt;&lt;/a&gt;&lt;font color=&quot;#2C3E50&quot;&gt;背景介绍&lt;/font&gt;&lt;/h2&gt;&lt;p&gt;由于工作内容的需要，我们处理的文本数据中会有繁体中文，但是我们的产品的使用客户都是习惯使用简体中文（台湾是中国不可割舍的一部分），所以为了方便用户使用简体中文检索到繁体中文，我们需要在建立索引的时候（这里以solr为例子）进行简繁转换。简单的查看了Solr的文档，以及简单的百度或者Google, 我们可以在Solr(7.7.2)中添加以下的字段类型来进行中文分词和简繁转换。&lt;br&gt;&lt;figure class=&quot;highlight xml&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;fieldType&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;name&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&quot;test&quot;&lt;/span&gt;  &lt;span class=&quot;attr&quot;&gt;class&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&quot;solr.TextField&quot;&lt;/span&gt; &amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;         &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;analyzer&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;charFilter&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;class&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&quot;solr.HTMLStripCharFilterFactory&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;charFilter&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;class&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&quot;solr.MappingCharFilterFactory&quot;&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;mapping&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&quot;mapping-ISOLatin1Accent.txt&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;class&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&quot;solr.StandardTokenizerFactory&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;		&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;filter&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;class&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&quot;solr.CJKWidthFilterFactory&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;filter&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;class&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&quot;solr.ICUTransformFilterFactory&quot;&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;id&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&quot;Traditional-Simplified&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;filter&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;class&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&quot;solr.LowerCaseFilterFactory&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                &lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;filter&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;class&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&quot;solr.CJKBigramFilterFactory&quot;&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;han&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&quot;true&quot;&lt;/span&gt; &lt;span class=&quot;attr&quot;&gt;outputUnigrams&lt;/span&gt;=&lt;span class=&quot;string&quot;&gt;&quot;true&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;         &lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;analyzer&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;fieldType&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;可以看到这是最简单的中文分词器-&amp;gt; 二元分词，并且在二元的基础上输出了一元的结果，这里可以简单看下分词结果&lt;br&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/07/21/eCmvwt.png&quot; alt=&quot;二元+一元分词效果&quot;&gt;&lt;br&gt;似乎配置+使用起来也就5s(666),但是实际情况是性能不够，射不高，跑不快。&lt;br&gt;
    
    </summary>
    
    
      <category term="Solr" scheme="https://www.fengqinglei.top/tags/Solr/"/>
    
      <category term="IBM ICU" scheme="https://www.fengqinglei.top/tags/IBM-ICU/"/>
    
      <category term="Traditional-Simplified" scheme="https://www.fengqinglei.top/tags/Traditional-Simplified/"/>
    
      <category term="Performance" scheme="https://www.fengqinglei.top/tags/Performance/"/>
    
      <category term="性能调优" scheme="https://www.fengqinglei.top/tags/%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/"/>
    
  </entry>
  
  <entry>
    <title>如何动态设置Solr Payload</title>
    <link href="https://www.fengqinglei.top/2019/07/10/SolrPayLoads/"/>
    <id>https://www.fengqinglei.top/2019/07/10/SolrPayLoads/</id>
    <published>2019-07-10T15:05:22.000Z</published>
    <updated>2019-09-27T00:41:27.954Z</updated>
    
    <content type="html"><![CDATA[<h1 id="如何动态设置Solr-Payload"><a href="#如何动态设置Solr-Payload" class="headerlink" title="如何动态设置Solr Payload "></a><font color="#0077bb">如何动态设置Solr Payload </font></h1><h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a><font color="#2C3E50">问题背景</font></h2><p>Solr 从6.x开始提供了index payload的功能，（payload是指可以指定为分词之后的每个term指定一个二进制的信息，用于针对每个term作出一定的行为，例如可以用来存储指定的score用于排序，也可以用来存储指定的数据，用于检查term的一定的检查），简单的查阅官方文档之后（这里使用solr7.7.2作为样例）,得知我们可以动态的设定每个term的payload（参考<a href="https://lucidworks.com/post/end-to-end-payload-example-in-solr/" target="_blank" rel="noopener">SolrPayload</a>），也可以根据指定的token type设置payload数值(参考<a href="https://lucene.apache.org/solr/guide/7_4/filter-descriptions.html#numeric-payload-token-filter" target="_blank" rel="noopener">Numeric Payload Token Filter</a>)，但是上述两种方式都无法根据输入的变化而动态的改变payload的值。<br><a id="more"></a></p><h3 id="具体需求"><a href="#具体需求" class="headerlink" title="具体需求"></a><font color="#2C3E50">具体需求</font></h3><p>由于和具体的的业务有关，我这里将需求简单化。</p><blockquote><ul><li>业务中的字段类型为多值字段</li><li>同一篇文档的多值字段中不同的数值需要设置不同的payload</li><li>需要更具不同的参数设定不同的payload数值</li></ul></blockquote><p>看到这里老司机肯定说这个需求太简单了，直接使用<strong>Numeric Payload Token Filter</strong> 就行了，可以将多值字段放到多个字段中，每个字段指定不同的payload的问题也可以解决啊 (只需要指定不同字段类型下面例子中的payload值就行了)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;analyzer&gt;</span><br><span class="line">  &lt;tokenizer class=&quot;solr.WhitespaceTokenizerFactory&quot;/&gt;</span><br><span class="line">  &lt;filter class=&quot;solr.NumericPayloadTokenFilterFactory&quot; payload=&quot;0.75&quot; typeMatch=&quot;word&quot;/&gt;</span><br><span class="line">&lt;/analyzer&gt;</span><br></pre></td></tr></table></figure></p><p>确实，在多值字段的值的个数较少的时候我也推荐这种做法，因为相对比较简单，而且容易操作。但是如果我有多值字段有20个值，那么这样我的schema里面必须得有20个字段类型，20个字段来对应的多个payload ,那么管理这20个字段就变成了一件蛋疼的事情。<br>当然这里也有人会说为啥不用solr的<strong>DelimitedPayloadTokenFilter</strong>(<a href="https://lucidworks.com/post/solr-payloads/）" target="_blank" rel="noopener">用法可以参考</a>,我们可以手动指定每个term的payload，例如 (分隔符|后面的数字为payload)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">id,vals_dpf</span><br><span class="line">1,one|1.0 two|2.0 three|3.0</span><br><span class="line">2,weighted|50.0 weighted|100.0</span><br></pre></td></tr></table></figure></p><p>是的，确实可以，但是前提是你的文档内容比较简单（内容简单的话推荐使用<strong>DelimitedPayloadTokenFilter</strong>,可以在index代码中根据业务动态控制），如果文档比较复杂，比如就是一段文本，那么相当于你得在index代码中将文本进行分词然后拼接成上述的例子，但是如果拼接的话solr的分词功能就形同摆设了，那么有没有办法既能保持index代码的简单性，也可以使用solr的分词功能来完成上述的需求呢？</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a><font color="#2C3E50">解决方案</font></h2><p>解决方案其实只有两步</p><blockquote><ul><li>可以动态的设置某个字段的某个值的某个属性</li><li>自定Tokenizer或者filter根据某个属性设定对用的payload</li></ul></blockquote><p>(我去，这简直就是正确的废话)<br>那么既然我们的“解决方案”已经有了，我们该怎么一步步的实现这个“解决方案呢”，我们来一步步分析下，</p><blockquote><ul><li>我们怎么动态的设置某个字段的某个值的某个属性？</li></ul></blockquote><p>由于在index的代码中，Solr的给我们的API比较简单，直接就是 SolrInputDocument.set(fieldname,fieldvalue),我们能控制的要么就是fieldname，要么就是fieldvalue， 控制fieldname的方案我前面已经介绍过,可以使用<strong>Numeric Payload Token Filter</strong> 来解决我们的问题，这里我们可能设法在fieldname上做文章了，那么可以选择的方案只能是在fieldvalue上下功夫了，显然，我们只能在fieldvalue的头上加一些特殊的标记来解决问题，当然这些特殊的标记不能和你的字段值的某个或者某些特征发生冲突，例如我们可以在字段值的前段加上”[|xxx|]”,那么我们就可以在更具xxx的内容设定某个字段的某个属性就行了</p><blockquote><ul><li>如何在solr端读取xxx的内容？</li></ul></blockquote><p>solr的index功能是基于Lucene提供的，那么Lucene给我的api也比较简单<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Document doc = new Document();</span><br><span class="line">doc.add(new StringField(&quot;fieldName&quot;,&quot;document context&quot;, Field.Store.YES));</span><br></pre></td></tr></table></figure></p><p>也就是所有的文档的字段到lucene这一层肯定都是转成一个document，然后向文档里面添加对应的字段，不同的字段使用不同的分词器来进行分词，对，我们可以自定义字段，在生成这个字段的时候解析<strong>xxx</strong>的内容，并且根据<strong>xxx</strong>的内容设定某个属性的值问题不就能解决了吗？<br>简单的翻看了Solr的某个字段类型的源码（这里以<strong>Solr.TextField</strong>为例子）<br><img src="https://s2.ax1x.com/2019/07/21/epDlO1.png" alt="Solr.TextField Method List"> ,都是一些get方法，貌似没啥用，看看父类<strong>org.apache.solr.schema.FieldType</strong><br><img src="https://s2.ax1x.com/2019/07/21/epDVoT.png" alt="FieldType Method List"><br>貌似<strong>createField</strong>就是我们要找的方法，我们可以在这里解析fieldvalue ,并且根据<strong>xxx</strong>设置这个字段的某个属性，然后拿掉<strong>xxx</strong>,还原原始的fieldvalue</p><blockquote><ul><li>“某个属性”到底是哪个属性</li></ul></blockquote><p>在上文中我们一直提到“某个属性”，但是我们到目前为止还是没有找到这个可以设置的属性到底是啥，但是如果我们不找到这个属性，上面的分析过程全是扯淡的，为了找到这个属性，我们必须知道这个属性满足什么样的特征呢，我们分析下这个属性需要满足的特征</p><blockquote><blockquote><ul><li>这个属性不能对原始的fieldvalue产生变更</li></ul></blockquote></blockquote><p>为什么？按照我们的想法，这个”某个属性“不能对原始的fieldvalue参数影响，如果改变了原始value，</p><blockquote><blockquote><ul><li>这个属性可以必须是跟着term走的，并且可以在tokenizer和filter中可以获取</li></ul></blockquote></blockquote><p>为什么？按照我们的做法，我们必须在某个tokenizer或者filter或者这个属性，才能设置payload的值，其他的地方没发接触到payload </p><p>明确了上述两个特征，我们找个filter／tokenizer的源代码看下 ,找个最简单的<strong>LowerCaseFilter</strong> 看看<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LowerCaseFilter</span> <span class="keyword">extends</span> <span class="title">TokenFilter</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Create a new LowerCaseFilter, that normalizes token text to lower case.</span></span><br><span class="line"><span class="comment">   * </span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> in TokenStream to filter</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">LowerCaseFilter</span><span class="params">(TokenStream in)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>(in);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">incrementToken</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (input.incrementToken()) &#123;</span><br><span class="line">      CharacterUtils.toLowerCase(termAtt.buffer(), <span class="number">0</span>, termAtt.length());</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span></span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>看到这里我们有了答案，原来CharTermAttribute就是Lucene／Solr原来在设定某个term的值的，那么我们可以自己造一个，放入根据<strong>xxx</strong>得到的属性，然后在filter里面或者这个属性，不就可以动态的设定payload的值了嘛， 对头， 上代码 </p><blockquote><ul><li>自定义Solr的Field </li></ul></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DynamicPayloadsField</span> <span class="keyword">extends</span> <span class="title">TextField</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger log = LoggerFactory.getLogger(MethodHandles.lookup().lookupClass());</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> IndexableField <span class="title">createField</span><span class="params">(SchemaField field, Object value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (!field.indexed() &amp;&amp; !field.stored()) &#123;</span><br><span class="line">            <span class="keyword">if</span> (log.isTraceEnabled())</span><br><span class="line">                log.trace(<span class="string">"Ignoring unindexed/unstored field: "</span> + field);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        String val;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            val = toInternal(value.toString());</span><br><span class="line">        &#125; <span class="keyword">catch</span> (RuntimeException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> SolrException( SolrException.ErrorCode.SERVER_ERROR, <span class="string">"Error while creating field '"</span> + field + <span class="string">"' from value '"</span> + value + <span class="string">"'"</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (val==<span class="keyword">null</span>) <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        String target = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">if</span> (val.startsWith(<span class="string">"[|"</span>) &amp;&amp; val.contains(<span class="string">"|]"</span>)) &#123;</span><br><span class="line">            target = val.substring(<span class="number">2</span>,val.indexOf(<span class="string">"|]"</span>));</span><br><span class="line">            <span class="keyword">if</span> (DynamicPayloadAttribute.FIELD_MAPPINGS.containsKey(target)) &#123;</span><br><span class="line">                String parsedData = val.substring(val.indexOf(<span class="string">"|]"</span>)+<span class="number">2</span>);</span><br><span class="line">                TokenizerChain tokenizerChain = (TokenizerChain)field.getType().getIndexAnalyzer();</span><br><span class="line">                Tokenizer tk = tokenizerChain.getTokenizerFactory().create(TokenStream.DEFAULT_TOKEN_ATTRIBUTE_FACTORY);</span><br><span class="line">                TokenStream ts = tk;</span><br><span class="line">                <span class="keyword">for</span> (TokenFilterFactory filter : tokenizerChain.getTokenFilterFactories()) &#123;</span><br><span class="line">                    ts = filter.create(ts);</span><br><span class="line">                &#125;</span><br><span class="line">                Analyzer.TokenStreamComponents components = <span class="keyword">new</span> Analyzer.TokenStreamComponents(tk, ts);</span><br><span class="line">                Reader stringReader = tokenizerChain.initReader(field.getName(),<span class="keyword">new</span> StringReader(parsedData));</span><br><span class="line">                tk.setReader(stringReader);</span><br><span class="line">                TokenStream stream = components.getTokenStream();</span><br><span class="line">                stream.getAttribute(DynamicPayloadAttribute.class).setPayloadType(target);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Field(field.getName(), stream, field);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">super</span>.createField(field.getName(), val, field);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">super</span>.createField(field.getName(), val, field);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><ul><li>自定义Attribute</li></ul></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">DynamicPayloadAttribute</span> <span class="keyword">extends</span> <span class="title">Attribute</span> </span>&#123;</span><br><span class="line">    Map&lt;String,Integer&gt; FIELD_MAPPINGS = <span class="keyword">new</span> HashMap&lt;String,Integer&gt;()&#123;</span><br><span class="line">        &#123;</span><br><span class="line">            put(<span class="string">"A"</span>,<span class="number">0</span>);</span><br><span class="line">            put(<span class="string">"B"</span>,<span class="number">1</span>);</span><br><span class="line">            put(<span class="string">"C"</span>,<span class="number">2</span>);</span><br><span class="line">            put(<span class="string">"D"</span>,<span class="number">3</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">    String DEFAULT_PAYLOAD_TYPE = <span class="string">"UNKNOW_PAYLOAD_TYPE"</span>;</span><br><span class="line">    <span class="function">String <span class="title">payloadType</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">setPayloadType</span><span class="params">(String type)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">byte</span> <span class="title">getPayLoad</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><ul><li>自定义filter</li></ul></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DynamicPayloadTokenFilter</span> <span class="keyword">extends</span> <span class="title">TokenFilter</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> PayloadAttribute payAtt = addAttribute(PayloadAttribute.class);</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> DynamicPayloadAttribute dypayAtt = addAttribute(DynamicPayloadAttribute.class);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="title">DynamicPayloadTokenFilter</span><span class="params">(TokenStream input)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(input);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">incrementToken</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (input.incrementToken()) &#123;</span><br><span class="line">            <span class="keyword">byte</span> payload = dypayAtt.getPayLoad();</span><br><span class="line">            <span class="keyword">if</span>(payload==Byte.MAX_VALUE)&#123;</span><br><span class="line">                payAtt.setPayload(<span class="keyword">null</span>);</span><br><span class="line">            &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">byte</span> array [] = &#123;payload&#125;;</span><br><span class="line">                payAtt.setPayload(<span class="keyword">new</span> BytesRef(array));</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里我只放上了重要的代码片段，完整的代码请参考<a href="https://github.com/fengqingleiyue/apache-solr-plugins/tree/master/dynamic-payload" target="_blank" rel="noopener">apache-solr-plugins/dynamic-payload</a></p><h2 id="爬坑指南"><a href="#爬坑指南" class="headerlink" title="爬坑指南"></a><font color="#2C3E50">爬坑指南</font></h2><p>当然如果所有的细节都如上述描述那么简单，那么各位看官肯定觉得无聊死了，所有的程序员朋友们肯定更加关注这个过程中有没有什么坑，当然坑是存在的，坑是痛苦的，爬坑的过程是痛苦的，爬坑是需要分享的。<br>遇到的坑:<br>在Solr的<a href="http://127.0.0.1:8983/solr/#/TEST/documents" target="_blank" rel="noopener">document</a>页面上添加文档是成功的，但是一旦用代码上多线程立马报错（主要的错误就是TokenStream 在调用前没有调用reset方法）<br>在没有修正这个问题之前的<strong>createField</strong>的方法实现为（部分）<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (val.startsWith(<span class="string">"[|"</span>) &amp;&amp; val.contains(<span class="string">"|]"</span>)) &#123;</span><br><span class="line">           target = val.substring(<span class="number">2</span>,val.indexOf(<span class="string">"|]"</span>));</span><br><span class="line">           <span class="keyword">if</span> (DynamicPayloadAttribute.FIELD_MAPPINGS.containsKey(target)) &#123;</span><br><span class="line">               String parsedData = val.substring(val.indexOf(<span class="string">"|]"</span>)+<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">               TokenStream stream = field.getType().getIndexAnalyzer().tokenStream(field.getName(),parsedData);</span><br><span class="line">               stream.addAttributeImpl(<span class="keyword">new</span> DynamicPayloadAttributeImpl(target));</span><br><span class="line">               <span class="keyword">return</span> <span class="keyword">new</span> Field(field.getName(),stream,field);</span><br></pre></td></tr></table></figure></p><p>而问题就出现在tokenStream方法上<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">field.getType().getIndexAnalyzer().tokenStream(field.getName(),parsedData);</span><br></pre></td></tr></table></figure></p><p>进入实现可以看到TokenStreamComponents是会重用的<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> TokenStream <span class="title">tokenStream</span><span class="params">(String fieldName, String text)</span> </span>&#123;</span><br><span class="line">        Analyzer.TokenStreamComponents components = <span class="keyword">this</span>.reuseStrategy.getReusableComponents(<span class="keyword">this</span>, fieldName);</span><br><span class="line">        ReusableStringReader strReader = components != <span class="keyword">null</span> &amp;&amp; components.reusableStringReader != <span class="keyword">null</span>?components.reusableStringReader:<span class="keyword">new</span> ReusableStringReader();</span><br><span class="line">        strReader.setValue(text);</span><br><span class="line">        Reader r = <span class="keyword">this</span>.initReader(fieldName, strReader);</span><br><span class="line">        <span class="keyword">if</span>(components == <span class="keyword">null</span>) &#123;</span><br><span class="line">            components = <span class="keyword">this</span>.createComponents(fieldName);</span><br><span class="line">            <span class="keyword">this</span>.reuseStrategy.setReusableComponents(<span class="keyword">this</span>, fieldName, components);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        components.setReader(r);</span><br><span class="line">        components.reusableStringReader = strReader;</span><br><span class="line">        <span class="keyword">return</span> components.getTokenStream();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p><p>这样做的好处就是不用重复生成tokenstream compents，并且不同的只是输入，其他的都是一样的，但是为啥在我们的应用场景就会报错呢，原来错误不再我们的使用方式上，而是在solr的后续的流程中会调用<strong>Field.tokenStream</strong>方法，并且在<strong>tokenStream</strong>属性不为空的情况下，就会直接return，所以想象下，如果同一个字段的不同值公用了同一个tokenstream属性，那么针对这个tokenstream设定的不同值，只会在最后一个调用的属性会成功。（显然这和我们的期望相违背）<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> TokenStream <span class="title">tokenStream</span><span class="params">(Analyzer analyzer, TokenStream reuse)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(<span class="keyword">this</span>.fieldType().indexOptions() == IndexOptions.NONE) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span>(!<span class="keyword">this</span>.fieldType().tokenized()) &#123;</span><br><span class="line">            <span class="keyword">if</span>(<span class="keyword">this</span>.stringValue() != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span>(!(reuse <span class="keyword">instanceof</span> Field.StringTokenStream)) &#123;</span><br><span class="line">                    reuse = <span class="keyword">new</span> Field.StringTokenStream();</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                ((Field.StringTokenStream)reuse).setValue(<span class="keyword">this</span>.stringValue());</span><br><span class="line">                <span class="keyword">return</span> (TokenStream)reuse;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span>(<span class="keyword">this</span>.binaryValue() != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span>(!(reuse <span class="keyword">instanceof</span> Field.BinaryTokenStream)) &#123;</span><br><span class="line">                    reuse = <span class="keyword">new</span> Field.BinaryTokenStream();</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                ((Field.BinaryTokenStream)reuse).setValue(<span class="keyword">this</span>.binaryValue());</span><br><span class="line">                <span class="keyword">return</span> (TokenStream)reuse;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Non-Tokenized Fields must have a String value"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span>(<span class="keyword">this</span>.tokenStream != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">this</span>.tokenStream;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span>(<span class="keyword">this</span>.readerValue() != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> analyzer.tokenStream(<span class="keyword">this</span>.name(), <span class="keyword">this</span>.readerValue());</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span>(<span class="keyword">this</span>.stringValue() != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> analyzer.tokenStream(<span class="keyword">this</span>.name(), <span class="keyword">this</span>.stringValue());</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Field must have either TokenStream, String, Reader or Number value; got "</span> + <span class="keyword">this</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p><p>但是即使这样，也不应该会报错，原来如果在create field字段中调用<strong>tokenstream</strong>方法，那么针对多值字段，那么同一个<strong>tokenstream</strong>对象就有可能可能共享，那么当Solr处理完第一个值结束的时候会将tokenstream进行重制，那么原有的reader就会失效，那么当Solr开始处理第二个值的时候由于共享的同一个tokenstream,那么对用的reader已经失效，就会抛出错误。所以我们需要每次生成不同的tokenstream就可以解决问题<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (val.startsWith(<span class="string">"[|"</span>) &amp;&amp; val.contains(<span class="string">"|]"</span>)) &#123;</span><br><span class="line">            target = val.substring(<span class="number">2</span>,val.indexOf(<span class="string">"|]"</span>));</span><br><span class="line">            <span class="keyword">if</span> (DynamicPayloadAttribute.FIELD_MAPPINGS.containsKey(target)) &#123;</span><br><span class="line">                String parsedData = val.substring(val.indexOf(<span class="string">"|]"</span>)+<span class="number">2</span>);</span><br><span class="line">                TokenizerChain tokenizerChain = (TokenizerChain)field.getType().getIndexAnalyzer();</span><br><span class="line">                Tokenizer tk = tokenizerChain.getTokenizerFactory().create(TokenStream.DEFAULT_TOKEN_ATTRIBUTE_FACTORY);</span><br><span class="line">                TokenStream ts = tk;</span><br><span class="line">                <span class="keyword">for</span> (TokenFilterFactory filter : tokenizerChain.getTokenFilterFactories()) &#123;</span><br><span class="line">                    ts = filter.create(ts);</span><br><span class="line">                &#125;</span><br><span class="line">                Analyzer.TokenStreamComponents components = <span class="keyword">new</span> Analyzer.TokenStreamComponents(tk, ts);</span><br><span class="line">                Reader stringReader = tokenizerChain.initReader(field.getName(),<span class="keyword">new</span> StringReader(parsedData));</span><br><span class="line">                tk.setReader(stringReader);</span><br><span class="line">                TokenStream stream = components.getTokenStream();</span><br><span class="line">                stream.getAttribute(DynamicPayloadAttribute.class).setPayloadType(target);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Field(field.getName(), stream, field);</span><br></pre></td></tr></table></figure></p><h2 id="参考的项目或者文章"><a href="#参考的项目或者文章" class="headerlink" title="参考的项目或者文章"></a><font color="#2C3E50">参考的项目或者文章</font></h2><p><a href="https://lucidworks.com/post/solr-payloads/" target="_blank" rel="noopener">SolrPayload 功能 Example</a><br><a href="https://lucene.apache.org/solr/guide/7_4/filter-descriptions.html#numeric-payload-token-filter" target="_blank" rel="noopener">Solr Numeric Payload Token Filter</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;如何动态设置Solr-Payload&quot;&gt;&lt;a href=&quot;#如何动态设置Solr-Payload&quot; class=&quot;headerlink&quot; title=&quot;如何动态设置Solr Payload &quot;&gt;&lt;/a&gt;&lt;font color=&quot;#0077bb&quot;&gt;如何动态设置Solr Payload &lt;/font&gt;&lt;/h1&gt;&lt;h2 id=&quot;问题背景&quot;&gt;&lt;a href=&quot;#问题背景&quot; class=&quot;headerlink&quot; title=&quot;问题背景&quot;&gt;&lt;/a&gt;&lt;font color=&quot;#2C3E50&quot;&gt;问题背景&lt;/font&gt;&lt;/h2&gt;&lt;p&gt;Solr 从6.x开始提供了index payload的功能，（payload是指可以指定为分词之后的每个term指定一个二进制的信息，用于针对每个term作出一定的行为，例如可以用来存储指定的score用于排序，也可以用来存储指定的数据，用于检查term的一定的检查），简单的查阅官方文档之后（这里使用solr7.7.2作为样例）,得知我们可以动态的设定每个term的payload（参考&lt;a href=&quot;https://lucidworks.com/post/end-to-end-payload-example-in-solr/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;SolrPayload&lt;/a&gt;），也可以根据指定的token type设置payload数值(参考&lt;a href=&quot;https://lucene.apache.org/solr/guide/7_4/filter-descriptions.html#numeric-payload-token-filter&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Numeric Payload Token Filter&lt;/a&gt;)，但是上述两种方式都无法根据输入的变化而动态的改变payload的值。&lt;br&gt;
    
    </summary>
    
    
      <category term="Solr" scheme="https://www.fengqinglei.top/tags/Solr/"/>
    
      <category term="Lucene" scheme="https://www.fengqinglei.top/tags/Lucene/"/>
    
      <category term="Payload" scheme="https://www.fengqinglei.top/tags/Payload/"/>
    
      <category term="动态设置" scheme="https://www.fengqinglei.top/tags/%E5%8A%A8%E6%80%81%E8%AE%BE%E7%BD%AE/"/>
    
      <category term="Dynamic" scheme="https://www.fengqinglei.top/tags/Dynamic/"/>
    
  </entry>
  
</feed>
